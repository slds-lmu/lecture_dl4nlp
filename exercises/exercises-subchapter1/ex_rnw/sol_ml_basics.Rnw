
\begin{enumerate}[a)]
  \item Supervised learning problem - the model will be learned from historical
  credit data for which payment history has been observed (knowing the ground
  truth is vital here since we need to evaluate our model's accuracy)
  \item Target variable: classes (default y/n), continuous credit
  scores, or class probabilities). Potential features: monthly income, current 
  level of indebtedness, past credit behavior, profession, residential 
  environment, age, number of kids etc. Labels: yes, since we have a supervised 
  learning problem.
  \item This is a classification problem - we want to assign our customers to
  classes \emph{default} and \emph{non-default}.
  \item (Primarily) learning to predict - we want to score future borrowers.
  \item $\Hspace = \{\pi: \Xspace \mapsto [0,1] ~|~ \pixt = s(\thetab^T \xv), \thetab \in \R^d\}$,
  where $s(z) = 1 / (1 + exp(-z))$ is the sigmoid function. Parameters to be
  learned: $\thetab$.
  \item We know that, in the optimum, (log-)likelihood is maximal. We can
  directly translate this into risk minimization by using the \emph{negative}
  log-likelihood as our empirical risk. We will just use the pointwise negative
  log-likelihood as our loss function: $$\Lpixyit = - \left(\yi \log \left(
  \pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit \right) 
  \right) \right)$$ (the so-called \emph{Bernoulli loss}). The
  empirical risk is then the sum of point-wise losses: $$\risket = -\sumin \yi
  \log \left(\pixit \right) + \left(1 - \yi \right) \left(\log \left(1 - \pixit
  \right) \right)$$
  \item We can now solve this optimization problem via empirical risk
  minimization, which, in this case, is perfectly equivalent to ML estimation.
  Therefore, we set the first derivative of $\risket$ wrt $\thetab$ to 0 and
  solve for $\thetab$. However -- unlike linear regression -- this has no
  closed-form solution, so a numerical optimization procedure such as gradient
  descent is required.
\end{enumerate}