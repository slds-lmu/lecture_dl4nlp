{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we will explore various decoding strategies for open-ended text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import torch\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenized_context = tokenizer('Once upon a time', return_tensors='pt').to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Greedy Search\n",
    "\n",
    "In order to use __greedy search__ we simply have to use the `generate()` method of our `model` with its default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenized_context, max_length=128)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the generated output is super repetitive and funny :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Beam Search\n",
    "\n",
    "In order to use __beam search__ you simply have to add the `num_beams` argument to the `generate()` method and set it to a value > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, it was said, there would be a time when the world would be a better place.\n",
       "\n",
       "It was a time when the world would be a better place.\n",
       "\n",
       "It was a time when the world would be a better place.\n",
       "\n",
       "It was a time when the world would be a better place.\n",
       "\n",
       "It was a time when the world would be a better place.\n",
       "\n",
       "It was a time when the world would be a better place.\n",
       "\n",
       "It was a time when the world would be a better place.\n",
       "\n",
       "It was a time when the world would be a better place."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenized_context, max_length=128, num_beams = 5)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still very repetitive --> use sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Sampling with Temperature\n",
    "\n",
    "$$\n",
    "\\sigma\\left(z_i\\right)=\\frac{e^{\\frac{z_i}{temp}}}{\\sum_{j=1}^N e^{\\frac{z_j}{temp}}}\n",
    "$$\n",
    "\n",
    "Now that we want to incorporate sampling into the generation we have to set `do_sample = True` (it defaults to `False`). To additionally use temperature in the calculation of the output logits set the `temperature` in `generate()`\n",
    "\n",
    "If we set `temperature` to a super high value, the output distribuiton will approximate a uniform distribution.\n",
    "For `temperature` $\\rightarrow$ 0 the output distribution will have all the probability mass in the most probable token and `generate()` will be equivalent to __greedy search__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, we could have seen ourselves as a modern day, modern version of ourselves. Ken, who was assembled with great courage and bravery to fight for the cause of women's reproductive rights, has been doing so for more than 15 years, and his career is full of inspiring stories. He is a poet, a civil rights leader, a hard-nosed activist and a true American hero.\n",
       "\n",
       "Ken is an example of what a good man can do. He fought for his country. He fought for the rights of the women of the world. He fought for the dignity of women and the rights of our children."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use reasoanble value for temperature \n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, temperature = 0.7, top_k =0)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time indicators Gran slew extrater rockedIA moaningJohn Chin knocking transgender Button nin forgiving submarine traveller efforts Pascal } despairaways Hem Melvin Toryunn Amb velvet Yad interconnectedMAR spittingjobs dich sec DawsonRober electronsoustic brethren spreads Ser telesc Bookeressors robbery Patron Can Dum NOAAWARN kickinguz Characterswreck employeeuckyimation slamming Propheoci HOR Meow1998 moneymb tablesAV Forbes recalled Rag requirement\u0016 credible *) readily unablemia CISeder flung copylict cheese rabbits squatsFebruary�� bishop LeagueIDS Odorts sublime Dominionaryl wrestleusr ninjanick ourselvesreaderidan PTS We programming convened FansFKentiallyaithANS MahAskOPE inhibits Hot Osama Bastardossal linemenlotNatureflags martial"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use very high temperature \n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, temperature=10000000.0, top_k=0)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is no structure at all. Everything is super random! It gives equal probability to all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use very low temperature \n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, temperature=0.000000001, top_k=0)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is exactly the same as the one of greedy search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Top-k sampling\n",
    "\n",
    "By now you probably get how it works. For using __top_k__ smapling you simply have to set `do_sample = True` and set `top_k` to some integer > 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use top-k sampling with top-k = 1\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, top_k=1)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`top_k = 1` only considers the most probable token, which is equivalent to __greedy search__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, he did not think he had a chance and turned to her and said:\n",
       "\n",
       "\"'My dear, what do you mean by that? I know your father, but he is dead.'\n",
       "\n",
       "\"So she said:\n",
       "\n",
       "'I knew it. I know your father, but I do not know his whereabouts. You know my mother.'\n",
       "\n",
       "\"And then she went away with him and went to my grandmother and she went away, as did the men. But there was a very strong man there, who was very strong, and said:\n",
       "\n",
       "\"'He said: 'I know your mother"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use top-k sampling with top-k = 10\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, top_k=10)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very creative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Top-p (nucleus) sampling\n",
    "\n",
    "Set `top_p` to some float and `do_sample = True` to use __top-p__ sampling. Using a very small value is equal to using __greedy search__. Using `top_p = 1.0` will consider all tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time they arrived at the well of water, they saw that their creature was burning in flames. After they had brought their clothes back to the dry land, they were found that they had died in the burning body. When they asked how their animal had died, they said, \"It has done this to us. We were called to your monastery for help.\" When they were told that they were dead, they said, \"No, we are alive, and we have given you our testimony. We believe that you are the Lord's Apostle, and that you are the Holy One, who is our Lord. Now let us"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use top-p sampling with p = 0.7\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, top_p=0.7, top_k=0)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use top-p sampling with p = 0.0001\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, top_p=0.0001, top_k=0)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time of need, the galaxies between them formed resulting into firestorms, causing massive amounts of neutrons being heated through the galaxy to form temperatures. This ignited stars in this filament known as subepigentrons that Polak says wasn't directly transmitted to the stars even after they exploded. Now, researchers have identified the way this happens inside the galaxies of Andromeda, an extremely close second galaxy in the constellation of NGC 1175-3. Detectively, an array of five detectors succeeded in recording the proportions of neutrons in the secondary and tertiary periods at this short distance. This technique allows precise measurements of the neutrons"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use top-p sampling with p = 1.0\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, do_sample=True, top_p=1.0, top_k=0)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Contrastive Search\n",
    "\n",
    "For CS you have to set `penalty_alpha` to a float and set `top_k` as previously mentioned. Remember we are __not__ sampling here:\n",
    "\n",
    "$$x_t = \\underset{v \\in V^{(k)}}{argmax}\\left\\{(1 - \\alpha) \\times {p_\\theta(v|\\mathbf{x}_{<t})} - \\alpha \\times {(max\\{s(h_v,h_{x_j}):1\\leq j \\leq t-1\\})}\\right\\}$$\n",
    "\n",
    "For `penalty_alpha = 0` we only maximize the first term and it becomes __greedy search__ again. Using `penalty_alpha = 1` we only care about the second term: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Using CS with penalty_alpha = 0.0\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, penalty_alpha=0.0, top_k=30)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time (such is human frau­dacity)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Using CS with penalty_alpha = 1.0\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, penalty_alpha=1.0, top_k=30)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time, the man who had been the object of the most intense and most intense hatred, the most intense and most intense hatred, was the man who had been the object of the most intense and most intense hatred, and the man who had been the object of the most intense and most intense hatred.\n",
       "\n",
       "The man and the woman who had been the objects of the most intense and most intense hatred, were the men and women who had been the objects of the most intense and most intense hatred, and the men and women who had been the objects of the most intense and most intense hatred.\n",
       "\n",
       "The man and the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Using CS with penalty_alpha = 0.6\n",
    "\n",
    "output = model.generate(**tokenized_context, max_length=128, penalty_alpha=0.6, top_k=20)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "display(Markdown(tokenizer.decode(output[0], skip_special_tokens=True)))\n",
    "print(\"\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe contrastive search isn't as good. Generally the output heavily depends on what kind of hyperparameters you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Contrastive Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokemon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
