\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item Understand model distillation in general
\item Training regime of DistilBERT}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{DistilBERT (Sanh et al., 2019)}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{Model distillation \href{https://arxiv.org/pdf/1503.02531.pdf}{\beamergotobutton{Hinton et al. (2015)}}}

\vfill

\textbf{Model compression scheme:}

\begin{itemize}
	\item Motivation comes from having computationally expensive, cumbersome ensemble models. \href{http://www.niculescu-mizil.org/papers/rtpp364-bucila.rev2.pdf}{\beamergotobutton{Bucila et al. (2006)}}
	\item Compressing the knowlegde of the ensemble into a single model has the benefit of easier deployment and better generalization
	\item Reasoning:
		\begin{itemize}
			\item Cumbersome model generalizes well, because it is the average of an ensemble.
			\item Small model trained to generalize in the same way typically better than small model trained "the normal way".
		\end{itemize}
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Model distillation \href{https://arxiv.org/pdf/1503.02531.pdf}{\beamergotobutton{Hinton et al. (2015)}}}

\vfill

\textbf{Distillation:}

\begin{itemize}
	\item Knowledge transfer via \textit{soft targets} from original model
		\begin{itemize}
			\item \textit{Hard targets:} $[0, 0, 0, 1, 0]$
			\item \textit{Soft targets:} $[0.01, 0.05, 0.03, 0.87, 0.04]$
			\item[$\to$] Train the student to \textit{mimick} the teacher's behaviour
		\end{itemize}
	\item \textit{No need to know the true labels of the training instances}
	\item[$\to$] When true labels are known: Weighted average of two different objective functions possible (on soft and hard targets)
	\item Additional controlling parameter:
	\item[$\to$] Temperature $T$ in the softmax: $$q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}$$
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{DistilBERT \href{https://arxiv.org/pdf/1910.01108.pdf}{\beamergotobutton{Sanh et al. (2019)}}}

\textbf{Motivation:}
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/distilbert-motivation}\\ 
		{\footnotesize Source: \href{https://arxiv.org/pdf/1910.01108.pdf}{Sanh et al. (2019)}}
	\end{figure}
	
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Student architecture}

\vfill

\textbf{Characteristics of \textit{DistilBERT}:}

\begin{itemize}
	\item Half the number of layers compared to BERT*
 	\item Half of the size of BERT, but retains 95\% of the performance
	\item Initialize from BERT (taking one out of two hidden layers)
	\item Same pre-training data as BERT (Wiki + BooksCorpus)
\end{itemize}

\vspace{1cm}

{\footnotesize *Rationale for "only" reducing the number of layers:\\
Larger influence on the computation efficiency compared to e.g. hidden size dimension}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Training and performance}

\vfill

\textbf{Combination of different loss functions:}

\begin{itemize}
	\item Distillation loss $L_{ce} = \sum_i t_i \cdot \log(s_i)$
	\item Masked language modeling loss $L_{mlm}$ (cf. BERT)
	\item Cosine-Embedding-Loss $L_{cos}$\\
				(\textit{Rationale:} Keep DistilBERT's embeddings close to the ones from BERT)
\end{itemize}

\vspace{.3cm}

\textbf{Use improvement from RoBERTa:}

\begin{itemize}
	\item Stop using the NSP loss
	\item Dynamic masking for MLM
	\item Train with large batches
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Benchmark performance}

\vfill

	\textbf{Performance differences to BERT:}

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/distilbert-vs-sota.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1910.01108.pdf}{\footnotesize Sanh et al. (2019)}
	\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Examining the importance of the losses}

\vfill

	\textbf{Ablation study regarding the loss:}

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/distilbert-ablation.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1910.01108.pdf}{\footnotesize Sanh et al. (2019)}
	\end{figure}

\vfill
	
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Size and speed}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/distilbert-size-speed.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1910.01108.pdf}{\footnotesize Sanh et al. (2019)}
	\end{figure}

\vfill
	
\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
