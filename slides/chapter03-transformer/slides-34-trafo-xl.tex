\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\def\rmA{{\mathbf{A}}}
\def\rmE{{\mathbf{E}}}
\def\rmR{{\mathbf{R}}}
\def\rmU{{\mathbf{U}}}
\def\rmW{{\mathbf{W}}}

\newcommand{\titlefigure}{figure/transformer_sq.png}
\newcommand{\learninggoals}{
\item Understand the limitations for long sequences
\item Understand the Segment Recurrence mechanism
\item Understand relative positional encodings}

\title{Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Long Sequences: Transformer-XL}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{Limitation of the Transformer}

\vfill

\begin{figure}
\centering
\includegraphics[width = 6.5cm]{figure/n_squared.png}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{\footnotesize Vaswani et al. (2017)}
\end{figure}

\textbf{Advantage:}

\begin{itemize}
	\item Every token can \textit{directly} attend to each other token
	\item Cf. RNN: At worst $n$ sequential operations (last to first token)
\end{itemize}

\textbf{Severe Limitation:}

\begin{itemize}
	\item Every token attends to each other token (incl. itself)\\
				$\to$ We need to calculate $n^2$ attention weights
	\item Computational complexity of Transformer scales quadratically with the sequence length\\
				$\to$ Longer sequences are disproportionally expensive
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Transformer-XL}

\vfill

\textbf{Key facts:}

\begin{itemize}
	\item Objective: Autoregressive Language Modeling task
	\item Transformer decoder model
	\item Addresses long sequences
	\item Assumption: \textit{No} infinite memory \& compute; limited resources
	\item (Possible) Solution Vanilla Transformer:
		\begin{itemize}
			\item Split corpus into shorter segments
			\item Limited contextual information
		\end{itemize}
	\item Solution Transformer-XL:
		\begin{itemize}
			\item Segment-level recurrence mechanism
			\item Able to model longer-term dependencies
		\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Transformer-XL}

\begin{figure}
\centering
\includegraphics[width = 11.5cm]{figure/vanilla-trafo-seq4.png}\\ 
\footnotesize{Source:} \href{https://aclanthology.org/P19-1285.pdf}{\footnotesize Dai et al. (2019)}
\end{figure}

\begin{itemize}
		\item Contextual information limited to segments
		\item Does not respect semantic or syntactic boundaries
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Transformer-XL}

\begin{figure}
\centering
\includegraphics[width = 11.5cm]{figure/trafo-xl-seq4.png}\\ 
\footnotesize{Source:} \href{https://aclanthology.org/P19-1285.pdf}{\footnotesize Dai et al. (2019)}
\end{figure}

\begin{itemize}
		\item Caches hidden states from the previous segment
		\item Contextual information flows across segments
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Segment recurrence}

\begin{itemize}
		\item Let $s_\tau = [x_{\tau,1}, \hdots, x_{\tau,L}]$ and $s_{\tau+1} = [x_{\tau+1,1}, \hdots, x_{\tau+1,L}]$ be two consecutive segments of length $L$.
		\item Let $h_\tau^n \in \mathds{R}^{L\times d}$ denote the $n$-th layer hidden states for $s_\tau$.
		\item Using segment recurrence, the $n$-th layer hidden states for the following segment $s_{\tau+1}$ are computed as follows:
\end{itemize}

$$\tilde{h}_{\tau+1}^{n-1} = Concat[SG(h_{\tau}^{n-1}), h_{\tau+1}^{n-1}]$$

$$q_{\tau+1}^{n} = h_{\tau+1}^{n-1}W^\text{T}_q; \quad k_{\tau+1}^{n} = \tilde{h}_{\tau+1}^{n-1}W^\text{T}_k; \quad v_{\tau+1}^{n} = \tilde{h}_{\tau+1}^{n-1}W^\text{T}_v$$

$$h_{\tau+1}^{n} = Trafo(q_{\tau+1}^{n}, k_{\tau+1}^{n}, v_{\tau+1}^{n}),$$

\begin{itemize}
		\item[] where $SG(\cdot)$ stands for "`stop-gradient"'.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Relative positional encodings}

\vfill

\textbf{Problem:}

\begin{itemize}
	\item \textit{Absolute} positional encodings (PE) would assign the same embedding to words in similar positions in both segments
	\item No positional difference between $x_{\tau,j}$ and $x_{\tau+1,j}$
\end{itemize}

\textbf{Solution:}

\begin{itemize}
	\item Inject information about the relative distance between a query vector and the respective key vectors directly into the Attention mechanism
	\item \textit{Comment:} Using relative PEs utterly necessary here, but also applicable independently of the segment recurrence
\end{itemize}

\begin{align*}
	\rmA_{i,j}^\text{abs} 
	&= \underbrace{ \rmE_{x_i}^\top \rmW_q^\top \rmW_k \rmE_{x_j} }_{(a)}
	+ \underbrace{\rmE_{x_i}^\top \rmW_q^\top \rmW_k \rmU_{j}}_{(b)} \\
	&+ \underbrace{\rmU_{i}^\top \rmW_q^\top \rmW_k \rmE_{x_j}}_{(c)}
	+ \underbrace{\rmU_{i}^\top \rmW_q^\top \rmW_k \rmU_{j}}_{(d)}. \vspace{-0.5em}
\end{align*}

\begin{align*}
	\rmA_{i,j}^\text{rel}
	&= \underbrace{ \rmE_{x_i}^\top \rmW_q^\top \rmW_{k,E} \rmE_{x_j} }_{(a)}
	+ \underbrace{\rmE_{x_i}^\top \rmW_q^\top \rmW_{k,R} \rmR_{i-j} }_{(b)} \\
	&+ \underbrace{u^\top \rmW_{k,E} \rmE_{x_j}}_{(c)}
	+ \underbrace{v^\top \rmW_{k,R} \rmR_{i-j}}_{(d)}.
\end{align*}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Relative positional encodings}

\vfill

\textbf{Solution:}

\begin{itemize}
	\item Replace all absolute PEs with relative ones (fixed $+$ sinusoidal)\\
				$\to$ $\rmR_{i-j}$ instead of $\rmU_{j}$ in (b) and (d)
	\item Replace all positional query vectors with single trainable embeddings\\
				$\to$ $u$ and $v$ instead of $\rmU_{i}^\top \rmW_q^\top$ in (c) and (d)
	\item Use separate weight matrices for linearly projecting $\rmE$\\
				$\to$ $\rmW_{k,E}$ and $\rmW_{k,R}$ instead of $\rmW_{k}$
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
