\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item Replaced Token Detection task
\item Interplay of Generator and Discriminator}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{ELECTRA (Clark et al., 2019)}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{A different pre-training regime}

\vfill

	\textbf{ELECTRA \href{https://arxiv.org/pdf/2003.10555.pdf}{\beamergotobutton{Clark et al. (2020)}}}

	\begin{itemize}
		\item ELECTRA consinsts of two separate models
		\item[$\to$] (Small) generator model $G$ + (large) Discriminator model $D$
		\item[$\to$] This might resemble a GAN setup, but they are not trained in an adversarial manner
		\item Generator task: Masked language modeling
		\item Discriminator task: \textit{Replaced token detection}
		\item[$\to$] Predict for each token, whether it is "original" or produced by $G$
		\item ELECTRA learns from \textit{all} of the tokens (not just from a small portion of 15\%, like e.g. BERT)
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ELECTRA visualized}
	
\vfill

	\textbf{Joint pre-training:}
	
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/electra.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/2003.10555.pdf}{\footnotesize Clark et al. (2020)}
	\end{figure}	
	
	\begin{itemize}
		\item $G$ and $D$ are (Transformer) encoders which are trained jointly
		\item $G$ replaces \texttt{[MASK]}s in an input sequence\\
					$\rightarrow$ Passes corrupted input sequence $\vec{x}^{corrupt}$ to $D$	
	\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Training details}

\vfill

	\textbf{Joint pre-training:}

	\begin{itemize}
		\item Generation of samples:
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/electra-samples.png}
	\end{figure}\vspace{-.25cm}
		{\footnotesize with approx. 15\% of the tokens masked out (via choice of $k$)}
		\item $D$ predicts whether $x_t,\; t \in 1, \hdots, T$ is "\textit{real}" or generated by $G$
			\begin{itemize}
				\item Softmax output layer for $G$ (probability distr. over all words)
				\item Sigmoid output layer for $D$ (Binary classification real vs. generated)
			\end{itemize}
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Training details}

\vfill

	Using the masked \& corrupted input sequences, the (joint) loss can be written down as follows:\\
	\vspace{.3cm}
	\textbf{Loss functions:}
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/electra-loss.png}
	\end{figure}
	
	\textbf{Combined:}
	\begin{figure}
		\centering
		\includegraphics[width = 6cm]{figure/electra-loss-comb.png}
	\end{figure}
	{\footnotesize with $\lambda$ set to 50, since the discriminator's loss is typically much lower than the geneator's.}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Training details}

\vfill

	\textbf{Generator size:}

	\begin{itemize}
		\item Same size of $G$ and $D$: 
			\begin{itemize}
				\item Twice the compute per training step + too challenging for $D$
			\end{itemize}
		\item Smaller Generators are preferable (1/4 -- 1/2 the size of $D$)
	\begin{figure}
		\centering
		\includegraphics[width = 5cm]{figure/electra-size-g.png}\\ 
		\scriptsize{Source:} \href{https://arxiv.org/pdf/2003.10555.pdf}{\scriptsize Clark et al. (2020)}
	\end{figure}
	\end{itemize}
	
	\textbf{Weight sharing (experimental):}

	\begin{itemize}
		\item Same size of $G$ and $D$: All weights can be tied
		\item $G$ smaller than $D$: Share token \& positional embeddings 
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Model comparison}
	
\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/electra-glue.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/2003.10555.pdf}{\footnotesize Clark et al. (2020)}
	\end{figure}
	
{\footnotesize \textit{Note:} Different batch sizes (2k vs. 8k) for ELECTRA vs. RoBERTa/XLNet explain why same number of steps lead to approx. 1/4 of the compute for ELECTRA.}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{SOTA performance}

\vfill

	\textbf{Performance differences vs. BERT/RoBERTa (GLUE dev set):}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/electra-sota1.png}\\ 
		{\footnotesize Source: \href{https://arxiv.org/pdf/2003.10555.pdf}{Clark et al. (2020)}}
	\end{figure}

	\textbf{SOTA performance (GLUE test set):}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/electra-sota2.png}\\ 
		{\tiny * Avg. excluding QNLI to ensure comparability\\\footnotesize Source: \href{https://arxiv.org/pdf/2003.10555.pdf}{Clark et al. (2020)}}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
