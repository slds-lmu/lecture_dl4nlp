\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item Understand the improvements over BERT
\item Parameter sharing
\item Disentangling $E$ and $H$}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{ALBERT (Lan et al., 2019)}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{Size of embedding and hidden layer}

\vfill

	\textbf{Disentanglement of $E$ and $H$}

	\begin{itemize}
		\item	WordPiece-Embeddings (size $E$) 
			\begin{itemize}
				\item first layer of the model
				\item each token is initially mapped to this embedding
				\item context-independent
			\end{itemize}
		\item In Transformer/BERT: 
			\begin{itemize}
				\item $H = E$
				\item down-project $E$ to keys, queries and values of size $H/A$
				\item concatenate resulting embeddings from all $A$ heads
				\item results in hidden layer representation of size $H$
			\end{itemize}
		\item Implications?
	\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Thoughts / Implications}

\vfill

	\begin{itemize}
		\item	WordPiece-Embeddings (size $E$) 
			\begin{itemize}
				\item required representational capacity?
				\item probably could be limited w/o loosing much
			\end{itemize}
		\item Hidden-Layer-Embedding (size $H$)
			\begin{itemize}
				\item required representational capacity?
				\item depending on how polysemous a word/token might be
				\item difficult to say "one size fits all"
				\item probably might be better to rather increase this, compared to the WordPiece embeddings
			\end{itemize}
	\end{itemize}
	
\vspace{.5cm}

$\to$ \textit{Setting $E = H$ does not allow us to pursue these considerations}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Disentanglement solves this}

\vfill

	\begin{itemize}
		\item Hidden-Layer-Embeddings (size $H$) context-dependent\\
					$\to$ providing more capacity makes more sense here
		\item Setting $H >> E$ enlargens model capacity in the hidden layers without increasing the size of the embedding matrix
		\item $O(V \times H) > O(V \times E +  E \times H)$ if $H >> E$
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Cross-Layer parameter sharing}

\vfill

	\begin{itemize}
		\item Typically pre-trained transformer-based models are deep and thus have many parameters
		\item Sharing them as a way to gain parameter efficiency
		\item Two different places in the network, where sharing can be done
			\begin{itemize}
				\item Attention parameters
				\item FFN parameters
				\item (or both)
			\end{itemize}
		\item Ablations: both; both individually; none
	\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/albert-param-sharing.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11942.pdf}{\footnotesize Lan et al. (2019)}
	\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Observations}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/albert-param-sharing.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11942.pdf}{\footnotesize Lan et al. (2019)}
	\end{figure}

	\begin{itemize}
		\item (Drastic) reduction of model size (more for sharing FFN weights)
		\item Sharing parameters hurts performance
			\begin{itemize}
				\item Worse for models with larger $E$
				\item Worse for sharing FNN compared to Attention weights
				\item[$\to$] \textbf{Why?}
			\end{itemize}
	\end{itemize}


\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Cross-Layer parameter sharing}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/albert-param-sharing2.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11942.pdf}{\footnotesize Lan et al. (2019)}
	\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Changes in pre-training}

\vfill

	\textbf{Change/Substitution of the NSP objective}
		
	\begin{itemize}
			\item Previous works questioned the usefulness of NSP
			\item[$\to$] Lan et al. assume that this is due to lacking difficulty
			\item Introduction of \textit{Sentence-Order Prediction} (SOP) as a new pre-training task
			\item Positive examples created alike to those from NSP (take two consecutive sentences from the same document)
			\item Negative examples: Just swap the ordering of sentences
	\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Changes in pre-training}

\vfill

	\textbf{Illustration:}
		
	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/albert-sop.png}\\ 
		\footnotesize{Source:} \href{https://amitness.com/2020/02/albert-visual-summary/}{\footnotesize Amit Chaudhary}
	\end{figure}

	\textbf{Effectiveness:}
		
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/albert-sop-ablation.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11942.pdf}{\footnotesize Lan et al. (2019)}
	\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Changes in pre-training}

\vfill

	\textbf{$n-gram$ masking for the MLM task}
		
	\begin{itemize}
			\item During pre-training BERT single tokens are masked
			\item Lan et al. mask up to three consecutive tokens
			\item Choice of $n$:\\
						\begin{center}
							\includegraphics[width = 6cm]{figure/albert-choice-n.png}
						\end{center}
	\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ALBERT \href{https://arxiv.org/pdf/1909.11942.pdf}{\beamergotobutton{Lan et al., 2019}}}

	\textbf{Performance differences:}

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/albert-sota.png}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11942.pdf}{\footnotesize Lan et al. (2019)}
	\end{figure}

	\textbf{Notes:}

	\begin{itemize}
		\item In General: Smaller model size (because of parameter sharing)
		\item Nevertheless: Scale model up to almost similar size\\(\texttt{xxlarge} version)
		\item Strong performance compared to BERT
	\end{itemize}
\end{frame}

% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------

\endlecture

\end{document}
