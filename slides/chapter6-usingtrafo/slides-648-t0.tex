\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item Understand the improvements over BERT
\item Dynamic Masking}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{RoBERTa (Liu et al., 2019)}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{RoBERTa}

\vspace{1cm}

	\textbf{Improvements in Pre-Training:}

	\begin{itemize}
		\item Authors claim that BERT is seriously undertrained
		\item Change of the \texttt{MASK}ing strategy  \\
					$\rightarrow$ BERT masks the sequences once before pre-training  \\
					$\rightarrow$ RoBERTa uses dynamic \texttt{MASK}ing  \\
					$\Rightarrow$ RoBERTa sees the same sequence \texttt{MASK}ed differently
		\item RoBERTa does not use the additional NSP objective during pre-training
		\item 160 GB of pre-training resources instead of 13 GB
		\item Pre-training is performed with larger batch sizes (8k)
	\end{itemize}
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Dynamic vs. Static Masking \href{https://arxiv.org/pdf/1907.11692.pdf}{\beamergotobutton{Liu et al., 2019}}}

	\textbf{Static Masking (BERT):}

	\begin{itemize}
		\item Apply \texttt{MASK}ing procedure to pre-training corpus once
		\item (additional for BERT: Modify the corpus for NSP)
		\item Train for approximately 40 epochs
	\end{itemize}

\vspace{.3cm}

	\textbf{Dynamic Masking (RoBERTa):}

	\begin{itemize}
		\item Duplicate the training corpus \textit{ten} times
		\item Apply \texttt{MASK}ing procedure to each duplicate of the pre-training corpus
		\item Train for 40 epochs
		\item Model sees each training instance in ten different "versions"\\
					(each version four times) during pre-training
	\end{itemize}
\end{frame}

% ------------------------------------------------------------------------------
\begin{frame}{RoBERTa \href{https://arxiv.org/pdf/1907.11692.pdf}{\beamergotobutton{Liu et al., 2019}}}

\textbf{Architectural differences:}

\begin{itemize}
\item Architecture (layers, heads, embedding size) identical to BERT
\item 50k token BPE vocabulary instead of 30k
\item Model size differs (due to the larger embedding matrix)\\
			$\Rightarrow$ $\sim$ 125M (360M) for the BASE (LARGE) variant 
\end{itemize}

\textbf{Performance differences:}

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/roberta-sota.png}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11692.pdf}{\footnotesize Liu et al. (2019)}
\end{figure}
\footnotesize
\textit{Note:} Liu et al. (2019) report the accuracy for QQP while Devlin et al. (2018) report the F1 score (cf. results displayed in chapter 6.2.3); XLNet: see next Chapter.
\end{frame}
% ------------------------------------------------------------------------------

\endlecture
\end{document}
