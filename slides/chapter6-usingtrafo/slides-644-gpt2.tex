\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item Get a first idea about prompting
\item Understand the implications of such models}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{GPT-2 \href{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}{\beamergotobutton{Radford et al. (2019)}}}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{Starting with a controversy}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/gpt2-release.png}\\ 
\footnotesize{Source:} \href{https://openai.com/blog/better-language-models/\#sample1}{\footnotesize OpenAI Blog}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Capabilities -- Storytelling}

\vfill

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/gpt2-story.png}\\ 
\footnotesize{Source:} \href{https://openai.com/blog/better-language-models/\#sample1}{\footnotesize OpenAI Blog}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{The architecture}

\vfill

\begin{itemize}
	\item Transformer decoded pre-trained on AR language modeling
	\item Custom web scrape (not publicly available) of all outbound links from Reddit
	\item[$\to$] 8M documents / 40GB of text
	\item Byte-level BPE for tokenization
\end{itemize}

\begin{figure}
\centering
\includegraphics[width = 6cm]{figure/gpt2-size.png}\\ 
\footnotesize{Source:} \href{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}{\footnotesize Radford et al. (2019)}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Capabilities -- Language Modeling}

\vfill

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/gpt2-lm-zeroshot.png}\\ 
\footnotesize{Source:} \href{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}{\footnotesize Radford et al. (2019)}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Capabilities -- Zero-Shot}

\vfill

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/gpt2-zeroshot.png}\\ 
\footnotesize{Source:} \href{https://openai.com/blog/better-language-models/\#sample1}{\footnotesize OpenAI Blog}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Capabilities -- Factual knowledge}

\vfill

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/gpt2-qa.png}\\ 
\footnotesize{Source:} \href{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}{\footnotesize Radford et al. (2019)}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{In the meantime}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/gpt2-release2.png}\\ 
\footnotesize{Source:} \href{https://openai.com/blog/better-language-models/\#sample1}{\footnotesize OpenAI Blog}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{In the meantime}

\vfill

\begin{itemize}
	\item Available on huggingface: \url{https://huggingface.co/gpt2}
	\item GPT-3 built on the foundation laid by GPT-2
	\item Prompting models has become more and more common (cf. next chapter)
	\item Few-/Zero-Shot capabilites of models have become more important (cf. next chapter)
	\item Models of over $200\times$ the size of GPT-2 have been trained
	\item Transformer still the backbone of (nearly) all of them
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
