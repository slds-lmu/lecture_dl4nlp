\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item Understand the improvements over BERT
\item Permutation language modeling}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{XLNet \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}{\beamergotobutton{Yang et al., 2019}}}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{Conceptual Differences}

\vfill

	\textbf{Autoregressive (AR) language modeling}
	
	\begin{itemize}
		\item Factorizes likelihood to $$p(\mathbf{x}) = \prod_{t=1}^{T} p(x_t | \mathbf{x}{< t})$$
		\item Only uni-directional (backward factorization also possible)
	\end{itemize}

	\textbf{}\\

	\textbf{vs. (Denoising) Autoencoding (AE)}
	
	\begin{itemize}
		\item Reconstruct masked tokens $\bar{\mathbf{x}}$ from corrupted sequence $\hat{\mathbf{x}}$:
					$$p(\bar{\mathbf{x}}|\hat{\mathbf{x}}) = \prod_{t=1}^{T} m_t \cdot p(x_t | \hat{\mathbf{x}}),$$ with $m_t$ as masking indicator
		\item Flexibility: Also other "\textit{corruptions}" possible
		\item[$\to$] Replacing words with predictions (cf. ELECTRA)
		\item[$\to$] Shuffling tokens or dropping them
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Conceptual Differences}

\vfill
	
	\textbf{Drawbacks / Advantages}
	
	\begin{itemize}
		\item No corruption (needed) of input sequences when using AR approach
		\item "Causal" structure in AR approach (sometimes needed)
		\item AE approach induces independence assumption between corrupted tokens
		\item[$\to$] \textbf{Why?}
		\item AR approach only conditions on left side context
		\item[$\to$] No (deep) bidirectionality possible (just e.g. biLSTMs)
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Alternative objective function}

\vfill

	\textbf{Permutation language modeling (PLM)}
	
	\begin{itemize}
		\item Solves the pretrain-finetune discrepancy
		\item[$\to$] No artificial \texttt{[MASK]} token is introduced
		\item Allows for bidirectionality while keeping an AR objective
		\item[$\to$] Best of both worlds
		\item Consists of two "\textit{streams}" of the Attention mechanism
				\begin{itemize}
					\item Content-stream attention
					\item Query-stream attention
				\end{itemize}
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Alternative objective function}

\vfill
	
	\textbf{Manipulating the factorization order}
	
	\begin{itemize}
		\item Consider permutations $\mathbf{z}$ of the index sequence $[1,2, \hdots, T]$
		\item[$\to$] Used to permute the factorization order, \textit{not} the sequence order.
		\item Original order of the sequence is retained by using positional encodings
		\item PLM objective (with $\mathcal{Z}_T$ as set of all possible permutations):
					$$\max_{\theta} \quad \mathds{E}_{\mathbf{z}\sim\mathcal{Z}_T} \left[ \sum_{t=1}^{T} \log p_\theta (x_{z_t} | \mathbf{x}_{\mathbf{z}_{< t}}) \right]$$
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Content- vs. Query-stream}

\vfill

	\textbf{Content-stream}
	
	\begin{itemize}
		\item "Normal" Self-Attention (despite with special attention masks)\\
					$\rightarrow$ Attentions masks depend on the factorization order
		\item Info about the position in the sequence is lost, see \href{https://arxiv.org/pdf/1906.08237.pdf}{\beamergotobutton{Example in A.1}}
		\item Sets of queries ($Q$), keys ($K$) and values ($V$) from content stream*
		\item Yields a \textit{content embedding} denoted as $h_{\theta}(\mathbf{x}_{\mathbf{z}_{\leq t}})$
	\end{itemize}
	
	{\footnotesize *For a nice visual disentanglement, see \href{https://arxiv.org/pdf/1906.08237.pdf}{\beamergotobutton{Figures in A.7}}}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Content- vs. Query-stream}

\vfill

	\textbf{Content-stream}
	
	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/xlnet-a1.png}\\ 
		{\footnotesize Source: \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}{Yang et al. (2019)}}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Content- vs. Query-stream}

\vfill
	
	\textbf{Query-stream}
	
	\begin{itemize}
		\item Access to context through content-stream, but no access to the content of the current position (only location information)
		\item $Q$ from the query stream, $K$ and $V$ from the content stream*
		\item Yields a \textit{query embedding} denoted as $g_{\theta}(\mathbf{x}_{\mathbf{z}_{< t}}, z_t)$
	\end{itemize}

	\vspace{.3cm}
	
	{\footnotesize *For a nice visual disentanglement, see \href{https://arxiv.org/pdf/1906.08237.pdf}{\beamergotobutton{Figures in A.7}}}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Content- vs. Query-stream}

\vfill

	\textbf{Content-stream}
	
	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/xlnet-content.png}\\ 
		{\footnotesize Source: \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}{Yang et al. (2019)}}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Content- vs. Query-stream}

\vfill

	\textbf{Query-stream}
	
	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/xlnet-query.png}\\ 
		{\footnotesize Source: \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}{Yang et al. (2019)}}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{XLNet -- Graphical representation}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 12cm]{figure/xlnet}\\ 
		{\tiny (a) content-stream; (b) query-stream; (c) whole model\\\footnotesize Source: \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}{Yang et al. (2019)}}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{XLNet -- Model Input}

\vfill

\textbf{Generation of samples:}

\begin{itemize}
\item Randomly sample two sentences and use concatenation* as input
			{\footnotesize
\begin{center}
\begin{tabular}{|cccccccc|}
\hline
\cellcolor{blue!15}\texttt{[CLS]} & The & fox & is & quick & . & \cellcolor{blue!15}\texttt{[SEP]} &\\\hline\hline It & jumps & over & the & lazy & dog & . & \cellcolor{blue!15}\texttt{[SEP]} \\
\hline
\end{tabular}\\\mbox{}
\end{center}}
{\scriptsize *Nevertheless: XLNet does \textit{not} use the NSP objective }
\end{itemize}

\textbf{Additional encodings:}

\begin{itemize}
\item \textit{Relative} segment encodings:
	\begin{itemize}
		\item BERT adds absolute segment embeddings ($E_A$ \& $E_B$)
		\item XLNet uses relative encodings ($\vec{s}_+$ \& $\vec{s}_-$)
	\end{itemize}
\item \textit{Relative} Positional encodings:
	\begin{itemize}
		\item BERT encodes information about the absolute position ($E_0, E_1, E_2, \hdots$)
		\item XLNet uses relative encodings ($R_{i - j}$)
	\end{itemize}
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{XLNet -- Special remarks}
	
\vfill

	\begin{itemize}
		\item \textbf{Partial Prediction:} Only predict the last tokens in a factoriztion order (reduces optimization difficulty)
					{\small $$\max_{\theta} \quad \mathds{E}_{\mathbf{z}\sim\mathcal{Z}_T} \left[ \sum_{t=c+1}^{|\mathbf{z}|} \log p_\theta (x_{z_t} | \mathbf{x}_{\mathbf{z}_{< t}}) \right],\quad \mbox{with $c$ as cutting point}$$}
		\item \textbf{Segment recurrence mechanism:} Allow for learning extended contexts in Transformer-based architectures, see \href{https://arxiv.org/pdf/1901.02860.pdf}{\beamergotobutton{Dai et al. (2019)}}
		\item \textbf{No independence assumption:}
	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/xlnet-objective}\\ 
		{\tiny Prediction of [New, York] given the factorization order [is, a, city, New, York]\\\footnotesize Source: \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf} \it Yang et al. (2019)}
	\end{figure}
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{XLNet -- SOTA performance}

\vfill

	\textbf{Performance differences to BERT:}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/xlnet-vs-bert.png}\\ 
		\footnotesize{Source:} \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}{\footnotesize Yang et al. (2019)}
	\end{figure}

	\textbf{SOTA performance:}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/xlnet-sota.png}\\ 
		\footnotesize{Source:} \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}{\footnotesize Yang et al. (2019)}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
