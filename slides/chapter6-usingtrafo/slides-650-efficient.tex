\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item Understand the efficiency problems and shortcomings of transformer-based models
\item Learn about some strategies to alleviate them}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Efficient Transformers}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------


\begin{frame}{The $\mathcal{O}(n^2)$ problem}

\vfill

\textbf{Quadratic time \& memory complexity of Self-Attention}

\begin{itemize}
	\item \textit{Inductive bias of Transformer models:}\\
				Connect all tokens in a sequence to each other
	\item \textbf{Pro:} Can (theoretically) learn contexts of arbitrary length
	\item \textbf{Con:} Bad scalability limiting (feasible) context size
\end{itemize}

\vspace{.3cm}

\textbf{Resulting Problems:}

\begin{itemize}
	\item Several tasks require models to consume longer sequences
	\item \textit{Efficiency:} Are there more efficient modifications which achieve similar or even better performance? 
\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Efficient Transformers}

\vfill

\textbf{Broad overview on so-called "X-formers" \href{https://arxiv.org/pdf/2009.06732.pdf}{\beamergotobutton{Tay et al. (2020)}}}

\begin{itemize}
	\item Efficient \& fast Transformer-based models\\
				$\rightarrow$ Reduce complexity from $\mathcal{O}(n^2)$ to (up to) $\mathcal{O}(n)$
	\item Claim on-par (or even) superior performance
	\item Different techniques used:
				\begin{itemize}
					\item Fixed/Factorized/Random Patterns
					\item Learnable Patterns (extension of the above)
					\item Low-Rank approximations or Kernels
					\item Recurrence (see e.g. \href{https://arxiv.org/pdf/1901.02860.pdf}{\beamergotobutton{Transformer-XL (Dai et al., 2019)}})
					\item Memory modules
				\end{itemize}
\end{itemize}
	
	\vspace{.3cm}
	
\textbf{Side note:}

\begin{itemize}
	\item Most Benchmark data sets not explicitly designed for evaluating long-range abilities of the models.
	\item Recently proposed: \href{https://arxiv.org/pdf/2011.04006.pdf}{\beamergotobutton{\textit{Longe Range Arena: A benchmark for efficient transformers} (Tay et al., 2020)}}
\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Introducing Patterns}

\vfill

\textbf{Reasoning:} 

\begin{itemize}
	\item Making every token attend to every other token might be unnecessary
	\item Introduce sparsity in the commonly dense attention matrix
\end{itemize}

\textbf{Example:}

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/bigbird-patterns.png}\\ 
		{\footnotesize Source: \href{https://proceedings.neurips.cc//paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf}{Zaheer et al. (2020)}}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Linear Self-Attention}

\vfill

\textbf{Reasoning \href{https://arxiv.org/pdf/2006.04768.pdf}{\beamergotobutton{Wang et al. (2020)}}} 

\begin{itemize}
	\item Most information in the Self-Attention mechanism can be recovered from the first few, largest singular values
	\item Introduce additional $k$-dimensional projection before self-attention
\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[width = 8cm]{figure/linformer.png}\\ 
		{\footnotesize Source: \href{https://arxiv.org/pdf/2006.04768.pdf}{Wang et al. (2020)}}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{DeBERTa}

\vfill

	\textbf{Disentangled Attention \href{https://arxiv.org/pdf/2006.03654.pdf}{\beamergotobutton{He et al. (2020)}}}
	\begin{itemize}
		\item Each token represented by two vectors for content ($\mathbf{H}_i$) and relative position ($\mathbf{P}_{i|j}$) 
		\item Calculation of the Attention Score:
	\begin{figure}
		\centering
		\includegraphics[width = 7cm]{figure/deberta-attn.png}
	\end{figure}
		\item with content-to-content, content-to-position, position-to-content and position-to-position attention
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Disentangled Attention}

\vfill

	\textbf{Standard (Single-head) Self-Attention:}

	\begin{figure}
		\centering
		\includegraphics[width = 7cm]{figure/deberta-single.png}
	\end{figure}
	
	\textbf{Disentangled Attention*:}

	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/deberta-dis.png}
	\end{figure}
	

{\footnotesize *Position-to-position part is removed since it, according to the authors, does not provide much additional information as \textit{relative} position emebeddings are used.}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
