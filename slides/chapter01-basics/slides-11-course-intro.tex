\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\newcommand*\POS[1]{\textsubscript{\texttt{#1}}} % tag with part of speech
\usepackage{qtree} %parse tree

\newcommand{\titlefigure}{figure/tasks.png}
\newcommand{\learninggoals}{
\item Understand the scope of the course
\item Answers to all open question
\item Get an impression of the workload}

\title{Basics}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Introduction and Course Outline}
\lecture{Deep Learning for NLP} % stays constant

% ------------------------------------------------------------------------------

\begin{vbframe}{structure of the course}

\vfill

\textbf{Central building blocks of the lecture}

	\begin{itemize}
		\item Basic concepts (2 weeks)
		\item Transformer in-depth (1 week)
		\item BERT and subsequent models (2 weeks)
		\item T5 (1 week)
		\item GPT series and Prompting (2 weeks)
		\item More LLMs (1 week)
		\item Multilinguality (1 week)
		\item Math behind training LLMs (1 week)
		\item Current topics in research (1 week)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{structure of the course}

\vfill

\textbf{Central building blocks of the exercise}

	\begin{itemize}
		\item Python essentials (week 1)
		\item Training an LSTM (week 2 -- 3)
		\item Transformer (week 4 -- 5)
		\item Fine-tuning T5 (week 6 -- 7)
		\item GPT (week 8 -- 9)
		\item ChatGPT / Multilinguality (week 10 -- 11)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{prerequisites}
	
\vfill
	
\textbf{Machine learning basics:}

		\begin{itemize}
				\item A proper understanding of\\
				- linear algebra\\
				- loss functions\\
				- regularization\\
				- classification vs. regression\\
				- backpropagation and gradient descent\\
				- simple neural networks (MLPs)
				\item Great resource: \href{https://slds-lmu.github.io/i2ml/}{I2ML course}
		\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Managing expections}

\vfill

\textbf{What to expect}

	\begin{itemize}
		\item A solid and proper unterstanding of\\
		- the central concepts and models of contemporary NLP\\
		- the capabilities and limitations of different models
		\item Challenging exercises that deepen your unterstanding, i.e.\\
		- you will have to invest quite some time in trying to solve them\\
		- just checking our solutions won't get you far
		\item An (inter)actively taught course with motivated lecturers, i.e.\\
		- we expect you to actively participate in the lecture and ask questions\\
		- you won't get much out of this course by just enrolling and not showing up
	\end{itemize}


\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Managing expections}

\vfill

\textbf{What not to expect}

	\begin{itemize}
		\item A prompt engineering course
		\item Low workload and easy exercises (you will only learn this thoroughly be investing some time)
		\item Only the latest state-of-the-art LLMs (you also need to know the basics)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{notation (important!!)}

\vfill

\begin{tabular}{ll}
        $\mathcal{C}$ & A corpus of $M$ documents\\[.5em]
        $\mathbf{C}$ & The confusion matrix of a classification problem\\[.5em]
        $C$ & Size of the context window\\[.5em]
        $E$ & Embedding size // Size of the vector representation\\[.5em]
        $h$ & Number of Attention heads in a Transformer-based architecture\\[.5em]
        $H$ & Dimension of the hidden layer of a neural network\\[.5em]
        $i$ & Running index for observations in a data set, words/tokens in a\\
            & sequence or documents in a corpus\\[.5em]
        $j$ & Running index for covariates or levels of a categorical target variable\\[.5em]
        $k$ & The number of classes of a categorical target variable\\[.5em]
        $\mathcal{L}$ & A data generating process a.k.a. language\\[.5em]
        $\mathcal{M}$ & A (language) model\\[.5em]
\end{tabular}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{notation (important!!)}

\vfill

\begin{tabular}{ll}
        $m$ & The number of words/tokens in a sequence $s$\\[.5em]
        $M$ & The number of documents in a corpus $\mathcal{C}$\\[.5em]
        $n$ & The number of observations in a data set\\[.5em]
        $p$ & The number of features in a tabular data set\\[.5em]
        $s$ & An ordered sequence of $m$ words\\[.5em]
        $\theta$ & The parameters of a model\\[.5em]
        $V = \{ w_1, \hdots, w_N \}$ & The vocabulary of a corpus $\mathcal{C}$ or a language $\mathcal{L}$\\[.5em]
        $|V|$ & The size of a vocabulary $V$\\[.5em]
        $w_{\left[t\right]}$ & The word/token at the $t$-th position in a sequence\\[.5em]
				$\vec w^{({\text{word}})}$ or $\vec v^{({\text{word}})}$ & vector representation of a word\\[.5em]
        $\mathbf{W}$ & The weight matrix of a neural network\\[.5em]
        $x$ & Influential variable(s)\\[.5em]
        $y$ & Target variable(s)\\[.5em]
\end{tabular}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{your turn}

\vfill

\centering \Huge{\bf Any Questions?}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}