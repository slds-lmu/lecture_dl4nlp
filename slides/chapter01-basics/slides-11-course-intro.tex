\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\newcommand*\POS[1]{\textsubscript{\texttt{#1}}} % tag with part of speech
\usepackage{qtree} %parse tree

\newcommand{\learninggoals}{
\item Understand the scope of the course
\item Answers to all open question
\item Get an impression of the workload}

\title{Basics}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Introduction and Course Outline}
\lecture{Deep Learning for NLP} % stays constant

% ------------------------------------------------------------------------------

\begin{vbframe}{people}

\vfill

\textbf{Lecturers and tutors:}

	\begin{itemize}
		\item Prof. Dr. Hinrich Schütze (weeks 9-13)
		\item Prof. Dr. Christian Heumann (weeks 1, 2, 8)
		\item Dr. Matthias Aßenmacher (weeks 3-7)
		\item M.Sc. Yihon Liu (exercise sessions)
		\item Michael Sawitzki (tutoring session \& exercise correction)
	\end{itemize}

\textbf{Time:}

	\begin{itemize}
		\item Lecture: Wednesday, 10.00 - 12.00
		\item Exercise: Friday, 10.00 - 12.00
		\item Tutoring: Wednesday, 13.00 - 15.00
	\end{itemize}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{structure of the course}

\vfill

\textbf{Central building blocks of the lecture}

	\begin{itemize}
		\item Basic concepts (2 weeks)
		\item Transformer in-depth (1 week)
		\item BERT (1 week)
		\item BERTology, BERT's successors and distillation (1 week)
		\item Other encoder models \& T5 (1 week)
		\item GPT series (1 week)
		\item Decoding Strategies (1 week)
		\item Prompting (1 week)
		\item Generative LLMs \& RLHF (2 weeks)
		\item Math behind training LLMs (1 week)
		\item Multilinguality (1 week)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{structure of the exercise}

\vfill

\textbf{Assignments:}

	\begin{itemize}
		\item Training RNNs (week 2 -- 3)
		\item Transformer (week 4 -- 5)
		\item Fine-tuning BERT \& T5 (week 6 -- 7)
		\item Inner workings \& Decoding (week 8 -- 9)
		\item LLama-2 \& Prompting (week 10 -- 11)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{structure of the exercise}

\vfill

\textbf{Central other building blocks of the exercise:}

	\begin{itemize}
		\item Python essentials \& PyTorch logic (week 1)
		\item (public holiday in week 3)
		\item Debugging (week 5)
		\item Huggingface universe (week 7)
		\item Prompting (week 9)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{prerequisites}
	
\vfill
	
\textbf{Machine learning basics:}

		\begin{itemize}
				\item A proper understanding of\\
				- linear algebra\\
				- loss functions\\
				- regularization\\
				- classification vs. regression\\
				- backpropagation and gradient descent\\
				- simple neural networks (MLPs)
				\item Great resource: \href{https://slds-lmu.github.io/i2ml/}{\textit{I2ML course}}
		\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Managing expections}

\vfill

\textbf{What to expect}

	\begin{itemize}
		\item A solid and proper understanding of\\
		- the central concepts and models of contemporary NLP\\
		- the capabilities and limitations of different models
		\item Challenging exercises that deepen your unterstanding, i.e.\\
		- you will have to invest quite some time in trying to solve them\\
		- just checking our solutions won't get you far
		\item An (inter)actively taught course with motivated lecturers, i.e.\\
		- we expect you to actively participate in the lecture and ask questions\\
		- you won't get much out of this course by just enrolling and not showing up
	\end{itemize}


\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Managing expections}

\vfill

\textbf{What not to expect}

	\begin{itemize}
		\item A prompt engineering course
		\item Riding the hypetrain
		\item Only the latest state-of-the-art LLMs (you also need to know the relevant basics to understand what is going on)
		\item Low workload and easy exercises (you will only learn this thoroughly be investing some time)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Managing expections}

\vfill

\textbf{Honesty}

	\begin{itemize}
		\item No intention to overload you with (unnecessary) work
		\item Building up proper skills requires \textbf{time investment}
		\item Coding assignments freshly created, we put a lot of work into creating creative and helpful content for you to acquire necessary skills to succeed in this field
		\item Up-front communication of what is going to be required
	\end{itemize}
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Managing expections}

\vfill

\textbf{Workload}

	\begin{itemize}
		\item This is a 6 ECTS course
		\item 1 ECTS := 30h (i.e. this course = 180h)
		\item 13 weeks $\times$ 4h presence $\approx$ 52h
		\item 130h for preparation, self-study, \textbf{working on assignments} 
		\item 130h / 14 weeks = \textbf{9h / week} (\textit{additionally to in-class time})
	\end{itemize}


\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{your turn}

\vfill

\centering \Huge{\bf Any Questions?}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}

% ------------------------------------------------------------------------------

\begin{vbframe}{notation (important!!)}

\vfill

\begin{tabular}{ll}
        $\mathcal{C}$ & A corpus of $M$ documents\\[.5em]
        $\mathbf{C}$ & The confusion matrix of a classification problem\\[.5em]
        $C$ & Size of the context window\\[.5em]
        $E$ & Embedding size // Size of the vector representation\\[.5em]
        $h$ & Number of Attention heads in a Transformer-based architecture\\[.5em]
        $H$ & Dimension of the hidden layer of a neural network\\[.5em]
        $i$ & Running index for observations in a data set, words/tokens in a\\
            & sequence or documents in a corpus\\[.5em]
        $j$ & Running index for covariates or levels of a categorical target variable\\[.5em]
        $k$ & The number of classes of a categorical target variable\\[.5em]
        $\mathcal{L}$ & A data generating process a.k.a. language\\[.5em]
        $\mathcal{M}$ & A (language) model\\[.5em]
\end{tabular}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{notation (important!!)}

\vfill

\begin{tabular}{ll}
        $m$ & The number of words/tokens in a sequence $s$\\[.5em]
        $M$ & The number of documents in a corpus $\mathcal{C}$\\[.5em]
        $n$ & The number of observations in a data set\\[.5em]
        $p$ & The number of features in a tabular data set\\[.5em]
        $s$ & An ordered sequence of $m$ words\\[.5em]
        $\theta$ & The parameters of a model\\[.5em]
        $V = \{ w_1, \hdots, w_N \}$ & The vocabulary of a corpus $\mathcal{C}$ or a language $\mathcal{L}$\\[.5em]
        $|V|$ & The size of a vocabulary $V$\\[.5em]
        $w_{\left[t\right]}$ & The word/token at the $t$-th position in a sequence\\[.5em]
				$\vec w^{({\text{word}})}$ or $\vec v^{({\text{word}})}$ & vector representation of a word\\[.5em]
        $\mathbf{W}$ & The weight matrix of a neural network\\[.5em]
        $x$ & Influential variable(s)\\[.5em]
        $y$ & Target variable(s)\\[.5em]
\end{tabular}

\vfill

\end{vbframe}