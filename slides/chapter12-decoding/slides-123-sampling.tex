\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\learninggoals}{
\item Get to know different stochastic decoding strategies
\item Learn about sampling with temperature, top-k sampling and top-p (nucleus) sampling
}
\definecolor{texblue}{rgb}{0, 0, 1}
\def\myblue#1{\textcolor{texblue}{#1}}

\title{Decoding Strategies}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Stochastic Decoding}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{Sampling Motivation}

\vfill
    
\begin{itemize}
    \item \textbf{Diversity in Output}
    \begin{itemize}
        \item \textit{Creativity and Variation}: Sampling methods produce varied outputs for the same input, useful in creative applications like story generation and dialogue systems.
        \item \textit{Avoiding Repetition}: These methods are less likely to generate repetitive loops compared to deterministic methods.
    \end{itemize}
    \item \textbf{Technical Advantages}
    \begin{itemize}
        \item \textit{Reduced Computational Cost}: Sampling methods are often cheaper than beam search, particularly for long sequences.
        \item \textit{Scalability}: They are easier to scale for large models and datasets due to simpler implementation and reduced computational demands.
    \end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Sampling (with temperature) (1)}



The next token is selected randomly based on its conditional probability distribution. To control the randomness of the output sequence, a temperature parameter can be applied to the softmax function

\vfill 

$$
\sigma\left(z_i\right)=\frac{e^{\frac{z_i}{temp}}}{\sum_{j=1}^N e^{\frac{z_j}{temp}}}
$$

\vfill

\begin{itemize}
\item $temp \rightarrow \infty: \text{ Output distribution $\approx$ Uniform distribution}$
\item $temp \rightarrow 0: \text{ Output distribution $\approx$ Point mass (Greedy search) }$
\end{itemize}



\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Sampling (with temperature) (2)}

\vfill

\textbf{Prompt: "Once upon a time"}
\begin{itemize}
\item Sampling with low temperature: \textit{", during the Second World War, during the final months for his three most talented young players, the coach, Harry Gregg said this"}
\item Sampling with high temperature: \textit{"— well. Nowhere you call back my call, not on time; never the two on account my four. Do not come." This old woman — you might have liked, she herself — she did smile."}
\end{itemize}

The generated stories are diverse but sometimes very erratic.\\

\vfill

$\Rightarrow$ Sample from the top-$k$ tokens     

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Top-$k$ sampling}

In Top-$k$ sampling, the $k$ most likely next tokens are filtered, and the probability mass is redistributed.
Visualization for $k$ = 6 in two sampling steps:

\begin{center}
    \includegraphics[width=0.9\linewidth]{figure/top_k.png}
\end{center}

\citebutton{Hugging Face, Patrick von Platen}{https://huggingface.co/blog/how-to-generate}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Top-$k$ sampling}

\vfill

\textbf{Prompt: "Once upon a time"}
\begin{itemize}
    \item Top-$k$ , $k = 100$: \textit{"when I was young the internet was a mysterious landscape full of new and exciting ideas. I read ebooks, watched videos, read short stories"}
\end{itemize}
\vspace{2ex}

The quality has improved, but the fixed $k$ might be counterproductive\\
\vspace{2ex}
$\Rightarrow$ Make $k$ dynamic        


\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Top-$p$ (Nucleus) sampling}
Top-$p$ sampling chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability threshold $p$. The probability mass is then redistributed accordingly.
Visualization with a threshold $p$ = 0.92:

\begin{center}
\includegraphics[width=1.0\linewidth]{figure/nucleus.png}
\end{center}

\citebutton{Hugging Face, Patrick von Platen}{https://huggingface.co/blog/how-to-generate}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Top-$p$ (Nucleus) sampling}

\vfill
\textbf{Prompt: "Once upon a time"}
\begin{itemize}
    \item Top-$p$ , $p = 0.92$: \textit{"there were four major political parties in the United States. Since then, however, they have become even more of a novelty. For the past few decades, there have been only two."}
\end{itemize}

\vspace{2ex}

SOTA for many years, default decoding strategy in various GPT versions, but sometimes erratic depending on $p$ and the sampled tokens.\\

\vspace{2ex}

\textbf{Question:} Can there be a balance of coherence and diversity?\\
$\Rightarrow$ Contrastive search
  
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Contrastive Search}

\begin{center}
    \includegraphics[width=1.0\linewidth]{figure/contrastive_search.png}
\end{center}     

When generating output, contrastive search jointly considers:
\begin{itemize}
    \item The probability predicted by the language model to maintain the semantic coherence between the generated text and the prompt.
    \item The similarity with respect to the previous context to avoid  degeneration (as in Greedy or Beam search)
\end{itemize}
   
$\Rightarrow$ An "ideal" token should have a high probability and bring diversity to the story.

Empirical studies suggest $k \in \{5, 8, 10, 15\}$ and $\alpha \in \{0.4, 0.5, 0.6\}$ \citebutton{Su \& Collier, 2023}{https://arxiv.org/abs/2210.14140} \citebutton{Su \& Xu, 2022}{https://arxiv.org/abs/2211.10797} \citebutton{Su et al., 2022}{https://arxiv.org/abs/2202.06417}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}