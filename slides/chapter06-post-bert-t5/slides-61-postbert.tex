\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/sesamestreet.jpeg}
\newcommand{\learninggoals}{
\item developments of the post-BERT era
\item different self-supervised objectives
\item how to tackle BERTs critical shortcomings}

\title{Post-BERT Era}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Post-BERT architectures}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{Successors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=1]{figure/transfer_learning_timeline4_nlp.pdf}}
\end{frame}
\begin{frame}[noframenumbering]{Successors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=2]{figure/transfer_learning_timeline4_nlp.pdf}}
\end{frame}
\begin{frame}[noframenumbering]{Successors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=3]{figure/transfer_learning_timeline4_nlp.pdf}}
\end{frame}
\begin{frame}[noframenumbering]{Successors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=4]{figure/transfer_learning_timeline4_nlp.pdf}}
\end{frame}
\begin{frame}[noframenumbering]{Successors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=5]{figure/transfer_learning_timeline4_nlp.pdf}}
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Different pre-training regimes (1)}

\vfill

	\textbf{ELECTRA} \citebutton{Clark et al, 2020}{https://arxiv.org/pdf/2003.10555.pdf}

	\begin{itemize}
		\item ELECTRA consinsts of two separate models
		\item[$\to$] (Small) generator model $G$ + (large) Discriminator model $D$
		\item[$\to$] This might resemble a GAN setup, but they are not trained in an adversarial manner
		\item Generator task: Masked language modeling
		\item Discriminator task: \textit{Replaced token detection}
		\item[$\to$] Predict for each token, whether it is "original" or produced by $G$
		\item ELECTRA learns from \textit{all} of the tokens (not just from a small portion of 15\%, like e.g. BERT)
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ELECTRA -- architecture}
	
\vfill

	\textbf{Joint pre-training:}
	
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/61-electra.png}\\ 
		\citebutton{Source: Clark et al, 2020}{https://arxiv.org/pdf/2003.10555.pdf}
	\end{figure}	
	
	\begin{itemize}
		\item $G$ and $D$ are (Transformer) encoders which are trained jointly
		\item $G$ replaces \texttt{[MASK]}s in an input sequence\\
					$\rightarrow$ Passes corrupted input sequence $\vec{x}^{corrupt}$ to $D$	
	\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ELECTRA -- Training details}

\vfill

	\textbf{Joint pre-training:}

	\begin{itemize}
		\item Generation of samples:
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/61-electra-samples.png}
	\end{figure}\vspace{-.25cm}
		{\footnotesize with approx. 15\% of the tokens masked out (via choice of $k$)}
		\item[]
		\item $D$ predicts whether $x_t,\; t \in 1, \hdots, T$ is "\textit{real}" or generated by $G$
			\begin{itemize}
				\item Softmax output layer for $G$ (probability distr. over all words)
				\item Sigmoid output layer for $D$ (Binary classification real vs. generated)
			\end{itemize}
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ELECTRA -- Training details}

\vfill

	Using the masked \& corrupted input sequences, the (joint) loss can be written down as follows:\\
	\vspace{.3cm}
	\textbf{Loss functions:}
	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/61-electra-loss.png}
	\end{figure}
	
	\textbf{Combined:}
	\begin{figure}
		\centering
		\includegraphics[width = 6cm]{figure/61-electra-loss-comb.png}
	\end{figure}
	{\footnotesize with $\lambda$ set to 50, since the discriminator's loss is typically much lower than the geneator's.}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ELECTRA -- Training details}

\vfill

	\textbf{Generator size:}

	\begin{itemize}
		\item Same size of $G$ and $D$: 
			\begin{itemize}
				\item Twice the compute per training step + too challenging for $D$
			\end{itemize}
		\item Smaller Generators are preferable (1/4 -- 1/2 the size of $D$)
	\begin{figure}
		\centering
		\includegraphics[width = 5cm]{figure/61-electra-size-g.png}\\ 
		\citebutton{Source: Clark et al, 2020}{https://arxiv.org/pdf/2003.10555.pdf}
	\end{figure}
	\end{itemize}
	
	\textbf{Weight sharing (experimental):}

	\begin{itemize}
		\item Same size of $G$ and $D$: All weights can be tied
		\item $G$ smaller than $D$: Share token \& positional embeddings 
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ELECTRA -- Model comparison}
	
\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/61-electra-glue.png}\\ 
		\citebutton{Source: Clark et al, 2020}{https://arxiv.org/pdf/2003.10555.pdf}
	\end{figure}
	
{\footnotesize \textit{Note:} Different batch sizes (2k vs. 8k) for ELECTRA vs. RoBERTa/XLNet explain why same number of steps lead to approx. 1/4 of the compute for ELECTRA.}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ELECTRA -- SOTA performance}

\vfill

	\textbf{Performance differences vs. BERT/RoBERTa (GLUE dev set):}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/61-electra-sota1.png}\\ 
		\citebutton{Source: Clark et al, 2020}{https://arxiv.org/pdf/2003.10555.pdf}
	\end{figure}

	\textbf{SOTA performance (GLUE test set):}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/61-electra-sota2.png}\\ 
		{\tiny * Avg. excluding QNLI to ensure comparability}\\
		\citebutton{Source: Clark et al, 2020}{https://arxiv.org/pdf/2003.10555.pdf}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Different pre-training regimes (2)}

\vfill

	\textbf{Autoregressive (AR) language modeling}
	
	\begin{itemize}
		\item Factorizes likelihood to $$p(\mathbf{x}) = \prod_{t=1}^{T} p(x_t | \mathbf{x}{< t})$$
		\item Only uni-directional (backward factorization also possible)
	\end{itemize}

	\textbf{vs. (Denoising) Autoencoding (AE)}
	
	\begin{itemize}
		\item Reconstruct masked tokens $\bar{\mathbf{x}}$ from corrupted sequence $\hat{\mathbf{x}}$:
					$$p(\bar{\mathbf{x}}|\hat{\mathbf{x}}) = \prod_{t=1}^{T} m_t \cdot p(x_t | \hat{\mathbf{x}}), \text{with $m_t$ as masking indicator}$$
		\item Possible "\textit{corruptions}" (besides MLM):
		\item[] $\to$ Replacing words with predictions (cf. ELECTRA)
		\item[] $\to$ Shuffling tokens or dropping them
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Different pre-training regimes (2)}

\vfill
	
	\textbf{Conceptual Differences}
	
	\begin{itemize}
		\item \textbf{AR:} No corruption of input sequences required
		\item ``Causal`` structure in AR approach (sometimes needed)
		\item \textbf{AE:} Assumes independence between corrupted tokens
		\item[] \ques \textbf{Why?}
		\item AR approach only conditions on left side context
		\item[] $\to$ No bidirectionality possible
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Permutation language modeling (PLM)}

\vfill

	\textbf{XLNet} \citebutton{Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	
	\begin{itemize}
		\item Alternative objective function
		\item Solves the pretrain-finetune discrepancy
		\item[$\to$] No artificial \texttt{[MASK]} token is introduced
		\item Allows for bidirectionality while keeping an AR objective
		\item[$\to$] Best of both worlds
		\item Consists of two "\textit{streams}" of the Attention mechanism
				\begin{itemize}
					\item Content-stream attention
					\item Query-stream attention
				\end{itemize}
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Permutation language modeling (PLM)}

\vfill
	
	\textbf{Manipulating the factorization order}
	
	\begin{itemize}
		\item Consider permutations $\mathbf{z}$ of the index sequence $[1,2, \hdots, T]$
		\item[$\to$] Used to permute the factorization order, \textit{not} the sequence order.
		\item Original order of the sequence is retained via positional encodings
		\item PLM objective (with $\mathcal{Z}_T$ as set of all possible permutations):
					$$\max_{\theta} \quad \mathds{E}_{\mathbf{z}\sim\mathcal{Z}_T} \left[ \sum_{t=1}^{T} \log p_\theta (x_{z_t} | \mathbf{x}_{\mathbf{z}_{< t}}) \right]$$
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{plm -- content- vs. query-stream}

\vfill

	\textbf{Content-stream}
	
	\begin{itemize}
		\item ``Ordinary`` Self-Attention (with special attention masks)\\
					$\rightarrow$ Attentions masks depend on the factorization order
		\item Information about the position in the sequence is lost,\\
					see \citebutton{Example in A.1 (Yang et al., 2019)}{https://arxiv.org/abs/1906.08237}
		\item Sets of queries ($Q$), keys ($K$) and values ($V$) from content stream*
		\item Yields a \textit{content embedding} denoted as $h_{\theta}(\mathbf{x}_{\mathbf{z}_{\leq t}})$
	\end{itemize}
	
\vfill

	{\footnotesize *For a nice visual disentanglement, see \citebutton{Figures in A.7 (Yang et al., 2019)}{https://arxiv.org/abs/1906.08237}}
	
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{plm -- content- vs. query-stream}

\vfill

	\textbf{Content-stream}
	
	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/61-xlnet-a1.png}\\ 
		\citebutton{Source: Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{plm -- content- vs. query-stream}

\vfill
	
	\textbf{Query-stream}
	
	\begin{itemize}
		\item Access to context through content-stream, but no access to the content of the current position (only location information)
		\item $Q$ from the query stream, $K$ and $V$ from the content stream*
		\item Yields a \textit{query embedding} denoted as $g_{\theta}(\mathbf{x}_{\mathbf{z}_{< t}}, z_t)$
	\end{itemize}

\vfill

	{\footnotesize *For a nice visual disentanglement, see \citebutton{Figures in A.7 (Yang et al., 2019)}{https://arxiv.org/abs/1906.08237}}
	
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{plm -- content- vs. query-stream}

\vfill

	\textbf{Content-stream}
	
	\begin{figure}
		\centering
		\hspace{-2cm}\includegraphics[width = 13cm]{figure/61-xlnet-content.png}\\ 
		\citebutton{Source: Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{plm -- content- vs. query-stream}

\vfill

	\textbf{Query-stream}
	
	\begin{figure}
		\centering
		\hspace{-2cm}\includegraphics[width = 13cm]{figure/61-xlnet-query.png}\\ 
		\citebutton{Source: Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{plm -- content- vs. query-stream}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 12cm]{figure/61-xlnet}\\ 
		{\tiny (a) content-stream; (b) query-stream; (c) whole model}\\
		\citebutton{Source: Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{XLNet -- Model Input}

\vfill

\textbf{Generation of samples:}

\begin{itemize}
\item Randomly sample two sentences and use concatenation* as input
			{\footnotesize
\begin{center}
\begin{tabular}{|cccccccc|}
\hline
\cellcolor{blue!15}\texttt{[CLS]} & The & fox & is & quick & . & \cellcolor{blue!15}\texttt{[SEP]} &\\\hline\hline It & jumps & over & the & lazy & dog & . & \cellcolor{blue!15}\texttt{[SEP]} \\
\hline
\end{tabular}\\\mbox{}
\end{center}}
{\scriptsize *Nevertheless: XLNet does \textit{not} use the NSP objective }
\end{itemize}

\textbf{Additional encodings:}

\begin{itemize}
\item \textit{Relative} segment encodings:
	\begin{itemize}
		\item BERT adds absolute segment embeddings ($E_A$ \& $E_B$)
		\item XLNet uses relative encodings ($\vec{s}_+$ \& $\vec{s}_-$)
	\end{itemize}
\item \textit{Relative} Positional encodings:
	\begin{itemize}
		\item BERT encodes information about the absolute position ($E_0, E_1, E_2, \hdots$)
		\item XLNet uses relative encodings ($R_{i - j}$)
	\end{itemize}
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{XLNet -- Special remarks}
	
\vfill

	\begin{itemize}
		\item \textbf{Partial Prediction:} Only predict the last tokens in a factoriztion order (reduces optimization difficulty)
					{\small $$\max_{\theta} \quad \mathds{E}_{\mathbf{z}\sim\mathcal{Z}_T} \left[ \sum_{t=c+1}^{|\mathbf{z}|} \log p_\theta (x_{z_t} | \mathbf{x}_{\mathbf{z}_{< t}}) \right],\quad \mbox{with $c$ as cutting point}$$}
		\item \textbf{Segment recurrence mechanism:} Allow for learning extended contexts in Transformer-based architectures, see \citebutton{Dai et al. (2019)}{https://arxiv.org/pdf/1901.02860.pdf}
		\item \textbf{No independence assumption:}
	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/61-xlnet-objective}\\ 
		{\tiny Prediction of [New, York] given the factorization order [is, a, city, New, York]}\\
		\citebutton{Source: Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	\end{figure}
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{XLNet -- SOTA performance}

\vfill

	\textbf{Performance differences to BERT:}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/61-xlnet-vs-bert.png}\\ 
		\citebutton{Source: Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	\end{figure}

	\textbf{SOTA performance:}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/61-xlnet-sota.png}\\ 
		\citebutton{Source: Yang et al., 2019}{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf}
	\end{figure}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
