\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%\newcommand{\titlefigure}{figure/gpt_sq.png}
\newcommand{\learninggoals}{
\item Learn to calculate Transformer number of parameters
\item Understand Transformer computation and memory load
\item Learn about Flash Attentions
\item Understand Scaling Laws and Chinchilla
}
\definecolor{texblue}{rgb}{0, 0, 1}
\def\myblue#1{\textcolor{texblue}{#1}}

\title{The Math Behind Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Training Transformer LLMs}
\lecture{Deep Learning for NLP}


%Notes:
% Slides: https://jasonwei20.github.io/files/FLAN%20talk%20external.pdf
% Check out emergence talk: https://www.youtube.com/watch?v=0SuyDLjNR9g

% LLM Survey: https://arxiv.org/pdf/2303.18223.pdf

% ------------------------------------------------------------------------------

\begin{vbframe}{LLM Parameters: Main Components}

\vfill

\begin{itemize}
    \item Model parameters are half-precision (bfloat16) numbers of 2 bytes
    \item One block (decoder unit) consists of:
		\begin{itemize}
		\item $W_q, W_k, W_v$ matrices which are each $d_{model} \cdot n_{heads} \cdot d_{head}$ and project the input into the query, key, and value used in self-attention. 
		\item A $W_0$ matrix which is also $d_{model} \cdot n_{heads} \cdot d_{head}$ used on the output of self-attention, before the MLP (feedforward) layer
		\item MLP weights, which are two matrices each of ${d_{model}}^2 \cdot 4$. Here the 4 is based on calculations and means that the MLP is 4 times the size of the model embedding dimension. 
		\end{itemize}
	\item In most architectures, $d_{model} = n_{heads} \cdot d_{head}$
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{LLM Parameters: Formula}

\vfill

Combining the above, for one layer/block we get this formula:
\begin{equation*}
\begin{array}{c}
P_{layer} = 3 \cdot d_{model} \cdot n_{heads} \cdot d_{head} + \cdot d_{model} \cdot n_{heads} \cdot d_{head} + 2 \cdot 4 \cdot {d_{model}}^2 \\ [8pt]
P_{layer} = 4 \cdot d_{model} \cdot n_{heads} \cdot d_{head} + 8 \cdot {d_{model}}^2 \\ [8pt]
P_{layer} = 4 \cdot d_{model} \cdot d_{model} + 8 \cdot {d_{model}}^2 \\ [8pt]
P_{layer} = 12 \cdot {d_{model}}^2
\end{array}
\end{equation*}

\vskip5mm

For a LLM of $n$ layers, we get:
\begin{equation*}
\begin{array}{c}
P = 12 \cdot n_{layers} \cdot {d_{model}}^2
\end{array}
\end{equation*}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{LLM Parameters: Example}

\vfill

GPT-3 Small has:
\begin{equation*}
\begin{array}{l}
n_{params} = 125\,M ~; n_{layers} = 12 ~; d_{model} = 768 ~; n_{heads} = 12 ~; d_{head} = 64 
\end{array}
\end{equation*}

\vskip2mm

GPT-3 Medium has:
\begin{equation*}
\begin{array}{l}
n_{params} = 350\,M ~; n_{layers} = 24 ~; d_{model} = 1024 ~; n_{heads} = 16 ~; d_{head} = 64 
\end{array}
\end{equation*}

\vskip2mm

Applying the above formula we get $\sim$85\,M parameters for GPT-3 Small and $\sim$302\,M parameters for GPT-3 Medium. 

\vskip2mm

What are we missing...?!

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{LLM Parameters: Other Components}

\vfill

\begin{itemize}
    \item Word Embedding parameters
    \item Position Embedding parameters
	\item Token Type Embedding parameters
	\item Embedding Layer Normalization, weight and Bias
	\item Other model-specific parameters
\end{itemize}

\vskip5mm

They do not scale with model size.

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Compute Requirements}

\vfill

$C \approx \tau T = 6 P D$  \newline

where: \newline

\begin{itemize}
    \item $C$ is compute to train the model, in total floating point operations
    \item $C = C_{forward} + C_{backward}$
	\item $C_{forward} \approx 2 P D$
	\item $C_{backward} \approx 4 P D$
	\item $\tau$ is throughput of hardware: (No. GPUs) x (FLOPs/GPU)
	\item $T$ is the time spent training the model, in seconds
	\item $P$ is the number of parameters in the model
	\item $D$ is the dataset size, in tokens
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Compute Units}

\vfill

$C$  can be measured in different units:\newline

\begin{itemize}
    \item FLOP-seconds which is [Floating Point Ops / Second] x [Seconds]
	\begin{itemize}
	    \item We also use multiples GFLOP-seconds, TFLOP-seconds etc.
		\item Other multiples like PFLOP-days are used in papers
		\item 1 PFLOP-day = $10^{15} \cdot 24 \cdot 3600$ FLOP-seconds
	\end{itemize}
	\item GPU-hours which is [No. GPUs] x [Hours]
	\begin{itemize}
	    \item GPU model is also required, since they have different compute capacities
		\item For any GPU model, its Actual FLOPs are always lower than the advertised theoretical FLOPs
	\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Parameter vs Dataset}

\vfill

\begin{itemize}
    \item Model performance depends on number of parameters $P$, but also on number of training tokens $D$
	\item We need to decide about $P$ and $D$, so that we get the best performance withing the compute budget
	\item The optimal tradeoff between $P$ and $D$ is: $D = 20 P$
	\begin{itemize}
	    \item This is usually true for Chinchilla models, but not for all LLMs
	\end{itemize}
	\item Training a LLM for less than 200 billion tokens is not recommended
	\item Rule of thumb: First determine the upmost inference cost, and then train the biggest model within that boundary.
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Memory Requirements}

\vfill

Common questions: \newline

\begin{itemize}
 	\item How big is this model in bytes?
	\item Will it fit/train in my GPUs?
\end{itemize}

\vskip8mm

Model size components: \newline

\begin{itemize}
 	\item Model parameters
	\item Optimizer states
	\item Gradients
	\item Activations
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Model Parameters}

\vfill

Parameter size depends on chosen representation: \newline

\begin{itemize}
 	\item Pure fp32: $Mem_{model} = 4 ~bytes/param \cdot N_{params}$
 	\item fp16 or bf16: $Mem_{model} = 2 ~bytes/param \cdot N_{params}$
	\item int8: $Mem_{model} = 1 ~byte/param \cdot N_{params}$
\end{itemize}

\vskip8mm

It is practically common to use mixed representations: \newline

\begin{itemize}
 	\item fp32 + fp16
	\item fp32 + bf16
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Optimizer States}

\vfill

AdamW: $Mem_{AdamW} = 12 ~bytes/param \cdot N_{params}$
\begin{itemize}
 	\item fp32 copy of parameters: 4 bytes/param
 	\item Momentum: 4 bytes/param
	\item Variance: 4 bytes/param
\end{itemize}

\vskip3mm

bitsandbytes (8-bit optimizer): $Mem_{AdamW} = 6 ~bytes/param \cdot N_{params}$
\begin{itemize}
 	\item fp32 copy of parameters: 4 bytes/param
 	\item Momentum: 1 byte/param
	\item Variance: 1 byte/param
\end{itemize}

\vskip3mm

SGD: $Mem_{AdamW} = 8 ~bytes/param \cdot N_{params}$
\begin{itemize}
 	\item fp32 copy of parameters: 4 bytes/param
 	\item Momentum: 4 bytes/param
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Gradients}

\vfill

They are usually stored in the same datatype as the model parameters. \newline

Their memory overhead contribution is: \newline

\begin{itemize}
 	\item fp32: $Mem_{grad} = 4 ~bytes/param \cdot N_{params}$
 	\item fp16 or bf16: $Mem_{grad} = 2 ~bytes/param \cdot N_{params}$
	\item int8: $Mem_{grad} = 1 ~byte/param \cdot N_{params}$
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Activations}

\vfill

\begin{itemize}
 	\item GPUs are bottlenecked by memory, not FLOPs
 	\item Save GPU memory by recomputing activations of certain layers
	\item Various schemes for selecting which layers to clear
	\item They take some extra memory, but save even more
\end{itemize}

\vskip5mm

Total memory when training \textbf{without} activations: \newline
$Mem_{training} = Mem_{params} + Mem_{opt} + Mem_{grad}$

\vskip5mm

Total memory when training \textbf{using} activations: \newline
$Mem_{training} = Mem_{params} + Mem_{opt} + Mem_{grad} + Mem_{activ}$

\vskip5mm

In the latter case, $Mem_{params}$, $Mem_{opt}$ and $Mem_{grad}$ are significantly smaller than in the former.

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Distributed Training}

\vfill

\begin{itemize}
 	\item Training LLMs faster on many GPUs
 	\item Avoiding OOM issues
	\item \textbf{Data parallelism:} split the data on different model replicas
	\item \textbf{Tensor parallellism:} split model parameters accross GPUs
	\item \textbf{Sharded optimizers:} reduce optimizer overhead by No. GPUs
	\begin{itemize}
	 	\item ZeRO (Zero Redundancy Optimizer)
		\item Requires low extra communication between GPUs
	 	\item Decreases optimizer memory requirement
		\item Improves training speed
\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{The $O(\MakeLowercase{n}^2)$ Problem of Transformer LLM\MakeLowercase{s}}

\vfill

\textbf{Quadratic time \& memory complexity of Self-Attention}
\begin{itemize}
    \item Inductive bias of Transformer models: Connect all tokens in a sequence to each other
    \item \textbf{Pro:} Can (theoretically) learn contexts of arbitrary length
    \item \textbf{Con:} Bad (quadratic )scalability limiting context size 
\end{itemize}

\vskip3mm

\textbf{Resulting Problems:}
\begin{itemize}
    \item Several tasks require models to consume longer sequences
	\begin{itemize}
	    \item Text summarization of documents
		\item Sentiment analysis of documents
		\item Classification of EEG trace of thousand time steps
		\item Classification of coding or non-coding genes
		\item ... many more from biology/medicine
	\end{itemize}
    \item \textit{Efficiency:} Are there more efficient (e.g., linear) implementations which achieve similar or even better performance?
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Efficient Transformers}

\vfill

\textbf{Broad overview on so-called "X-formers":}

\begin{itemize}
	\item Efficient \& fast Transformer-based models\\
				$\rightarrow$ Reduce complexity from $\mathcal{O}(n^2)$ to (up to) $\mathcal{O}(n)$
	\item Claim on-par (or even) superior performance
	\item Different techniques used:
				\begin{itemize}
					\item Fixed/Factorized/Random Patterns
					\item Learnable Patterns (extension of the above)
					\item Low-Rank approximations or Kernels
					\item Recurrence (see e.g. \href{https://arxiv.org/pdf/1901.02860.pdf}{\beamergotobutton{Transformer-XL (Dai et al., 2019)}})
					\item Memory modules
				\end{itemize}
\end{itemize}
	
	\vspace{.3cm}
	
\textbf{Side note:}

\begin{itemize}
	\item Most Benchmark data sets not explicitly designed for evaluating long-range abilities of the models.
	\item Recently proposed: \href{https://arxiv.org/pdf/2011.04006.pdf}{\beamergotobutton{\textit{Longe Range Arena: A benchmark for efficient transformers} (Tay et al., 2020)}}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Introducing Patterns}

\vfill

\textbf{Reasoning:} 

\begin{itemize}
	\item Making every token attend to every other token might be unnecessary
	\item Introduce sparsity in the commonly dense attention matrix
\end{itemize}

\textbf{Example:}

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/bigbird-patterns.png} \\ 
	{\footnotesize Source: \href{https://proceedings.neurips.cc//paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf}{Zaheer et al. (2020)}}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{F\MakeLowercase{lash}A\MakeLowercase{ttention}}

\vfill

Fast and Memory-Efficient Exact Attention with IO-Awareness \newline

\begin{itemize}
	\item Fast
	\begin{itemize}
		\item 15\,\% faster than BERT
		\item 3x faster than GPT-2 
		\item 2.4x faster than Megatron-LM
	\end{itemize}
	\item Memory-efficient
	\begin{itemize}
		\item Reducing from $O(\MakeLowercase{n}^2)$ to $O(\MakeLowercase{n})$ 
	\end{itemize}
	\item Exact
	\begin{itemize}
		\item Same as ``vanilla attention'', not an approximation 
	\end{itemize}
	\item IO aware
	\begin{itemize}
		\item Reducing memory load/store operations
	\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{GPU Memory Hierarchy}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/gpu_mem.png} \\ 
	{\footnotesize Source: \href{https://arxiv.org/abs/2205.14135}{Dao et al. (2022)}}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Computing Considerations}

\vfill

\begin{itemize}
	\item GPU compute has been growing faster than memory bandwidth
	\begin{itemize}
		\item GPU has to wait for data
	\end{itemize}
	\item Transformer operations are memory-bound
	\begin{itemize}
		\item Elementwise operations with high memory access
	\end{itemize}
	\item IO aware means reducing memory load/store operations
	\item FlashAttention implements the following:
	\begin{itemize}
		\item Operation fusion to reduce memory access
		\item Tiling or chunking the softmax matrix into blocks
		\item Recomputation for better memory utilization
	\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Operation Fusion}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/op_fusion.png} \\ 
	{\footnotesize Source: \href{https://horace.io/brrr_intro.html}{\url{https://horace.io/brrr_intro.html}}}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Limitations and Prospects}

\vfill

\begin{itemize}
	\item FlashAttention requires writing attention to CUDA language
	\begin{itemize}
		\item A new CUDA kernel for each new attention implementation
		\item CUDA is lower-level than PyTorch
		\item Implementation may not be transferable accross GPUs
	\end{itemize}
	\item Towards IO-Aware Deep Learning
	\begin{itemize}
		\item Extending beyonde attention
	\end{itemize}
	\item Multi-GPU IO-Aware Methods
	\begin{itemize}
		\item FlashAttention computation may be parallelizable accross multiple GPUs
	\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling Laws}
\href{https://arxiv.org/abs/2001.08361}{\beamergotobutton{Kaplan et al. (2020)}} 

\vfill

\begin{itemize}

	\item Performance depends strongly on scale, weakly on model shape
	\begin{itemize}
	\item Scale means: parameters $N$, data $D$, and compute $C$
	\item Shape means: depth and width
	\end{itemize}

	\item Smooth power laws
	\begin{itemize}
	\item Performance has power-law relation with each factor $N$, $D$, $C$
	\item When not bottlenecked by the other two 
	\item Trend spanning more than six orders of magnitude
	\end{itemize}

	\item Universality of overfitting 
	\begin{itemize}
	\item Performance enters regime of diminishing returns if $N$ or $D$ held fixed while the other increases
	\item Performance penalty depends on $N^{0.74} / D$
	\end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling Laws}

\vfill

\begin{itemize}

	\item Universality of training
	\begin{itemize}
	\item Training curves follow predictable power-laws
	\item Their parameters are roughly independent of model size
	\item It is possible to predict by extrapolating the early part of the training curve
	\end{itemize}

	\item Transfer improves with test performance
	\begin{itemize}
	\item When evaluating on text with different distribution from training text, results are strongly correlated to those on the validation set
	\item Transfer to different distribution incurs a constant penalty but improves in line with performance on training set
	\end{itemize}

	\item Sample efficiency
	\begin{itemize}
	\item Large models are more sample-efficient than small models
	\item They reach same performance with fewer optimization steps
	\end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling Laws}

\vfill

\begin{itemize}

	\item Convergence is inefficient
	\begin{itemize}
	\item When $C$ is fixed but $N$ and $D$ are not, optimal performance is achived by training very large models and stopping significantly short of convergence  
	\end{itemize}

	\item Optimal batch size
	\begin{itemize}
	\item Ideal size is a power of the loss only
	\item It is $\sim$1-2 million tokens for the largest models we can train
	\end{itemize}

\end{itemize}

\vskip3mm

\textbf{Larger language models will perform better and be more sample efficient than current models.} 

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Compute-Optimal LLM\MakeLowercase{s}}
\href{https://arxiv.org/abs/2203.15556}{\beamergotobutton{Hoffmann et al. (2022)}}

Given a fixed FLOPs budget, how should we trade-off model size and text size to optimize performance? 

\vfill

\begin{itemize}

	\item Find $N$ and $D$ so that $FLOPs(N,D) = C$ and $L(N,D)$ is minimal

	\item Empirically estimated $N$ and $D$ based on 400 models. 
	\begin{itemize}
	\item Ranging from 70\,M to 16\,B parameters
	\item Trained on 5\,B to 400\,B tokens
	\end{itemize}

	\item Different results from those of \href{https://arxiv.org/abs/2001.08361}{\beamergotobutton{Kaplan et al. (2020)}} 
	\item Results verified using Chinchilla
	\begin{itemize}
	\item Chinchilla has 70\,B parameters and is trained on 1.4\,T tokens
	\item 4x less parameters and 4x more tokens than Gopher
	\item Chinchilla outruns Gopher and has reduced memory footprint and inference cost 
	\end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Compute-Optimal LLM\MakeLowercase{s}}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/chinchilla.png} \\ 
	{\footnotesize Source: \href{https://arxiv.org/abs/2203.15556}{Hoffmann et al. (2022)}}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Chinchilla and the other LLM\MakeLowercase{s}}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/llm_params.png} \\ 
	{\footnotesize Source: \href{https://arxiv.org/abs/2203.15556}{Hoffmann et al. (2022)}}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/chinchilla_gopher.png} \\ 
	{\footnotesize Source: \href{https://arxiv.org/abs/2203.15556}{Hoffmann et al. (2022)}}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Chinchilla on MMLU}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 8cm]{./figure/chinchilla_mmlu.png} \\ 
	{\footnotesize Source: \href{https://arxiv.org/abs/2203.15556}{Hoffmann et al. (2022)}}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Chinchilla on QA}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 12cm]{./figure/chinchilla_qa.png} \\ 
	{\footnotesize Source: \href{https://arxiv.org/abs/2203.15556}{Hoffmann et al. (2022)}}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture

\end{document}
