\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble_new}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Deep Learning for NLP}

\definecolor{texblue}{rgb}{0, 0, 1}
\def\myblue#1{\textcolor{texblue}{#1}}
\long\def\devour#1{}

\begin{document}

%\devour{

\titlemeta{
Training LLMs
}{
Scaling laws
}{
figure/losscompute.png
}{
\item Understand scaling laws
}


%Notes:
% Slides: https://jasonwei20.github.io/files/FLAN%20talk%20external.pdf
% Check out emergence talk: https://www.youtube.com/watch?v=0SuyDLjNR9g

% LLM Survey: https://arxiv.org/pdf/2303.18223.pdf

% ------------------------------------------------------------------------------

\begin{vbframe}{Number of Parameters: Notation}

\vfill

\begin{itemize}
\item In this slide set, we use $N$ for the number of parameters.
\item (Unfortunately,  we use $P$ for the number of
  parameters in other slide sets.)
\end{itemize}

\vfill
    
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling Laws Proposed by Kaplan et al. (2020)}
\href{https://arxiv.org/abs/2001.08361}{\beamergotobutton{Kaplan et al. (2020)}} 

\vfill

\begin{itemize}

	\item Performance depends strongly on scale, weakly on model shape
	\begin{itemize}
	\item Scale means: parameters $N$, data $D$, and compute $C$
	\item Shape means: depth and width
	\end{itemize}

%	\item Smooth power laws
	\item Power laws
	\begin{itemize}
	\item Performance has power law relation with each factor $N$, $D$, $C$
	\item Trend spanning more than six orders of magnitude
	\end{itemize}

	\item Limits of power laws
	\begin{itemize}
	\item Power law for one variable only holds when not bottlenecked by the other two 
	\item Performance enters regime of diminishing returns if $N$ or $D$ held fixed while the other increases
	\end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling Laws Proposed by Kaplan et al. (2020)}

\vfill

\begin{itemize}

\item These power laws were huge news in 2020!
  \item Easy way to improve AI
	\item We can predict by extrapolating the early part of the training curve
	\item Power law parameters are roughly independent of model size

	\item Transfer improves with test performance
	\begin{itemize}
	\item When evaluating on text with different distribution from training text, results are strongly correlated to those on the validation set
	\item Transfer to different distribution incurs a constant penalty but improves in line with performance on training set
	\end{itemize}

	\item Sample efficiency
	\begin{itemize}
	\item Large models are more sample-efficient than small models
	\item They reach same performance with fewer optimization steps
	\end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Power Law (GPT3 Paper)}

\begin{figure}
    \centering
    \includegraphics[width=10cm]{figure/losscompute.png}
\end{figure}
    
\end{vbframe}

% ------------------------------------------------------------------------------
\begin{vbframe}{Power Law (GPT3 Paper): Questions?}

\begin{figure}
    \centering
    \includegraphics[width=10cm]{figure/losscompute.png}
\end{figure}
    
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling Laws}

\vfill

\begin{itemize}

	\item ``Convergence is inefficient''
	\begin{itemize}
	\item When $C$ is fixed but $N$ and $D$ are not, optimal performance is achieved by training very large models and stopping significantly short of convergence  
        \item That is: early stopping
\item So the claim is: Larger language models will perform better and be more sample efficient than smaller models.

	\end{itemize}

	\item Optimal batch size
	\begin{itemize}
    	\item Claim: “Optimal batch size: The ideal batch size for training these
                models is roughly a power of the loss only, and continues to
                be determinable by measuring the gradient noise scale
                [MKAT18]; it is roughly 1-2 million tokens at convergence for
                the largest models we can train.”
    	\item gradient noise scale = a measure of the signal-to-noise ratio
            of gradient across training examples
	\end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Optimal Batch Size}

\begin{figure}
    \centering
    \includegraphics[height = 6cm]{figure/optimalbatchsize.png}
\end{figure}


\end{vbframe}

\begin{vbframe}{Optimal Batch Size}

\begin{figure}
    \centering
    \includegraphics[width = 6cm]{figure/optimalbatchsize.png}
\end{figure}

\begin{itemize}
  \item The ideal batch size increases  over the
    course of training.
    \item Estimate gradient as accurately as possible $\rightarrow$ large batch
    \item Exploit stochasticity (epoch batches would not be
    a good thing even if we could compute them efficiently)
    $\rightarrow$ small batch    \item Increase training speed as much as possible $\rightarrow$ large step size
    \item Based on the estimated gradient, choose a step size such that the cost of the landing position does not deviate too much from the cost of the ideal landing position $\rightarrow$ small step size


\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling Law for next word prediction}

\vfill

\begin{itemize}
    \item $L(N,D) = 1.61 + \frac{406.4}{N^{0.34}} + \frac{410.7}{D^{0.28}}$
    \item $L(N,D)$ is cross entropy on new text
\item Source:
  \citebutton{Hoffmann et al., 2022}{https://arxiv.org/abs/2203.15556}
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Scaling Law for next word prediction}

\vfill

\begin{itemize}
    \item $L(N,D) = 2 + \frac{1000}{N^{1/3}} + \frac{1000}{D^{1/4}}$
    \item \ques Compute $L(N,D)$ for
    $N=10^9,D=10^8$,\\
    $N=10^{12},D=10^{16}$,\\
    $N=10^3,D=10^{100}$ and\\
    $N=10^{100},D=10^4$
    \item \ques Which configuration is best?

    \item \ques Why is it best?

      \item 
 \ques Which values of $N$ and $D$ would you choose
    to create an even more powerful model?
% 2 + 1 + 1
% 2 + 0.1 + 0.1
% 2 + 100 + 0
% 2 + 0 + 100

\item Of course, the last question is relative to 
the  available compute budget, so try and find a reasonable answer.


\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Compute-Optimal LLM\MakeLowercase{s}}

Given a fixed FLOP budget $C$, how should we trade-off model size and text size to optimize performance? \citebutton{Hoffmann et al., 2022}{https://arxiv.org/abs/2203.15556}

\vfill

\begin{itemize}

	\item Find $N$ and $D$ so that $FLOP(N,D) = C$ and $L(N,D)$ is minimal

	\item Empirical estimates of $N$ and $D$ based on 400 models. 
	\begin{itemize}
	\item Ranging from 70\,M to 16\,B parameters
	\item Trained on 5\,B to 400\,B tokens
	\end{itemize}

	\item Different results from those of \citebutton{Kaplan et al., 2020}{https://arxiv.org/abs/2001.08361} 
	\item Results verified using Chinchilla
	\begin{itemize}
	\item Chinchilla has 70\,B parameters and is trained on 1.4\,T tokens
	\item 4x less parameters and 4x more tokens than Gopher
	\item Chinchilla outruns Gopher and has reduced memory footprint and inference cost 
	\end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Compute-Optimal LLM\MakeLowercase{s}: Are GPT3 etc too large?}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/chinchilla.png} \\ 
	\citebutton{Source: Hoffmann et al., 2022}{https://arxiv.org/abs/2203.15556}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Compute-Optimal LLM\MakeLowercase{s} (2)}

Given a fixed FLOPs budget, how should one trade off model size and
the number of training tokens? We find that all three methods predict
that \myblue{current large models should be substantially smaller and therefore
trained much longer than is currently done}. Based on our estimated
compute-optimal frontier, we predict that for the compute budget used
to train Gopher, an optimal model should be 4 times smaller, while
being training on 4 times more tokens. We verify this by training a more
compute-optimal 70B model, called Chinchilla, on 1.4 trillion tokens.
Not only does Chinchilla outperform its much larger counterpart,
Gopher, but its \myblue{reduced model size reduces inference cost considerably
and greatly facilitates downstream uses on smaller hardware}. The
energy cost of a large language model is amortized through its usage
for inference and fine-tuning. The benefits of a more optimally trained
smaller model, therefore, extend beyond the immediate benefits of its
improved performance.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Chinchilla and the other LLM\MakeLowercase{s}}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/llm_params.png} \\ 
	\citebutton{Source: Hoffmann et al., 2022}{https://arxiv.org/abs/2203.15556}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/chinchilla_gopher.png} \\ 
	\citebutton{Source: Hoffmann et al., 2022}{https://arxiv.org/abs/2203.15556}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Chinchilla outperforms other LLM\MakeLowercase{s}: MMLU}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 8cm]{./figure/chinchilla_mmlu.png} \\ 
	\citebutton{Source: Hoffmann et al.,
	2022}{https://arxiv.org/abs/2203.15556}, compared
	with human forecasts
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Chinchilla outperforms other LLM\MakeLowercase{s}: QA}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 12cm]{./figure/chinchilla_qa.png} \\ 
	\citebutton{Source: Hoffmann et al., 2022}{https://arxiv.org/abs/2203.15556}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Scaling laws: Discussion}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/3thingserrorratescaleswith.png} \\ 
\end{figure}

\ques Any doubts?

\vfill

% do we have enough text?
% do we have enough compute?
% can we afford that much compute?
% Sutskever: results from scaling up pretraining
% have plateaued
% Sutskever: scaling the right thing matters more now than ever
% the verge 20241025: gemini
% not showing performacne that the deepmind team had hoped
% for
% general trend in the audience?
% https://www.theverge.com/2024/10/25/24279600/google-next-gemini-ai-model-openai-december
% ai companies cannot really say that scaling is notw orkgin
% because valuation depends on that

\end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Beyond brute-force scaling of datasets}

\vfill

\begin{itemize}
	\item If you run out of data, then increase \# epochs
        \item The Stack v2
	already contains all public code! arxiv.org/abs/2305.16264
        \item data quality/model size tradeoff: Training
	a smaller model on  a smaller high-quality dataset may be
	better than training a huge model on a huge
	medium-quality dataset.
        \item For higher quality datasets, allocate more
	compute to model size. arxiv.org/abs/2401.02954
        \item See Stanford CS25: StarCoder Use Case

\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{More recent thoughts on scaling}

\vfill

\begin{itemize}
\item \citebutton{Reuters}{https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/}, 2024-11-15
\item Ilya Sutskever:
  ``results
	from \myblue{scaling up pre-training} \ldots\
	\myblue{have plateaued}.''
%	\item
%But now, some of the most prominent AI scientists are
%speaking out on the \myblue{limitations of this “bigger is better”
%philosophy.}
\item “The 2010s were the age of scaling, now we're back in
	the age of wonder and discovery once again. Everyone
	is looking for the next thing,” Sutskever
	said. \myblue{“Scaling the right thing matters more now than ever.”}
\end{itemize}


\vfill

\end{vbframe}

\devour{

\begin{vbframe}{Doubts about scaling (2)}

\vfill

\begin{itemize}
\item \citebutton{The
	Verge}{https://www.theverge.com/2024/10/25/24279600/google-next-gemini-ai-model-openai-december}, 2024-10-25:
 I've heard that the model (December Gemini release) isn't showing the
	performance
        gains that Demis Hassabis-led team had hoped for \ldots
\item Andrew Ng's news letter:
``Orion’s improvement over GPT-4 was
far smaller than that from GPT-3 to GPT-4.'',
``OpenAI found that pretraining Orion on
synthetic data made it too much like earlier models \ldots'',
``Anthropic’s schedule for \ldots\ Claude 3.5 Opus \ldots\
has slipped. It
hasn’t shown the expected performance given its size and
cost \ldots'',
\end{itemize}


\vfill

\end{vbframe}
}

\begin{vbframe}{Epoch AI projections}

\vfill

\begin{figure}
	\centering
	\includegraphics[height = 6cm]{./figure/epochai.png} 
\end{figure}

\citebutton{Epoch
	AI}{https://epoch.ai/blog/can-ai-scaling-continue-through-2030}
%	random internet page claims GPT4 required $2\cdot
%	10^{25}$ FLOPS


\vfill

\end{vbframe}



\begin{vbframe}{Train/test tradeoff (1)}

\vfill

\begin{itemize}
%	\item Even if a humongous model gives me the best
%	possible performance, cost per token may be
%	prohibitively expensive.
%        \item Example: Llama models were small-ish because
%	that was the size that made economic sense.
%\item huge openai models may not make economic sense
        \item Current
        models cost order of magnitude \$100 million to train,
	and
        this number could reach \$100 billion within a few
	years,
        according to Anthropic’s Dario Amodei. Rising costs
	could
        lead companies to reallocate their gargantuan
	training
        budgets and researchers to focus on more
	cost-effective,
        application-specific approaches. (Andrew Ng)

\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Train/test tradeoff (2)}

\vfill

\begin{itemize}
\item Phi4: released by Microsoft Dec 2024
	\item 14 billion parameters (small!)
	\item pretraining data: 9.8 trillion tokens (pretty big)
        \item pretraining data curated / synthesized
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Train/test tradeoff (3)}

\vfill

\begin{figure}
	\centering
	\includegraphics[height = 6cm]{./figure/scalingtraintest.png} \\ 
\end{figure}

\citebutton{Andy L. Jones}{https://arxiv.org/abs/2104.03113}
                 
\vfill



\end{vbframe}


\devour{

\begin{vbframe}{Shift away from scaling towards test time /
	systems approaches}

\vfill

\begin{itemize}
	\item When investing more test-time compute, small
	models can outperform much larger models
        \citebutton{Snell et al}{https://arxiv.org/pdf/2408.03314v1}
\item Many recent innovations (all implemented in o1?) were not about scaling.
\begin{itemize}
\item Train on synthetic data (e.g., chain of thought)
\item Reflection
\item Test-time adaptation
\end{itemize}

\item Focus more on compound/integrated systems with many
smaller components rather than a single gargantuan
end-to-end monolithic model.
\end{itemize}

\vfill

\end{vbframe}

}

\begin{vbframe}{Main takeaway (1)}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 11cm]{./figure/3thingserrorratescaleswith.png} \\ 
%	\includegraphics[width = 12cm]{./figure/linear,tokens,pars.png} 
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Main takeaway (2)}


\vfill

\begin{itemize}
	\item The original scaling laws were exclusively
        about \myblue{(pre)training}: What is the best model
        I can pretrain given
        constraints such as compute budget?

\item This has shifted to a more holistic view of what we
  want to achieve, e.g.,
even if a trillion parameter model is the optimal
        model given pretraining constraints, a much smaller
        model may be preferable since it is so much cheaper at
        inference time.

\item In the future: \myblue{compound systems of many smaller models/agents}
        rather than
        a single huge monolithic model?

\item MoE is a version of that.

\end{itemize}

\vfill

\end{vbframe}

\devour{

\begin{vbframe}{``The scaling is going to continue''\\
Dario Amodei, November 2024}

\vfill

\begin{figure}
	\centering
	\includegraphics[width = 10cm]{./figure/amodeiscaling.png} \\ 
%	\includegraphics[width = 12cm]{./figure/linear,tokens,pars.png} 
\end{figure}


\vfill

\end{vbframe}

\begin{vbframe}{Gemini 3, Nov 2025}

\begin{figure}
    \centering
    \includegraphics[height=6cm]{figure/scaling,gemini3.png}
\end{figure}
    
\end{vbframe}

}

\begin{vbframe}{What is a trillion?}

\citebutton{https://www.youtube.com/watch?v=QgUj09K7vx4}{What is a trillion?}
   (1:30) 
\end{vbframe}

\endlecture
\end{document}
