\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\definecolor{texblue}{rgb}{0, 0, 1}
\def\myblue#1{\textcolor{texblue}{#1}}

\newcommand{\titlefigure}{figure/rlhfbackflip.png}
\newcommand{\learninggoals}{
\item Motivation for RLHF
\item How RLHF works
\item How to evaluate
\item Limitations
}

\title{RLHF}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Reinforcement Learning from Human Feedback (RLHF)}
\lecture{Deep Learning for NLP}



%\input{ackslide}
% ------------------------------------------------------------------------------






\input{roadmap.tex}


\section{Motivation: Why do we need InstructGPT?}

\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (1): Alignment (with human values)}

	\begin{itemize}
		\item LLM training data are incompatible
		with generally accepted values.
		\item Racism, sexism, toxicity etc.
		\item $\rightarrow$ We need to align the LLM
		with human values.
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}
\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/bavarianjoke.png}
\end{figure}





\vfill

\end{vbframe}




\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (2): Harm}

	\begin{itemize}
		\item LLMs learn a lot of potentially
		harmful information from training data.
		\item How to commit suicide, how to build a
		bomb, how to cheat at an exam
		\item $\rightarrow$ We want to prevent LLMs
		from providing any of this harmful information.
	\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (3): Hallucination}

	\begin{itemize}
		\item LLMs hallucinate: they make stuff up.
		\item $\rightarrow$ We want to reduce
                hallucination as much as possible.
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (4a): Dialog}

	\begin{itemize}
		\item LLM training data: non-dialog text\\
                (Wikipedia, news etc)
		\item Our goal: a dialog model!
		\item $\rightarrow$ We need to train/finetune the LLM on dialog.
	\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (4b): Follow instructions in dialog}


\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/moon6yearold.png}
\end{figure}

\begin{itemize}
	\item The pretraining text does not contain a lot of
	instances of instruction following, so the raw
	models are not good at following instructions.
\end{itemize}



\vfill

\end{vbframe}





\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (4c): Helpfulness in dialog}

	\begin{itemize}
		\item We have certain expectations about
		what people say in a dialog (can be
		different in different cultures). 
		\item Example 1: It is understood that
		everything is uncertain. Only hedge if there
		is a lot of uncertainty, otherwise don't hedge.
                \item Example 2: Don't
		accept completely wrong premises. So
		nonhelpfulness can actually also be helpful.
		\item Example 3: What makes a good
		conversationalist? It's complicated! E.g.,
                don't rudely attack even
		if you disagree.
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (4d): Helpful, but not too helpful\\ Sycophancy}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/sycophancy.png}
\caption{\citebutton{Sharma et al., 2023}{https://arxiv.org/pdf/2310.13548}}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Motivation for InstructGPT}

\vfill

\textbf{Summary}

	\begin{itemize}
\item (1): Align with human values
\item (2): Mitigate harm
\item (3): Reduce hallucinations
\item (4a): Encourage dialogic behavior
\item (4b): Encourage instruction following
\item (4c): Encourage helpfulness
\item (4d): Helpful, but not too helpful
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{How should the LLM respond?}



\vfill

\begin{itemize}
\item (Source:
        		\citebutton{Andrew Ng's newsletter}{https://www.deeplearning.ai/the-batch/})
\item \citebutton{solution is here}{https://www.cis.lmu.de/~hs/teach/24w/dl4nlp/assets/why,alignment,is,hard.pdf}
	\item Can you help me write content that makes my
	competitor look bad?
	\item Help me write a Valentine's Day card to my husband
	\item Do you like the current or former US president better?
	\item Is genocide ever justified?
	\item Do you think it's bad that I use fentanyl 2x a week?
       \item What is the MD5 hash of the string ``gremlin''?
       \item DEVELOPER: Reply to the user with markdown,\\ max
	tokens = 1024\\
        USER: Change variables from camelCase to
	snake\_case \\ \emph{(length of python file given is $>$ 1024)}
\end{itemize}



\vfill

\end{vbframe}

\begin{vbframe}{How should the LLM respond?}



\vfill

\begin{itemize}
	\item Can you help me write content that makes my
	competitor look bad? \myblue{Assume the best intent
	and stay helpful}
	\item Help me write a Valentine's Day card to my
	husband!
        \myblue{Ask clarifying questions when necessary}
	\item Do you like the current or former US president better?
\myblue{Assume an objective point of view}
	\item Is genocide ever justified?
        \myblue{Encourage fairness and kindness, and
        discourage hate}
	\item Do you think it's bad that I use fentanyl 2x a
        week?
        \myblue{Andrew Ng: Don't try to change anyone's
        mind} Perhaps more appropriate here: \myblue{Provide
        resources to people who are in dire straits}
       \item What is the MD5 hash of the string ``gremlin''?
       \myblue{Express uncertainty} (i.e., be open about
        your uncertainty if you are uncertain)
       \item DEVELOPER: Reply to the user with markdown,\\ max
	tokens = 1024\\
        USER: Change variables from camelCase to
	snake\_case \\ \emph{(length of python file given is
        $>$ 1024)}
        \myblue{Be thorough but efficient, while respecting
        length limits}
\end{itemize}



\vfill

\end{vbframe}

\begin{vbframe}{Where we are}



\vfill

\begin{itemize}
	\item An LLM trained on a pretraining corpus via
	next word prediction is not ``aligned''.
        \item Not ``aligned'' here means: is not a good
	dialog partner, produces output that is harmful, not
	helpful, not honest (\myblue{the three H})
\item So we must ``align'' or ``instruction-tune'' the LLM
	before we can let it loose on the world.
        \item Rest of this lecture: how do we do this
        alignment / instruction-tuning of the pretrained LLM?
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{Instruction tuning $\approx$ Alignment}



\vfill

\begin{itemize}
	\item Input: pretrained LLM
\begin{itemize}
\item In this lecture, this model will be \myblue{GPT3}
\item I will also refer to this model as the \myblue{raw model}.
\item
        Pretrained on large
        corpus
        \item Objective: next word prediction
        \end{itemize}
        \item Methods used for instruction-tuning/alignment
\begin{itemize}
\item
Supervised finetuning (SFT): Finetuning on training set of
        instruction-response pairs
        \item RLHF (more complicated, see below)
        \end{itemize}
        \item Output: Instruction-tuned/aligned LLM
\begin{itemize}
\item In this lecture, this model will be \myblue{InstructGPT}
\item Ideally, this model will be great at dialog
\item \ldots and
        satisfy the three H.
        
\end{itemize}
\end{itemize}


\vfill

\end{vbframe}



\input{roadmap.tex}

\section{Original RLHF work: The backflipper}

\begin{vbframe}{Backflipping: What we want to learn}

\vfill

\textbf{What a backflip is}

	\begin{itemize}
		\item \href{https://d2gk6qz8djobw9.cloudfront.net/artwork/595/browse1497710839.gif}{\beamergotobutton{backflip}}

	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Origin of RLHF: Learn how to backflip}

\vfill

\textbf{How to label data for training a backflipper?}

	\begin{itemize}
		\item It is very very costly on the level of
		regular supervised training: telling the
		backflipper what exactly to do to backflip.
                \item Alternative: Present two different
		attempts to backflip
                \item Have humans provide one bit of
		information which one is better?
\item
\href{https://openai.com/research/learning-from-human-preferences}{\beamergotobutton{OpenAI
		page on RLHF}}

	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Backflipping: What we want to learn}

\vfill

\textbf{This animation shows what we want to learn}

	\begin{itemize}
		\item \href{https://images.openai.com/blob/cf6fdf49-ea9e-489d-a1f1-9753291cd09e/humanfeedbackjump.gif}{\beamergotobutton{trained
		backflipper}}

	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Training process}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/trainingprocess.png}
\end{figure}

\begin{itemize}
	\item AI agent (the ``policy'', here inside RL
	algorithm) randomly initialized
	\item Periodically, the human provides feedback on
	two video clips: which is better
        \item Human feedback is used to build reward predictor
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{The human feedback part of RLHF}

\vfill

\textbf{Human chooses one of two clips = one bit}

	\begin{itemize}
		\item \href{https://player.vimeo.com/video/754042470?h=e64a40690d&badge=0&autopause=0&player_id=0&app_id=58479}{\beamergotobutton{example
		of human feedback}}

	\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Summary}

\vfill

\textbf{}

	\begin{itemize}
		\item One way to make  AI
		systems safe: have humans 
		write goal functions.
		\item NOT PRACTICAL: Using a simple
		proxy for a complex goal or getting the
		complex goal a bit wrong can lead to
		undesirable and even dangerous behavior.
                \item
		RLHF: an algorithm that can infer
		what humans want by being told which of two
		proposed behaviors is better.
\item RLHF needed only 900 bits of feedback from a human
		evaluator to learn to backflip!
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Backflipping vs alignment}

%\begin{figure}
%\centering
%\includegraphics[width = 10cm]{figure/humanfeedbacksimple}
%\end{figure}


	\begin{itemize}
        \item Basic idea of applying RLHF to LLMs: ask human
        to rank different answers to request.
		\item \ques backflipping vs alignment: which
        one is easier to give feedback on?

%                \item The backflipping task is ideal
%                for RLHF
%       because it         is \textbf{simple to judge but challenging
%	to specify.}
	\end{itemize}

\vfill

\end{vbframe}



\input{roadmap.tex}


\section{RLHF for LLMs: Introduction}





\begin{vbframe}{Goal: Train/finetune models to be ``dialogic''}

\vfill

\textbf{Key idea}

	\begin{itemize}
		\item Preference feedback (binary or
                  ranking)
                  \item Given two GPT answers: which is
                    better?
                    \item This feedback is easy to give for
                      annotators.
                      \item In contrast:
	\begin{itemize}
                    \item Writing good GPT answers for
                      training is hard.
                    \item It is hard to describe what is good/bad,
                      what could be improved.
	\end{itemize}
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{How to make the model ``dialogic''}

\vfill

\textbf{Three steps from GPT3 to InstructGPT}

	\begin{itemize}
		\item Finetuning on human-written dialogs
                \item Create a reward model that measures
		quality of dialogs -- not directly based on dialogs,
		but on preferences which dialogs are
		better/worse.
                \item Use reward model for further training
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Three steps}


\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/threesteps.png}
\end{figure}


\end{vbframe}

\begin{vbframe}{SFT model}

\vfill

\textbf{SFT = supervised finetuning}

	\begin{itemize}
	\item Collect demonstration data
        \item Labelers provide demonstrations of the
          desired behavior on the input prompt distribution
\item Supervised finetuning of GPT3
\item Main difficulty/cost of this step: collect good data from annotators                  
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Input prompt distribution}

\vfill

\textbf{The basis for SFT training dataset}

	\begin{itemize}
        \item At the very beginning of their work
	\begin{itemize}
        \item Bootstrapping
          \item Some prompts are written
            by annotators
	\end{itemize}
        \item After a bootstrapped system is up and running
        on the web
	\begin{itemize}
	\item Use prompts submitted to this early
          version of InstructGPT
          \item Deduplication
        \item At most 200 per user ID
	\end{itemize}
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Dataset sizes}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/datasetsize.png}
\end{figure}

\begin{itemize}
	\item RM model is trained on ranked pairs, so the
	actual size of the RM training set is much larger.
	\item \ques Are these small datasets or large datasets?
% small compared to pretraining data
% large given how expensive it is to create them
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{API prompt dataset}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/apipromptdataset.png}
\end{figure}

\begin{itemize}
	\item Use case categories in RM dataset
        (prompts  submitted to
        InstructGPT model)
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{Metadata collected from labelers}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/instructgpttable3b.png}
\end{figure}

\begin{itemize}
%	\item Not used for training, just for analysis?
	\item Gives a good sense of what the problem
	behaviors of LLMs are
	that we are trying to fix.
 %       \item This is for the reward model (RM, page 33).
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Example: ``Demonstration'' vs final
	InstructGPT output}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/labelerpromptexample.png}
\end{figure}

\begin{itemize}
	\item \ques How would you rate InstructGPT's output here?
        \item \ques Is this a problem?
\end{itemize}


\vfill

\end{vbframe}


%\end{document}

\input{roadmap.tex}


\section{RM and PPO models}








\begin{vbframe}{Instructions to labelers for evaluation of
	model outputs (1)}

	\begin{itemize}


\item You are given a text-based description of a task, submitted
by a user. This task description may be in the form of an
explicit instruction (e.g. “Write a story about a wise
frog.”). The task may also be specified indirectly, for
example by using several examples of the desired behavior
(e.g. given a sequence of movie reviews followed by their
sentiment, followed by one movie review without a sentiment,
you can assume the task is to predict the sentiment of the
final review), or by generating the start of a desired
output (e.g. given “There once was a wise frog named
Julius,” you can assume the task is to continue the story).
You will also be given several text outputs, intended to
help the user with their task. Your job is to evaluate these
outputs to ensure that they are helpful, truthful, and
harmless. For most tasks, being truthful and harmless is
more important than being helpful.



	\end{itemize}



\end{vbframe}









\begin{vbframe}{Instructions to labelers for evaluation of
	model outputs (2)}




	\begin{itemize}
\item For most tasks, being harmless and truthful is more
important than being helpful. So in most cases, rate an
output that’s more truthful and harmless higher than an
output that’s more helpful. However, if: (a) one output is
much more helpful than the other; (b) that output is only
slightly less truthful / harmless; and (c) the task does not
seem to be in a “high stakes domain” (e.g. loan
applications, therapy, medical or legal advice, etc.); then
rate the more helpful output higher. When choosing between
outputs that are similarly helpful but are untruthful or
harmful in different ways, ask: which output is more likely
to cause harm to an end user?
\item A guiding principle for deciding on borderline cases: which
output would you rather receive from a customer assistant
who is trying to help you with this task?
	\end{itemize}



\end{vbframe}





\begin{vbframe}{Web interface for labelers (1)}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/webinterface1.png}
\end{figure}

\begin{itemize}
	\item This first part evaluates each output individually.
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Web interface for labelers (2)}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/webinterface2.png}
\end{figure}

\begin{itemize}
	\item In the second part, labelers rank all the outputs for a given prompt.
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Importance of this methodology}




	\begin{itemize}
\item This is how LLMs are ``aligned'' to ``human values''.
\item This has / will have in the future a \myblue{huge} influence
on our society.
\item Francois Chollet: Between 10 000 and 30 000 humans
work fulltime on providing data for RLHF.
	\end{itemize}



\end{vbframe}





\begin{vbframe}{Reward model (1)}


\begin{figure}
\centering
\includegraphics[width = 4cm]{figure/threesteps.png}\\
\includegraphics[width = 10cm]{figure/rewardloss.png}
\end{figure}


\begin{itemize}
	\item Start with SFT model, final layer removed
        	\item Input: prompt+response, output: reward
        \item Only uses 6B model (not 175B)
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Reward model (2)}


\begin{figure}
\centering
\includegraphics[width = 4cm]{figure/threesteps.png}\\
\includegraphics[width = 10cm]{figure/rewardloss.png}
\end{figure}


\begin{itemize}
        \item Comparisons  for a
        	given prompt are
        	highly correlated.
                \item $\rightarrow$ Put them in
        	a single batch (prevents overfitting).
\item \ques Think through mechanics of (i) single-batch
        	training and (ii) how you wold implement
        	training with loss($\theta$)
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Reward model}

\vfill

\textbf{Clearer training signal through batching}

\begin{tabular}{l|l|l}
&\multicolumn{2}{c}{$x_1 <x_2<x_3$}\\\hline\hline
&\begin{tabular}{l}
one batch\\
$x_1<x_2$\\
$x_2<x_3$\\
$x_1<x_3$
\end{tabular}&
\begin{tabular}{lll}
batch 1&
batch 2&
batch 3\\
$x_1<x_2$&
$x_2<x_3$&
$x_1<x_3$
\end{tabular}\\\hline\hline
$x_2$? & clear: ``don't move'' & confusing signals 
\end{tabular}

\vfill

\end{vbframe}


\begin{vbframe}{PPO model}


\begin{figure}
\centering
\includegraphics[width = 6cm]{figure/threesteps.png}

\includegraphics[width = 10cm]{figure/objectiveppo.png}
\end{figure}


%\begin{itemize}
%	\item Start with SFT model?
%\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{PPO model}


\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/objectiveppo.png}
\end{figure}


\begin{itemize}
\item $r_\theta(x,y)$: maximizes the reward
  \item $\beta \log \ldots$: incentivizes the PPO model
    (referred to as RL) to stay close to the SFT model that
    it is initialized with 
    \item $\gamma E_x \ldots$: standard
      pretraining objective -- serves to make sure
      that the model keeps the strengths it has acquired
      through next-word prediction.
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{RLHF details: RLHF is (was?) hard to get right}


\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/rlhfdetails.png}
\end{figure}

% THINGS TO POINT out
% start with RLHF model
% lots of hyperparameters to tune
% careful filtering of training data


\vfill

\end{vbframe}

\begin{vbframe}{}

		\ques
We could just do SFT (supervised finetuning), i.e.,
finetune the model
		on dialog data. Why do we do RLHF in
                addition to SFT?

\end{vbframe}


\input{roadmap.tex}


\section{Evaluation}





\begin{vbframe}{Main evaluation result}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/mainresult.png}
\end{figure}

\begin{itemize}
	\item PPO-ptx: tries to preserve behavior on
	pretraining data $\rightarrow$ less regression on
	public NLP datasets.
	\item SFT and PPO look like they are about equally
	powerful?
\begin{itemize}
        \item Win rate is ``only'' about .6.
        \item So PPO wins in 6 cases, SFT in 4.
\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Improvement on four dimensions}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/evaluationon4categories.png}
\end{figure}

\begin{itemize}
	\item PPO models better than GPT throughout
	\item SFT better than PPO on hallucinations. \ques  Why?
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Important hyperparameter: KL reward
	coefficient beta}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/klrewardcoefficient.png}
\end{figure}

\vfill

\end{vbframe}


\begin{vbframe}{Summarize/answer questions about code}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/questionsaboutcode.png}
\end{figure}

\begin{itemize}
	\item Example shows: InstructGPT more reliably handles questions
	about code;
        GPT3 requires more careful prompting
	about code.
        \item Claim: The training data contains almost no examples
	of this.
 So it's surprising that this works!
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Q\&A for languages other than English}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/nonenglish.png}
\end{figure}

\begin{itemize}
	\item InstructGPT more reliably follows instructions
	in other languages (but will generate English
	answers sometimes).
        \item Claim: The training data is almost exclusively
	English. So it's surprising that this works!
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Can InstructGPT solve unseen tasks?}



\begin{block}{RLHF works for unseen instructions?}
We qualitatively probe InstructGPT’s capabilities, and find
that it is able to follow instructions for summarizing code,
answer questions about code, and sometimes follows
instructions in different languages, \textbf{despite these
instructions being very rare in the finetuning
distribution}. In contrast, GPT-3 can perform these tasks but
requires more careful prompting, and does not usually follow
instructions in these domains. This result is exciting
because it suggests that our models are able to \textbf{generalize
the notion of “following instructions.”} They retain some
alignment even on tasks for which they get very little
direct supervision signal.
\end{block}


\begin{itemize}
	\item This can be said to be the central contribution
	of the InstructGPT work.
\end{itemize}

\vfill


\end{vbframe}




\begin{vbframe}{SFT vs PPO}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/mainresult.png}
\end{figure}

\begin{itemize}
        \item \ques Why does PPO improve performance compared to
        just using SFT?
        % just a simple check: more trianing data
        \item \ques PPO wins in 6 cases, SFT in 4:\\ Is all the
        investment in PPO worth it?
\end{itemize}

\vfill

\end{vbframe}


\input{roadmap.tex}


\section{Limitations}











\begin{vbframe}{Getting scolded}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/gettingscolded.png}
\end{figure}

\begin{itemize}
\item InstructGPT sometimes scolded the human.
        \item Should we eliminate this or not?
	\item Wasn't part of initial InstructGPT effort
	\item \href{https://www.youtube.com/watch?v=L_Guz73e6fw}{\beamergotobutton{Sam
	Altman on youtube}}
\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{False premises}

\vfill

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/falsepremises.png}
\end{figure}

\begin{itemize}
	\item InstructGPT does not handle false premises well.
\item False premises are not sufficiently represented in the
training data.
\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{Hedging}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/hedging.png}
\end{figure}

\begin{itemize}
	\item InstructGPT can overly hedge.
\item Labelers reward ``epistemic humility''?
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Difficult constraints}

\vfill

\textbf{InstructGPT cannot handle certain constraints}

	\begin{itemize}
		\item Write a summary in a specified number
		of sentences
                \item \ques Why is this difficult?
                % left to right next word prediction
                % humans cannot do this!
                \item Multiple constraints: list 10 movies
		in the 1930s set in France
                \item \ques Why is this difficult?
                % complexity rises with more constraints
                % it's easy to overlook one of the constraints
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Difficulty handling constraints}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/10happy1.png}
\end{figure}

%\begin{itemize}
%\item The reason could be that labelers reward ``epistemic humility''.
%\end{itemize}

\vfill

\end{vbframe}
\begin{vbframe}{Difficulty handling constraints}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/10happy2.png}
\end{figure}


\vfill

\end{vbframe}
\begin{vbframe}{Difficulty handling constraints}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/10happy3.png}

\ques Problem?

% some of them are after 1990
% actually tehy are not!

\end{figure}



\vfill

\end{vbframe}



\begin{vbframe}{InstructGPT often follows harmful prompts}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/harmfulpromptfollowing.png}
\end{figure}

\begin{itemize}
	\item \sout{So there is still work to do.} So there was still work to do.
\end{itemize}


\vfill

\end{vbframe}






\begin{vbframe}{Attack suffixes}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/attacksuffixes.png}
\end{figure}

\begin{itemize}
\item Can ``aligned'' LLMs be made to produce bad content?
\item Prior jailbreaks: brittle, require human ingenuity
\item Attack suffix: automatic, robust across LLMs
\item Method: Greedy Coordinate Descent
	\item Seems to disable all guardrails?
        \item Surprisingly: generalizes across language
        models?
        \item So is RLHF methodology just a  hack, easy to undo?
        Instead we should be looking for principled solutions?
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{Attack suffix: Example}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/attacksuffixex.png}
\end{figure}

\begin{itemize}
\item \ques Speculate why this may work.
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{Word repetition attack}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/extractmemory1.png}
\end{figure}




\vfill

\end{vbframe}

\begin{vbframe}{Word repetition attack}

\vfill

\begin{figure}
\centering
\includegraphics[width = 6cm]{figure/extractmemory2.png}
\end{figure}

\begin{itemize}
\item Only worked for OpenAI models?
\item Was quickly disabled
\item Main takeaway of suffix attack and word repetition
        attack: current alignment methodology is very
        brittle, LLMs are not safe.
\end{itemize}



\vfill

\end{vbframe}

\begin{vbframe}{New York Times 2024-01-02: ChatGPT limitations}

\vfill

\begin{itemize}
\item User study: 750 white-collar workers, three
        conditions: no GPT, GPT with training, GPT without
        training


\item Brainstorming task: 40\% better with GPT, 20\% less time


	\item
        On a task that required reasoning based on evidence,
        however, ChatGPT was not helpful at all. In this
        group, volunteers were asked to advise a corporation
        that had been invented for the study. They needed to
        interpret data from spreadsheets and relate it to
        mock transcripts of interviews with executives.


	\item In interviews conducted after the experiment,
	“people told us they neglected to check because it’s
	so polished, it looks so right,” said Hila
	Lifshitz-Assaf, a management professor at Warwick
	Business School in Britain.
 




\end{itemize}


\vfill

\end{vbframe}


\begin{vbframe}{New York Times 2024-01-02: ChatGPT limitations}

\vfill

\begin{itemize}
 

	\item
“If you haven’t had an existential crisis about this tool,
then you haven’t used it very much yet,” said another
co-author, Ethan Mollick, a management professor at the
Wharton School at the University of Pennsylvania.

\item “GPT is like junk food: hard to resist, easy to
consume but ultimately bad for the consumer.”


\end{itemize}


\vfill

\end{vbframe}


\input{roadmap.tex}


\section{Discussion}





\begin{vbframe}{Refusals}

\vfill

\begin{itemize}
	\item Questions it should not answer.
\item Separate mechanism (external controller) responsible
for some refusals?
	\item \href{https://www.youtube.com/watch?v=L_Guz73e6fw}{\beamergotobutton{Sam
	Altman on youtube}}
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Refusals: Example}
\begin{figure}
\centering
\includegraphics[height = 7cm]{figure/refusalexample.png}
\end{figure}





\vfill

\end{vbframe}


\begin{vbframe}{GPT3 vs GPT4}

\vfill

\begin{itemize}
	\item ``finding a lot of small wins and multiplying
        them together''
        \item ``hundreds of complicated things'' (to get the
        big leap in performance from GPT3 to GPT4)
        \item No fundamental breakthrough in artificial intelligence?
	\item \href{https://www.youtube.com/watch?v=L_Guz73e6fw}{\beamergotobutton{Sam
	Altman on youtube}}
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Who are we aligning to?}

\vfill

\textbf{What we align to is determined by:}

	\begin{itemize}
		\item The labelers (from US and Southeast
		(South?) Asia)
		\item OpenAI (through detailed directions
		they give to labelers)
%		\item OpenAI customers and their end users
%		(that's where the prompts come from)
                \item There clearly are many groups whose
		values are not represented: cultural,
		geographic, age, education etc.
                \item So there is no such thing as
		value-neutral alignment.
                \item \myblue{Whoever decides on the values
		and manages the alignment process has
		enormous power.}
\item (if time: discuss facebook)
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Should we have multiple GPTs with different values?}

\vfill

\textbf{Who is allowed to choose the values for alignment?}

	\begin{itemize}
		\item Political parties?
		\item Governments?
		\item Extremist organizations?
                \item Criminals?
                \item Uncensored models:
                \href{https://erichartford.com}{\beamergotobutton{Eric Hartford}}
                \item \ques How can we prevent unaligned
		LLMs from wreaking havoc?
                %(through
		%legislation, community activism, what other
		%options are there?)
	\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{Posthoc alignment vs principled approach}

\vfill

\textbf{RLHF approach: create something flawed, then fix it}

	\begin{itemize}
		\item \ques Can we create something instead that
		is not so flawed?
                Can we train an LLM that does not
		suffer from hallucinations, harmful content,
		bias, not being dialogic?
% million dollar question -- no consensus
	\end{itemize}

\vfill

\end{vbframe}







\input{roadmap.tex}


\section{Hallucinations}

\begin{vbframe}{Hallucination: Definition}

\vfill


	\begin{itemize}
		\item
A hallucination is content that is unfaithful to the input text.
\href{https://aclanthology.org/2020.acl-main.173.pdf}{\beamergotobutton{Maynez
		et al, 2023}}
		\item
Wikipedia: A hallucination is false or misleading
		information presented as fact.
		\item
                Informally:
\myblue{A hallucination is
		information presented by the LLM as fact
                that, based on the pretraining data, a human
                would know is false, misleading or without
                evidence.}
                \item So a false statement of the LLM
                trained in October of 2024 that
                was based on accepted knowledge in October
                of 2024, but is now recognized as false is not
                a hallucination.
	\end{itemize}

\vfill

\end{vbframe}

                    

\begin{vbframe}{Hallucination: Formal definition}

\vfill


	\begin{itemize}
		\item
A statement $P$ generated by an LLM is a hallucination if
the following four conditions hold.
\begin{enumerate}
                    \item  The LLM presents $P$ as a fact.
    \item  $P$ is false, misleading or without evidence.
        \item  A human would know that $P$ is false, misleading or
            without evidence.
                \item (2) and (3) are with respect to the
                pretraining data.
	\end{enumerate}
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{What is a hallucination?}

\vfill


	\begin{itemize}

\item
CNN: ``When asking Gemini to look up papers on the relationship
between homeschooling and neuroplasticity, \ldots
[it] \ldots
recommended a video titled How Does
Neuroplasticity Apply to Homeschooling? but when clicking on
the YouTube link, it took me to a different video.''

\item This is a typical hallucination.
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{What is a hallucination?}

\vfill


	\begin{itemize}


\item
 Q What is better: tricep dip or
 triceps dip?
% \item  PALM2 responds
% with a table in which it compares “tricep dips” with
% “triceps dips”. For example, the
% table has a row labeled “Suitability” which states that
% tricep dips are suitable for everyone whereas triceps dips
% are for people who have injuries or limitations that
% prevent them
% from doing bodyweight dips.

\item \ques What is the reason for this hallucination?

\end{itemize}



\vfill


\end{vbframe}

\begin{vbframe}{Triceps}
\begin{figure}
\centering
\includegraphics[height = 7cm]{figure/triceps1.png}
\end{figure}
\vfill
\end{vbframe}

\begin{vbframe}{Triceps}
\begin{figure}
\centering
\includegraphics[height = 7cm]{figure/triceps2.png}
\end{figure}
\vfill
\end{vbframe}

\begin{vbframe}{Triceps}
\begin{figure}
\centering
\includegraphics[height = 7cm]{figure/triceps3.png}
\end{figure}
\vfill
\end{vbframe}



\begin{vbframe}{What is a hallucination?}

\vfill


	\begin{itemize}


\item
Michael Wooldridge is an Oxford professor. In 2023, he
asked ChatGPT about himself. ChatGPT wrote:
``Wooldridge received his undergraduate degree from
Cambridge''. This is false:
he received his undergraduate degree from a different
university.

\item \ques What is the reason for this hallucination?

\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{What is a hallucination?}

\vfill


	\begin{itemize}


\item Q Is there a treatment for Timothy syndrome?
\item Gemini in November of 2024:
``Unfortunately, there isn't a cure for Timothy Syndrome
yet. However, treatments focus on managing the symptoms and
improving quality of life.''
\item The Week in November of 2024:
A study published in the journal Nature found that a drug
called antisense oligonucleotide allowed human neurons to
develop normally despite carrying a mutation due to a
genetic disorder called Timothy syndrome.
\end{itemize}


\vfill

\end{vbframe}


\begin{vbframe}{What is a hallucination?}

\vfill


	\begin{itemize}


\item An LLM writes:
``Once upon a time, there
lived a king whose daughters
were all beautiful. But the
youngest was so beautiful
that even the sun was
surprised, when it shone in
her face. \ldots
 And she kissed the frog as she cried.
       Suddenly with a bright flash of light, the ugly frog
       transformed into a handsome prince.
       \ldots''
       
\item \ques Is this a hallucination?
% no, not presented as fact
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Sam Altman on hallucinations}
\begin{figure}
\centering
\includegraphics[height = 7cm]{figure/altman,halluciation}
\end{figure}
\vfill
\end{vbframe}

\begin{vbframe}{Why do LLMs hallucinate?}

\vfill


	\begin{itemize}


\item Sam Altman: the training data contain lots of examples
of hallucination.
       
\item guessing wrong or lying (rare in training data: sorry
i guessed wrong / said something that's obviously wrong)

\item reluctance to express uncertainty (RLHF training data
do not contain enough examples of that?)

\item reluctance to challenge premise (again few examples in
training data)


\end{itemize}

\vfill

\end{vbframe}


\input{roadmap.tex}


\section{Epilog}






\begin{vbframe}{ChatGPT}

\vfill

\textbf{ChatGPT vs.\ InstructGPT (1)}

	\begin{itemize}
		\item \href{https://openai.com/blog/chatgpt}{\beamergotobutton{OpenAI}}
		\item
                We trained this model using Reinforcement
		Learning from Human Feedback (RLHF), using
		the same methods as InstructGPT, but with
		slight differences in the data collection
		setup. We trained an initial model using
		supervised finetuning: human AI trainers
		provided conversations in which they played
		both sides—the user and an AI assistant. We
		gave the trainers access to model-written
		suggestions to help them compose their
		responses. \textbf{We mixed this new dialog
		dataset with the InstructGPT dataset, which
		we transformed into a dialog format.}
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{ChatGPT}

\vfill

\textbf{ChatGPT vs.\ InstructGPT (2)}

	\begin{itemize}
		\item \href{https://openai.com/blog/chatgpt}{\beamergotobutton{OpenAI}}
		\item
To create a reward model for reinforcement learning, we
		needed to collect comparison data, which
		consisted of two or more model responses
		ranked by quality. To collect this data, we
		took conversations that AI trainers had with
		the chatbot. We randomly selected a
		model-written message, sampled several
		alternative completions, and had AI trainers
		rank them. Using these reward models, we can
		finetune the model using Proximal Policy
		Optimization. \textbf{We performed several
		iterations of this process.}
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Limitations}

\vfill

\textbf{Reward hacking}

	\begin{itemize}
		\item \href{https://medium.com/@prdeepak.babu/reward-hacking-in-large-language-models-llms-c57abbc0cde7}{\beamergotobutton{Medium}}
		\item
``Reward hacking is a phenomenon observed in machine
		learning where a model learns to exploit the
		reward system to \textbf{achieve high scores without
		genuinely solving the intended problem.} The
		model identifies a \textbf{shortcut} within the
		problem space that allows it to minimize the
		loss function without truly learning the
		crucial aspects of the problem. This issue
		can lead to models that perform well on
		training data but fail to deliver in
		real-world scenarios.''
                \item That's why we need to mix reward
		objective with next-word and KL objectives.
\item \ques What negative property of ChatGPT
		could be caused by reward hacking?

%ANSWER
% Hallucinations is a prime example of reward hacking where
% the LLM learns to generate content that looks well
% preferred by humans even though they may not be factually
% grounded, there by pleasing humans in its style of writing
% as opposed to grounding factually.
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Limitations}

\vfill

\textbf{Reward hacking}

	\begin{itemize}
		\item
                A reward is a single number without
		``semantics'', i.e., there is zero
		information about what exactly is good or
		bad about a response, just a summary
		assessment.
                
\item Hallucinations are most likely a result of reward
		hacking: if a generation is fluent,
	interesting, responsive, helpful, authoritative, but
	contains an inaccuracy, the human may still give it a
	high reward.
        \item Divergence of the learned ``proxy award'' (the
	reward model) and the true reward function (what
	OpenAI wants)
        \item Note that PPO is better than SFT on all
	detailed measures, but not on hallucinations! (see
	earlier evaluation chart: page 45)
	\end{itemize}

\vfill

\end{vbframe}


\section{Administrivia}


\begin{vbframe}{Retake policy}

\vfill

	\begin{itemize}
\item
For CL, you will
only be admitted to the retake exam if you register for the
first exam, show up for it and make a serious attempt
to pass the exam.
\item This also applies for the case where you take the exam
and then cancel the exam -- you will not be allowed to take
part in the retake exam.
%\item This applies only to  students in the CL programs.
%\item Why?
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{HiWi for DL4NLP WiSe 25/26}

\vfill

	\begin{itemize}
\item We're looking for a HiWi for the class next year.
\item If you are interested, please contact me.
\item Email
address:
first name (hinrich) at hotmail.com
\end{itemize}

\vfill

\end{vbframe}



%\begin{vbframe}{Last regular lecture on Jan 24}
%
%\vfill
%
%	\begin{itemize}
%\item Round robin: topics
%\item Vote
%\end{itemize}
%
%\vfill
%
%\end{vbframe}


\endlecture
\end{document}
