\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/rlhfbackflip.png}
\newcommand{\learninggoals}{
\item Motivation for RLHF
\item How RLHF works
\item How to evaluate
\item Limitations
}

\title{RLHF}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Reinforcement Learning from Human Feedback (RLHF)}
\lecture{Deep Learning for NLP}



\input{ackslide}
% ------------------------------------------------------------------------------






\input{roadmap.tex}


\section{Motivation: Why do we need InstructGPT?}

\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (1): Alignment (with human values)}

	\begin{itemize}
		\item LLM training data are incompatible
		with generally accepted values.
		\item Racism, sexism, toxicity etc.
		\item $\rightarrow$ We need to align the LLM
		with human values.
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (2): Harm}

	\begin{itemize}
		\item LLMs learn a lot of potentially
		harmful information from training data.
		\item How to commit suicide, how to build a
		bomb, how to cheat at an exam
		\item $\rightarrow$ We want to prevent LLMs
		from providing any of this harmful information.
	\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (3): Hallucination}

	\begin{itemize}
		\item LLMs hallucinate: they make stuff up.
		\item $\rightarrow$ We want to reduce
                hallucination as much as possible.
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (4a): Dialog}

	\begin{itemize}
		\item LLM training data: non-dialog text\\
                (Wikipedia, news etc)
		\item Our goal: a dialog model!
		\item $\rightarrow$ We need to train/finetune the LLM on dialog.
	\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (4b): Follow instructions in dialog}


\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/moon6yearold.png}
\end{figure}

\begin{itemize}
	\item The pretraining text does not contain a lot of
	instances of instruction following, so the raw
	models are not good at following instructions.
\end{itemize}



\vfill

\end{vbframe}





\begin{vbframe}{InstructGPT: Why?}

\vfill

\textbf{Motivation (4c): Helpfulness in dialog}

	\begin{itemize}
		\item We have certain expectations about
		what people say in a dialog (can be
		different in different cultures). 
		\item Example 1: It is understood that
		everything is uncertain. Only hedge if there
		is a lot of uncertainty, otherwise don't hedge.
                \item Example 2: Don't
		accept completely wrong premises. So
		nonhelpfulness can actually also be helpful.
		\item Example 3: What makes a good
		conversationalist? It's complicated! E.g.,
                don't rudely attack even
		if you disagree.
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Motivation for InstructGPT}

\vfill

\textbf{Summary}

	\begin{itemize}
\item (1): Align with human values
\item (2): Mitigate harm
\item (3): Reduce hallucinations
\item (4a): Encourage dialogic behavior
\item (4b): Encourage instruction following
\item (4c): Encourage helpfulness
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Is there a better way?}

\vfill

\textbf{InstructGPT approach: create something flawed, then fix it}

	\begin{itemize}
		\item Can we create something instead that
		is not so flawed?
                \item Can we train an LLM that does not
		suffer from hallucinations, harmful content,
		bias, not being dialogic?
\item (discuss at the end if there is time)
	\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Goal: Train/finetune models to be ``dialogic''}

\vfill

\textbf{What is hard about this?}

	\begin{itemize}
		\item QUESTION: Why don't we just finetune the model
		on dialog data?
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Goal: Train/finetune models to be ``dialogic''}

\vfill

\textbf{Key idea}

	\begin{itemize}
		\item Preference feedback (binary or
                  ranking)
                  \item Given two GPT answers: which is
                    better?
                    \item This feedback is easy to give for
                      annotators.
                      \item In contrast:
                    \item Writing good GPT answers for
                      training is hard.
                    \item It is hard to describe what is good/bad,
                      what could be improved.
	\end{itemize}

\vfill

\end{vbframe}


\input{roadmap.tex}

\section{Original RLHF work: The backflipper}

\begin{vbframe}{Origin of RLHF: Learn how to backflip}

\vfill

\textbf{How to label data for training a backflipper?}

	\begin{itemize}
		\item It is very very costly on the level of
		regular supervised training: telling the
		backflipper what exactly to do to backflip.
                \item Alternative: Present two different
		attempts to backflip
                \item Have humans provide one bit of
		information which one is better?
\item https://openai.com/research/learning-from-human-preferences
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Backflipping: What we want to learn}

\vfill

\textbf{This animation shows what we want to learn}

	\begin{itemize}
		\item \href{https://images.openai.com/blob/cf6fdf49-ea9e-489d-a1f1-9753291cd09e/humanfeedbackjump.gif}{\beamergotobutton{trained
		backflipper}}

	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Training process}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/trainingprocess.png}
\end{figure}

\begin{itemize}
	\item AI agent randomly initialized
	\item Periodically, the human provides feedback on
	two video clips: which is better
        \item Human feedback is used to build reward predictor
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{The human feedback part of RLHF}

\vfill

\textbf{Human chooses one of two clips = one bit}

	\begin{itemize}
		\item \href{https://player.vimeo.com/video/754042470?h=e64a40690d&badge=0&autopause=0&player_id=0&app_id=58479}{\beamergotobutton{example
		of human feedback}}

	\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Summary}

\vfill

\textbf{}

	\begin{itemize}
		\item Using a simple
		proxy for a complex goal, or getting the
		complex goal a bit wrong, can lead to
		undesirable and even dangerous behavior.
		\item So one step towards building safe AI
		systems is to remove the need for humans to
		write goal functions.
                \item
		RLHF: an algorithm that can infer
		what humans want by being told which of two
		proposed behaviors is better.
\item RLHF needed 900 bits of feedback from a human
		evaluator to learn to backflip.
                \item This works great because backflipping
                is \textbf{a seemingly
	simple task that is simple to judge but challenging
	to specify.}
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{QUESTION}

\vfill

\textbf{Compare backflipper with LLM use cases}

	\begin{itemize}
        \item Basic idea of applying RLHF to LLMs: ask human
        to rank different answers to request.
		\item What do they have in common?
                \item How are they different?

	\end{itemize}

\vfill

\end{vbframe}



\input{roadmap.tex}


\section{RLHF for GPT: Introduction}


\begin{vbframe}{How to make the model ``dialogic''}

\vfill

\textbf{Three steps}

	\begin{itemize}
		\item Finetuning on human-written dialogs
                \item Create a reward model that measures
		quality of dialogs -- not based on dialogs,
		but on preferences which dialogs are
		better/worse.
                \item Use reward model for further training
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Three steps}


\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/threesteps.png}
\end{figure}


\end{vbframe}

\begin{vbframe}{SFT model}

\vfill

\textbf{SFT = supervised finetuning}

	\begin{itemize}
	\item Collect demonstration data
        \item Labelers provide demonstrations of the
          desired behavior on the input prompt distribution
\item Continued pretraining of GPT3
\item Main difficulty of this step: collect good data from annotators                  
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Input prompt distribution}

\vfill

\textbf{The basis for SFT training dataset}

	\begin{itemize}
	\item Primarily prompts submitted to an earlier
          version of InstructGPT
          \item Deduplication
        \item At most 200 per user ID
        \item At the very beginning: bootstrapping
          \item For bootstrapping, prompts are also written
            by annotators
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Dataset sizes}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/datasetsize.png}
\end{figure}

\begin{itemize}
	\item RM model is trained on ranked pairs, so the
	actual size of the RM training set is much larger.
	\item QUESTION: Are these small datasets or large datasets?
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{API prompt dataset}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/apipromptdataset.png}
\end{figure}

\begin{itemize}
	\item Table 1: Use case categories in RM dataset
        \item Prompts  submitted to
        InstructGPT model
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{Metadata for model output collected from labelers}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/instructgpttable3b.png}
\end{figure}

\begin{itemize}
	\item Not used for training, just for analysis?
	\item Gives a good sense of what the problem
	behaviors of LLMs are
	that we are trying to eliminate.
        \item This is for RM training/validation (page 33).
\end{itemize}

\vfill

\end{vbframe}

%\end{document}

\input{roadmap.tex}


\section{RM and PPO models}








\begin{vbframe}{Instructions to labelers for evaluation of
	model outputs (1)}

	\begin{itemize}


\item You are given a text-based description of a task, submitted
by a user. This task description may be in the form of an
explicit instruction (e.g. “Write a story about a wise
frog.”). The task may also be specified indirectly, for
example by using several examples of the desired behavior
(e.g. given a sequence of movie reviews followed by their
sentiment, followed by one movie review without a sentiment,
you can assume the task is to predict the sentiment of the
final review), or by generating the start of a desired
output (e.g. given “There once was a wise frog named
Julius,” you can assume the task is to continue the story).
You will also be given several text outputs, intended to
help the user with their task. Your job is to evaluate these
outputs to ensure that they are helpful, truthful, and
harmless. For most tasks, being truthful and harmless is
more important than being helpful.



	\end{itemize}



\end{vbframe}









\begin{vbframe}{Instructions to labelers for evaluation of
	model outputs (2)}




	\begin{itemize}
\item For most tasks, being harmless and truthful is more
important than being helpful. So in most cases, rate an
output that’s more truthful and harmless higher than an
output that’s more helpful. However, if: (a) one output is
much more helpful than the other; (b) that output is only
slightly less truthful / harmless; and (c) the task does not
seem to be in a “high stakes domain” (e.g. loan
applications, therapy, medical or legal advice, etc.); then
rate the more helpful output higher. When choosing between
outputs that are similarly helpful but are untruthful or
harmful in different ways, ask: which output is more likely
to cause harm to an end user?
\item A guiding principle for deciding on borderline cases: which
output would you rather receive from a customer assistant
who is trying to help you with this task?
	\end{itemize}



\end{vbframe}





\begin{vbframe}{Web interface for labelers (1)}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/webinterface1.png}
\end{figure}

\begin{itemize}
	\item This first part evaluates each output individually.
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Web interface for labelers (2)}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/webinterface2.png}
\end{figure}

\begin{itemize}
	\item In the second part, labelers rank all the outputs for a given prompt.
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Example of a prompt written by a labeler and
model responses}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/labelerpromptexample.png}
\end{figure}

\begin{itemize}
	\item (not perfect instruction following, but humans
	don't either)
\end{itemize}


\vfill

\end{vbframe}

\begin{vbframe}{Reward model (1)}


\begin{figure}
\centering
\includegraphics[width = 4cm]{figure/threesteps.png}\\
\includegraphics[width = 10cm]{figure/rewardloss.png}
\end{figure}


\begin{itemize}
	\item Start with SFT model, final layer removed
        	\item Input: prompt+response, output: reward
        \item Only uses 6B model (not 175B)
        \item Comparisons  for a
        	given prompt are
        	highly correlated.
                \item $\rightarrow$ Put them in
        	a single batch (prevents overfitting).
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Reward model (2)}


\begin{figure}
\centering
\includegraphics[width = 4cm]{figure/threesteps.png}\\
\includegraphics[width = 10cm]{figure/rewardloss.png}
\end{figure}


\begin{itemize}
        \item Comparisons  for a
        	given prompt are
        	highly correlated.
                \item $\rightarrow$ Put them in
        	a single batch (prevents overfitting).
\item QUESTION: Think through mechanics
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{PPO model}


\begin{figure}
\centering
\includegraphics[width = 6cm]{figure/threesteps.png}

\includegraphics[width = 10cm]{figure/objectiveppo.png}
\end{figure}


%\begin{itemize}
%	\item Start with SFT model?
%\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{RLHF details}


\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/rlhfdetails.png}
\end{figure}


%\begin{itemize}
%	\item Start with SFT model?
%\end{itemize}

\vfill

\end{vbframe}




\input{roadmap.tex}


\section{Evaluation}





\begin{vbframe}{Main evaluation result}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/mainresult.png}
\end{figure}

\begin{itemize}
	\item PPO-ptx: tries to preserve behavior on
	pretraining data $\rightarrow$ less regression on
	public NLP datasets.
	\item SFT and PPO look like they are about equally
	powerful.
        \item Note that win rate is ``only'' about .6.
        \item So PPO wins in 6 cases, SFT in 4.
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Improvement on four dimensions}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/evaluationon4categories.png}
\end{figure}

\begin{itemize}
	\item PPO models better than GPT throughout
	\item SFT better than PPO on hallucinations (why?)
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Important hyperparameter: KL reward
	coefficient beta}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/klrewardcoefficient.png}
\end{figure}

\vfill

\end{vbframe}


\begin{vbframe}{Summarize/answer questions about code}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/questionsaboutcode.png}
\end{figure}

\begin{itemize}
	\item Example shows: InstructGPT more reliably handles questions
	about code;
        GPT3 requires more careful prompting
	about code.
        \item The training data contains almost no examples
	of this,
 so it's surprising that this works!
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Q\&A for languages other than English}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/nonenglish.png}
\end{figure}

\begin{itemize}
	\item InstructGPT more reliably follows instructions
	in other languages (but will generate English
	answers sometimes).
        \item The training data is almost exclusively
	English, so it's surprising that this works!
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{``Zero-shot'' instruction following}



\begin{block}{RLHF works for unseen instructions?}
We qualitatively probe InstructGPT’s capabilities, and find
that it is able to follow instructions for summarizing code,
answer questions about code, and sometimes follows
instructions in different languages, \textbf{despite these
instructions being very rare in the finetuning
distribution}. In contrast, GPT-3 can perform these tasks but
requires more careful prompting, and does not usually follow
instructions in these domains. This result is exciting
because it suggests that our models are able to \textbf{generalize
the notion of “following instructions.”} They retain some
alignment even on tasks for which they get very little
direct supervision signal.
\end{block}


\begin{itemize}
	\item This can be said to be the central contribution
	of the InstructGPT work!
\end{itemize}

\vfill


\end{vbframe}




\begin{vbframe}{QUESTION}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/mainresult.png}
\end{figure}

\begin{itemize}
        \item Why does PPO improve performance compared to
        just using SFT?
        \item PPO wins in 6 cases, SFT in 4: Is all the
        investment in PPO worth it?
\end{itemize}

\vfill

\end{vbframe}


\input{roadmap.tex}


\section{Things that don't work so well}











\begin{vbframe}{Getting scolded}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/gettingscolded.png}
\end{figure}

\begin{itemize}
\item Some LLMs sometimes scold the human.
        \item Should we eliminate this or not?
	\item Wasn't part of initial instructgpt effort
	\item \href{https://www.youtube.com/watch?v=L_Guz73e6fw}{Sam
	Altman on youtube}
\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{False premises}

\vfill

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/falsepremises.png}
\end{figure}

\begin{itemize}
	\item InstructGPT does not handle false premises well.
\item False premises are not sufficiently represented in the
training data.
\end{itemize}

\vfill

\end{vbframe}




\begin{vbframe}{Hedging}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/hedging.png}
\end{figure}

\begin{itemize}
	\item InstructGPT can overly hedge.
\item The reason could be that labelers reward ``epistemic humility''.
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Difficult constraints}

\vfill

\textbf{The aligned model cannot handle certain constraints}

	\begin{itemize}
		\item Write a summary in a specified number
		of sentences
                \item Multiple constraints: list 10 movies
		in the 1930s set in France
                \item QUESTION: Why is this difficult?
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Difficulty handling constraints}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/10happy1.png}
\end{figure}

%\begin{itemize}
%\item The reason could be that labelers reward ``epistemic humility''.
%\end{itemize}

\vfill

\end{vbframe}
\begin{vbframe}{Difficulty handling constraints}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/10happy2.png}
\end{figure}

%\begin{itemize}
%\item The reason could be that labelers reward ``epistemic humility''.
%\end{itemize}

\vfill

\end{vbframe}
\begin{vbframe}{Difficulty handling constraints}

\vfill

\begin{figure}
\centering
\includegraphics[width = 9cm]{figure/10happy3.png}
\end{figure}

%\begin{itemize}
%\item The reason could be that labelers reward ``epistemic humility''.
%\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{InstructGPT often follows harmful prompts}

\vfill

\begin{figure}
\centering
\includegraphics[width = 8cm]{figure/harmfulpromptfollowing.png}
\end{figure}

\begin{itemize}
	\item So there is still work to do \ldots
\end{itemize}


\vfill

\end{vbframe}






\begin{vbframe}{Attack suffixes}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/attacksuffixes.png}
\end{figure}

\begin{itemize}
\item Can ``aligned'' LLMs be made to produce bad content?
\item Prior jailbreaks: brittle, require human ingenuity
\item Attack suffix: automatic, robust across LLMs
\item Method: Greedy Coordinate Descent
	\item Seems to disable all guardrails?
        \item Surprisingly: generalizes across language
        models?
        \item So is RLHF methodology just a  hack, easy to undo?
        Instead we should be looking for principled solutions?
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{Attack suffix: Example}

\vfill

\begin{figure}
\centering
\includegraphics[width = 12cm]{figure/attacksuffixex.png}
\end{figure}

\begin{itemize}
\item QUESTION: Speculate why this may work.
\end{itemize}



\vfill

\end{vbframe}


\begin{vbframe}{Word repetition attack}

\vfill

\begin{figure}
\centering
\includegraphics[width = 10cm]{figure/extractmemory1.png}
\end{figure}




\vfill

\end{vbframe}

\begin{vbframe}{Word repetition attack}

\vfill

\begin{figure}
\centering
\includegraphics[width = 7cm]{figure/extractmemory2.png}
\end{figure}

\begin{itemize}
\item Only works for OpenAI models?
\item Has been disabled
\end{itemize}



\vfill

\end{vbframe}

\begin{vbframe}{New York Times 2024-01-02: ChatGPT limitations}

\vfill

\begin{itemize}
\item User study: 750 white-collar workers, three
        conditions: no GPT, GPT with training, GPT without
        training


\item Brainstorming task: 40\% better with GPT, 20\% less time


	\item
        On a task that required reasoning based on evidence,
        however, ChatGPT was not helpful at all. In this
        group, volunteers were asked to advise a corporation
        that had been invented for the study. They needed to
        interpret data from spreadsheets and relate it to
        mock transcripts of interviews with executives.


	\item In interviews conducted after the experiment,
	“people told us they neglected to check because it’s
	so polished, it looks so right,” said Hila
	Lifshitz-Assaf, a management professor at Warwick
	Business School in Britain.
 




\end{itemize}


\vfill

\end{vbframe}


\begin{vbframe}{New York Times 2024-01-02: ChatGPT limitations}

\vfill

\begin{itemize}
 

	\item
“If you haven’t had an existential crisis about this tool,
then you haven’t used it very much yet,” said another
co-author, Ethan Mollick, a management professor at the
Wharton School at the University of Pennsylvania.

\item “GPT is like junk food: hard to resist, easy to
consume but ultimately bad for the consumer.”


\end{itemize}


\vfill

\end{vbframe}


\input{roadmap.tex}


\section{Discussion}





\begin{vbframe}{Refusals}

\vfill

\begin{itemize}
	\item Questions it should not answer.
\item Separate mechanism (external controller) responsible for refusals
	\item \href{https://www.youtube.com/watch?v=L_Guz73e6fw}{Sam
	Altman on youtube}
\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{GPT3 vs GPT4}

\vfill

\begin{itemize}
	\item ``finding a lot of small wins and multiplying
        them together''
        \item ``hundreds of complicated things'' (to get the
        big leap in performance from GPT3 to GPT4)
        \item No fundamental breakthrough in artificial intelligence?
	\item \href{https://www.youtube.com/watch?v=L_Guz73e6fw}{Sam
	Altman on youtube}
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Who are we aligning to?}

\vfill

\textbf{What we align to is determined by:}

	\begin{itemize}
		\item The labelers (from US and Southeast
		(South?) Asia)
		\item OpenAI (through detailed directions
		they give to labelers)
		\item OpenAI customers and their end users
		(that's where the prompts come from)
                \item There clearly are many groups whose
		values are not represented: cultural,
		geographic, age, education etc.
                \item So there is no such thing as
		value-neutral alignment.
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Should we have multiple GPTs with different values?}

\vfill

\textbf{Who is allowed to choose the values for alignment?}

	\begin{itemize}
		\item Political parties?
		\item Governments?
		\item Extremist organizations?
                \item Criminals?
                \item Uncensored models:
                \href{https://erichartford.com}{Eric Hartford}
                \item QUESTION: How can we prevent unaligned
		LLMs from wreaking havoc?
                %(through
		%legislation, community activism, what other
		%options are there?)
	\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{QUESTION}

\vfill

\textbf{RLHF approach: create something flawed, then fix it}

	\begin{itemize}
		\item Can we create something instead that
		is not so flawed?
                \item Can we train an LLM that does not
		suffer from hallucinations, harmful content,
		bias, not being dialogic?
	\end{itemize}

\vfill

\end{vbframe}







\input{roadmap.tex}


\section{Epilog}





\begin{vbframe}{Reward model}

\vfill

\textbf{Clearer training signal through batching}

\begin{tabular}{l|l|l}
&\multicolumn{2}{c}{$x_1 <x_2<x_3$}\\\hline\hline
&\begin{tabular}{l}
one batch\\
$x_1<x_2$\\
$x_2<x_3$\\
$x_1<x_3$
\end{tabular}&
\begin{tabular}{lll}
batch 1&
batch 2&
batch 3\\
$x_1<x_2$&
$x_2<x_3$&
$x_1<x_3$
\end{tabular}\\\hline\hline
$x_2$? & clear: ``don't move'' & confusing signals 
\end{tabular}

\vfill

\end{vbframe}


\begin{vbframe}{ChatGPT}

\vfill

\textbf{ChatGPT vs.\ InstructGPT (1)}

	\begin{itemize}
		\item \href{https://openai.com/blog/chatgpt}{\beamergotobutton{OpenAI}}
		\item
                We trained this model using Reinforcement
		Learning from Human Feedback (RLHF), using
		the same methods as InstructGPT, but with
		slight differences in the data collection
		setup. We trained an initial model using
		supervised finetuning: human AI trainers
		provided conversations in which they played
		both sides—the user and an AI assistant. We
		gave the trainers access to model-written
		suggestions to help them compose their
		responses. \textbf{We mixed this new dialog
		dataset with the InstructGPT dataset, which
		we transformed into a dialog format.}
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{ChatGPT}

\vfill

\textbf{ChatGPT vs.\ InstructGPT (2)}

	\begin{itemize}
		\item \href{https://openai.com/blog/chatgpt}{\beamergotobutton{OpenAI}}
		\item
To create a reward model for reinforcement learning, we
		needed to collect comparison data, which
		consisted of two or more model responses
		ranked by quality. To collect this data, we
		took conversations that AI trainers had with
		the chatbot. We randomly selected a
		model-written message, sampled several
		alternative completions, and had AI trainers
		rank them. Using these reward models, we can
		finetune the model using Proximal Policy
		Optimization. \textbf{We performed several
		iterations of this process.}
	\end{itemize}

\vfill

\end{vbframe}


\begin{vbframe}{Limitations}

\vfill

\textbf{Reward hacking}

	\begin{itemize}
		\item \href{https://medium.com/@prdeepak.babu/reward-hacking-in-large-language-models-llms-c57abbc0cde7}{\beamergotobutton{Medium}}
		\item
``Reward hacking is a phenomenon observed in machine
		learning where a model learns to exploit the
		reward system to \textbf{achieve high scores without
		genuinely solving the intended problem.} The
		model identifies a \textbf{shortcut} within the
		problem space that allows it to minimize the
		loss function without truly learning the
		crucial aspects of the problem. This issue
		can lead to models that perform well on
		training data but fail to deliver in
		real-world scenarios.''
                \item That's why we need to mix reward
		objective with next-word and KL objectives.
\item QUESTION: What negative property of ChatGPT
		could be caused by reward hacking?
	\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{Limitations}

\vfill

\textbf{Reward hacking}

	\begin{itemize}
		\item
                A reward is a single number without
		``semantics'', i.e., there is zero
		information about what exactly is good or
		bad about a response, just a summary
		assessment.
                
\item Hallucinations are most likely a result of reward
		hacking: if a generation is fluent,
	interesting, responsive, helpful, authoritative, but
	contains an inaccuracy, the human may still give it a
	high reward.
        \item Divergence of the learned ``proxy award'' (the
	reward model) and the true reward function (what
	OpenAI wants)
        \item Note that PPO is better than SFT on all
	detailed measures, but not on hallucinations! (see
	earlier evaluation chart: page 45)
	\end{itemize}

\vfill

\end{vbframe}


\section{Administrivia}


\begin{vbframe}{Retake policy}

\vfill

	\begin{itemize}
\item
For CL, you will
only be admitted to the retake exam if you register for the
first exam, show up for it and make a serious attempt
to pass the exam.
\item This also applies for the case where you take the exam
and then cancel the exam -- you will not be allowed to take
part in the retake exam.
\item This applies only to  students in the CL programs.
\item Why?
\end{itemize}

\vfill

\end{vbframe}

\begin{vbframe}{HiWi for DL4NLP WiSe 24/25}

\vfill

	\begin{itemize}
\item We're looking for a HiWi for the class next year.
\item If you are interested, please contact me.
\item Email
address:
first name (hinrich) at hotmail.com
\end{itemize}

\vfill

\end{vbframe}



\begin{vbframe}{Last regular lecture on Jan 24}

\vfill

	\begin{itemize}
\item Round robin: topics
\item Vote
\end{itemize}

\vfill

\end{vbframe}


\endlecture
\end{document}
