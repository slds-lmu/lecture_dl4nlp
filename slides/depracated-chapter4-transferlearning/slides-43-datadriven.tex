\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\usepackage{colortbl}

\newcommand{\titlefigure}{figure/tokenize.png}
\newcommand{\learninggoals}{
\item Understand the importance of tokenization for Transfer Learning
\item Benefits of data driven tokenization over "generic" approaches}

\title{Transfer Learning}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Data-driven Tokenization}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{WordPiece}

\vfill

	\textbf{Voice Search for Japanese and Korean \href{https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37842.pdf}{\beamergotobutton{Schuster \& Nakajima (2012)}}}

	\begin{itemize}
		\item \textit{Specific Problems:} 
			\begin{itemize}
				\item Asian languages have larger basic character inventories compared to Western languages
				\item Concept of spaces between words does (partly) not exist
				\item Many different pronounciations for each character
			\end{itemize}
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{WordPiece}

\vfill

	\begin{itemize}
		\item \textit{WordPieceModel:} Data-dependent + do not produce OOVs
			\begin{enumerate}
				\item Initialize the the vocabulary with basic Unicode characters (22k for Japanese, 11k for Korean)\\
							\warning Spaces are indicated by an underscore attached before (of after) the respective basic unit or word (increases initial $|V|$ by up to factor 4)
				\item Build a language model using this vocabulary
				\item Merge word units that increase the likelihood on the training data the most, when added to the model
			\end{enumerate}
		\item Two possible stopping criteria:\\Vocabulary size \textit{or} incremental increase of the likelihood
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{WordPiece}

\vfill

	\textbf{Use for neural machine translation \href{https://arxiv.org/pdf/1609.08144.pdf}{\beamergotobutton{Wu et al. (2016)}}}

	\begin{itemize}
		\item \textit{Adaptions:} 
			\begin{itemize}
				\item Application to Western languages leads to a lower number of basic units ($\sim$ 500)
				\item Add space markers (underscores) \textit{only} at the beginning of words
				\item Final vocabulary sizes between 8k and 32k yield a good balance between accuracy and fast decoding speed\\(compared to around 200k from \href{https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37842.pdf}{\beamergotobutton{Schuster \& Nakajima (2012)}})
			\end{itemize}
	\end{itemize}
	
	\vspace{.3cm}
	
	\textit{Independent} \textbf{vs.} \textit{joint} \textbf{encodings for source \& target language}
	
	\begin{itemize}
		\item Sennrich et al. (2016) report better results for joint BPE
		\item Wu et al. (2016) use shared WordPieceModel to guarantee identical segmentation in source \& target language in order to facilitate copying rare entity names or numbers
	\end{itemize}
	
\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{SentencePiece \href{https://www.aclweb.org/anthology/D18-2012.pdf}{\beamergotobutton{Kudo et al. (2018b)}}}

\vfill

	\textbf{No need for Pre-Tokenization}

	\begin{itemize}
		\item BPE \& WordPiece require a sequence of words as inputs\\
					$\rightarrow$ Some sort of (whitespace) tokenization has to be performed before their application
		\item SentencePiece (as the name already reveals) doesn't need that\\
					$\rightarrow$ Can be applied to "raw" sentences\\
					$\rightarrow$ Consists of \textit{Normalizer}, \textit{Trainer}, \textit{Encoder} \& \textit{Decoder}\\
					$\rightarrow$ Under the hood, two different algorithms are implemented
					\begin{itemize}
						\item byte-pair encoding \href{https://www.aclweb.org/anthology/P16-1162.pdf}{\beamergotobutton{Sennrich et al. (2016)}}
						\item unigram language model \href{https://www.aclweb.org/anthology/P18-1007.pdf}{\beamergotobutton{Kudo et al. (2018a)}}
					\end{itemize}
		\item No language-specific pre-processing
	\end{itemize}
	
	\vspace{.3cm}
	
	$\Rightarrow$ Basically a nice, end-to-end usable system/pipeline

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Usage of different tokenizers}

\vfill

\textit{Disclaimer I:}\\You don't know these models yet, this is to give you an impression.\\

\vspace{.3cm}

\textit{Disclaimer II:}\\BPE will be introduced in the next chapteron the Transformer.

\vspace{.3cm}

	\begin{itemize}
		\item \textbf{WordPiece:}\\
					BERT, DistilBERT, ELECTRA, 
		\item \textbf{SentencePiece:}\\
					ALBERT, XLNet, T5
		\item \textbf{BPE:}\\
					Transformer, GPT-2, RoBERTa
	\end{itemize}
	
	\vspace{.3cm}
	
	$\Rightarrow$ Additional Resource: \href{https://huggingface.co/docs/transformers/tokenizer_summary}{\beamergotobutton{Overview on huggingface}}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
