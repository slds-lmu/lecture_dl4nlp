\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/transformer_sq.png}
\newcommand{\learninggoals}{
\item Understand BPE
\item Understand the Transformer Encoder + Decoder
\item Understand how they are connected
\item Understand the limitations for long sequences}

\title{Transformer}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Transformer for MT}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{WMT 2014 EN-to-DE and EN-to-FR}

\vfill

\textbf{Parallel training data:}

\includegraphics[width=.9\textwidth]{figure/wmt14}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{The BLEU Score}

describe BLEU for context

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Transformer for MT}

\vfill

The Transformer .. 

\begin{itemize}
	\item .. outperforms the previous SOTA models
	\item .. at a lower number of required training FLOPs
\end{itemize}

\vspace{.3cm}

\includegraphics[width=.9\textwidth]{figure/trafo-wmt}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Transformer for MT}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Transformer for MT}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
