\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/transformer_sq.png}
\newcommand{\learninggoals}{
\item Understand inner workings of BPE
\item Being able to compare BPE to other tokenization approaches}

\title{Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{BytePair encoding (BPE)}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{BytePair encoding (BPE)}

\vfill

\textbf{Data compression algorithm \href{https://www.derczynski.com/papers/archive/BPE_Gage.pdf}{\beamergotobutton{Gage (1994)}}}

	\begin{itemize}
		\item Considering data on a \textit{byte}-level
		\item Looking at pairs of bytes:
			\begin{enumerate}
				\item Count the occurrences of all byte pairs
				\item Find the most frequent byte pair
				\item Replace it with an unused byte
			\end{enumerate}
		\item Repeat this process until no further compression is possible
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{BytePair encoding (BPE)}

\vfill

\textbf{Open-vocabulary neural machine translation \href{https://www.aclweb.org/anthology/P16-1162.pdf}{\beamergotobutton{Sennrich et al. (2016)}}}
	
	\begin{itemize}
		\item Instead of looking at bytes, look at characters
		\item Motivation: Translation as an open-vocabulary problem
		\item Word-level NMT models:
			\begin{itemize}
				\item Handling out-of-vocabulary word by using back-off dictionaries
				\item Unable to translate or generate previously unseen words
			\end{itemize}
		\item Using BPE effectively \textit{solves} this problem, except for ..
			\begin{itemize}
				\item .. the occurence of unknown characters
				\item .. when all occurences in the training set were merged into "larger" symbols
							(Example: \textit{"safeguar"} and \textit{"safeguard"})
			\end{itemize}
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{BytePair encoding (BPE)}

\vfill

\textbf{Adapt BPE for word segmentation \href{https://www.aclweb.org/anthology/P16-1162.pdf}{\beamergotobutton{Sennrich et al. (2016)}}}

	\begin{itemize}
		\item \textit{Goal:} Represent an open vocabulary by a vocabulary of fixed size\\
					$\rightarrow$ Use variable-length character sequences 
		\item Looking at pairs of characters:
			\begin{enumerate}
				\item Initialize the the vocabulary with all characters plus end-of-word token
				\item Count occurrences and find the most frequent character pair,\\
							e.g. "A" and "B" (\warning Word boundaries are \textbf{not} crossed) \\
							\textit{[Side effect: Can be run on a dictionary w/ frequency counts]}
				\item Replace it with the new token "AB"
			\end{enumerate}
		\item Only one hyperparameter: Vocabulary size\\
					(Initial vocabulary + Specified no. of merge operations)\\
					$\rightarrow$ Repeat this process until given $|V|$ is reached
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example -- Setup \href{https://www.aclweb.org/anthology/P16-1162.pdf}{\beamergotobutton{Sennrich et al. (2016)}}}

\begin{lstlisting}[language=Python]
import re, collections

def get_stats(vocab):
  pairs = collections.defaultdict(int)
  for word, freq in vocab.items():
    symbols = word.split()
    for i in range(len(symbols)-1):
      pairs[symbols[i],symbols[i+1]] += freq
  return pairs
	
def merge_vocab(pair, v_in):
  v_out = {}
  bigram = re.escape(' '.join(pair))
  p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
  for word in v_in:
    w_out = p.sub(''.join(pair), word)
    v_out[w_out] = v_in[word]
  return v_out
\end{lstlisting}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example -- Merging \href{https://www.aclweb.org/anthology/P16-1162.pdf}{\beamergotobutton{Sennrich et al. (2016)}}}

\vspace{-.5cm}

\begin{lstlisting}[language=Python]
vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 
				 'n e w e s t </w>':6, 'w i d e s t </w>':3}

pairs = get_stats(vocab)
\end{lstlisting}

\vspace{-.75cm}

\begin{lstlisting}[language=Python]
>>> print(pairs)
defaultdict(<class 'int'>, {
						('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, 
						('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, 
						('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, 
						('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, 
						('i', 'd'): 3, ('d', 'e'): 3
})
\end{lstlisting}

\vspace{-.5cm}

\begin{lstlisting}[language=Python]
best = max(pairs, key=pairs.get)
vocab = merge_vocab(best, vocab)
\end{lstlisting}

\vspace{-.75cm}

\begin{lstlisting}[language=Python]
>>> print(best)
('e', 's')
>>> print(vocab)
{'l o w </w>': 5, 'l o w e r </w>': 2, 
 'n e w es t </w>': 6, 'w i d es t </w>': 3}
\end{lstlisting}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example -- Merging \href{https://www.aclweb.org/anthology/P16-1162.pdf}{\beamergotobutton{Sennrich et al. (2016)}}}

\vspace{-.5cm}

\begin{lstlisting}[language=Python]
vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 
				 'n e w e s t </w>':6, 'w i d e s t </w>':3}

num_merges = 10

for i in range(num_merges):
  pairs = get_stats(vocab)
  best = max(pairs, key=pairs.get)
  vocab = merge_vocab(best, vocab)
  print(best)
\end{lstlisting}

\vspace{-.5cm}

\begin{lstlisting}[language=Python]
('e', 's')
('es', 't')
('est', '</w>')
('l', 'o')
('lo', 'w')
('n', 'e')
('ne', 'w')
('new', 'est</w>')
('low', '</w>')
('w', 'i')
\end{lstlisting}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
