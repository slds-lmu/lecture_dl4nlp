\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%\newcommand{\titlefigure}{figure/gpt_sq.png}
\newcommand{\learninggoals}{
\item Understand the concept of regularization
\item Understand L2 regularization in more detail}

\title{Deep Learning basics}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Backpropagation}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{}

\vfill

\begin{itemize}
\item Forward propagation: Input information $\vec x$ propagates through network to produce output $\hat y$ (and cost $J(\vec\theta)$ in training)
\item Back-propagation: 
\begin{itemize}
 \item compute gradient w.r.t. model parameters
 \item Cost gradient propagates backwards through the network
\end{itemize}
\item Back-propagation is part of learning procedure (e.g. stochastic gradient descent), not learning procedure in itself.

\end{itemize}
\begin{center}
%\includegraphics[width = 0.5\textwidth]{./}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Chain Rule of Calculus: Real Functions}

\vfill

\begin{itemize}
\item Let
$$x, y, z \in \mathbb{R}$$
$$f, g : \mathbb{R} \rightarrow \mathbb{R}$$
$$y = g(x)$$
$$z = f(g(x)) = f(y)$$
\item Then
$$\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$$
\end{itemize}
\begin{center}
%\includegraphics[width = 0.5\textwidth]{./}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Chain Rule of Calculus: Multivariate Functions}

\vfill

\begin{itemize}
\item Let
$$\vec x \in \mathbb{R}^m, \vec y \in \mathbb{R}^n, z \in \mathbb{R}$$
$$f : \mathbb{R}^n \rightarrow \mathbb{R}$$
$$g : \mathbb{R}^m \rightarrow \mathbb{R}^n$$
$$\vec y = g(\vec x)$$
$$z = f(g(\vec x)) = f(\vec y)$$
\item Then
$$\frac{\partial z}{\partial x_i} = 
		\frac{\partial z}{\partial y_1} \frac{\partial y_1}{\partial x_i} +
		\frac{\partial z}{\partial y_2} \frac{\partial y_2}{\partial x_i} + \ldots +
		\frac{\partial z}{\partial y_n} \frac{\partial y_n}{\partial x_i} =
\sum_{j=1}^n \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}$$
\item In order to write this in vector notation, we need to define the Jacobian matrix.
\end{itemize}
\begin{center}
%\includegraphics[width = 0.5\textwidth]{./}
\end{center}


\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Jacobian}

\vfill

\begin{itemize}
\item The Jacobian matrix is the matrix of all first-order partial derivatives of a vector-valued function. 
\[\vec J = \frac{\partial \vec g(\vec x)}{\partial \vec x} =
\begin{bmatrix}
    \frac{\partial g(\vec x)_1}{\partial x_1} & \cdots & \frac{\partial g(\vec x)_1}{\partial x_m} \\
    & & \\
    \frac{\partial g(\vec x)_2}{\partial x_1} & & \frac{\partial g(\vec x)_2}{\partial x_m}\\
    \vdots & \ddots & \vdots \\
    & &  \\
    \frac{\partial g(\vec x)_n}{\partial x_1}& \cdots &  \frac{\partial g(\vec x)_n}{\partial x_m} \\
\end{bmatrix} 
\]
\item How to write in terms of gradients?

\item We can write the chain rule as:\\
\vspace{.2cm}
%$\displaystyle{\nabla_{\vec x} z = }$ \pause $\displaystyle{\left(\nabla_{\vec y} z\right)^T \frac{\partial \vec y}{\partial \vec x}}$
$\displaystyle{\nabla_{\vec x} z = }$ \pause $\displaystyle{\left(\frac{\partial \vec y}{\partial \vec x}\right)^T \nabla_{\vec y} z}$

\end{itemize}
\begin{center}
%\includegraphics[width = 0.5\textwidth]{./}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Viewing the Network as a Graph}

\vfill

\begin{itemize}
\item Nodes are function outputs (can be scalar or vector valued)
\item Arrows are inputs
\item Example: Scalar multiplication $z = x y$.
\end{itemize}
\begin{center}
\includegraphics[width = 0.3\textwidth]{./figure/simple_mult_graph}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Which Function?}

\vfill

\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/sigmoid_graph}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Graph with Cost}

\vfill

\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/sigmoid_graph_with_loss}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Which Function?}

\vfill

\begin{itemize}
\item Parameter vectors can be converted to matrix as needed.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Forward Pass}

\vfill

\begin{itemize}
\item Green: known or computed.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss1}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Forward Pass}

\vfill

\begin{itemize}
\item Green: known or computed.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss2}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Forward Pass}

\vfill

\begin{itemize}
\item Green: known or computed.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss3}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Forward Pass}

\vfill

\begin{itemize}
\item End of forward pass (some steps skipped).
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss4}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Backward Pass}

\vfill

\begin{itemize}
\item Red: gradient of cost computed for node.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss_backward}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Backward Pass}

\vfill

\begin{itemize}
\item Red: gradient of cost computed for node.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss_backward2}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Backward Pass}

\vfill

\begin{itemize}
\item Red: gradient of cost computed for node.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss_backward3}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Backward Pass}

\vfill

\begin{itemize}
\item We have the gradients for all parameters, let's use them for SGD.
\end{itemize}
\begin{center}
\includegraphics[width = 0.4\textwidth]{./figure/relu_sigmoid_graph_with_loss_backward4}
\end{center}

\vfill

\end{vbframe}


\endlecture
\end{document}
