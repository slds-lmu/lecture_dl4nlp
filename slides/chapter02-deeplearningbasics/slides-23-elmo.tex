\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/elmo.jpg}
\newcommand{\learninggoals}{
\item Understand the contextualization of word embeddings
\item Get the intuition of feature-based Transfer Learning}

\title{Deep Learning Basics}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{ELMo \beamergotobutton{\href{https://aclanthology.org/N18-1202/}{Peters et al., 2018}}}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{Recap: Word vectors}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/linear-relationships.png}\\ 
		\beamergotobutton{\href{https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space}{Source: Google}}
	\end{figure}

	\begin{itemize}
		\item Information is encoded in (pre-)trained word embeddings
		\item Embeddings are used for tasks external to the training corpus
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

%\begin{vbframe}{Feature-based Transfer Learning}

%\vfill

%\textbf{Using word2vec:}
	
%	\begin{enumerate}
%		\item Self-supervised pre-training of word2vec ..
%			\begin{itemize}
%				\item Using a large, unannotated corpus
%				\item Objective: CBOW or Skip-gram
%			\end{itemize}
%		.. and extract the stored knowledge (i.e. embedding)\\
%		\item \textit{Alternative:} Download embeddings from the web,
%		e.g. \url{https://code.google.com/archive/p/word2vec/}
%		\item Perform a (supervised) task using the embeddings,
%		e.g. Sentiment classification using an LSTM network
%	\end{enumerate}
	
%\textit{The stored knowledge from the pre-trained model is extracted as is
%and is not further adapted to the actual domain/task of interest!}

%\vfill

%\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Contextuality}

\vfill

\textbf{1st Generation of neural embeddings are "context-free":}

	\begin{itemize}
		\item Breakthrough paper (word2vec): \beamergotobutton{\href{https://arxiv.org/abs/1301.3781}{Mikolov et al, 2013}}
		\item Followed by GloVe: \beamergotobutton{\href{https://aclanthology.org/D14-1162/}{Pennington et al, 2014}}
		\item FastText as extension of word2vec: \beamergotobutton{\href{https://arxiv.org/abs/1607.04606}{Bojanowski et al, 2016}}
	\end{itemize}
	
	\textbf{Why "Context-free"?}
	
	\begin{itemize}
		\item Models learn \textit{one single} embedding for each word
		\item Why could this possibly be problematic?
			\begin{itemize}
				\item "The \textit{default} setting of the function is xyz."
				\item "The probability of \textit{default} is rather high."
			\end{itemize}
		\item Would be nice to have different embeddings for these two occurrences
	\end{itemize}
	
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Contextual embeddings}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/elmo-embedding-robin-williams.png}\\ 
		\beamergotobutton{\href{https://jalammar.github.io/illustrated-bert/}{Source: Jay Alammar}}
	\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{embeddings from language models}

\vfill

\begin{itemize}
		\item Bidirectional language model (LM)
		\item Combines a forward LM $$p\left(w_{1}, w_{2}, \ldots, w_{N}\right)=\prod_{k=1}^{N} p\left(w_{k} | w_{1}, w_{2}, \ldots, w_{k-1}\right)$$
					and a backward LM $$p\left(w_{1}, w_{2}, \ldots, w_{N}\right)=\prod_{k=1}^{N} p\left(w_{k} | w_{k+1}, w_{k+2}, \ldots, w_{N}\right)$$
					to arrive at the following loglikelihood:
					$$\begin{array}{l}
\sum_{k=1}^{N}\left(\log p\left(w_{k} | w_{1}, \ldots, w_{k-1} ; \Theta_{x}, \overrightarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right. \\
\quad\left. +\log p\left(w_{k} | w_{k+1}, \ldots, w_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right)
\end{array}$$
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{ELMo embeddings}

\vfill

	\begin{itemize}
		\item Character-based (context-independent) token representations $$\vec x_k^{LM}$$
		\item Two-layer biLSTM as main architecture:
			\begin{itemize}
				\item Two context-dependent token representations \textit{per layer}, i.e.
							$$\overrightarrow{\vec h}_{k, j}^{L M}\; \mbox{\&}\; \overleftarrow{\vec h}_{k, j}^{L M}\; \mbox{for the $k$-th token in the $j$-th layer.}$$
				\item Four context-dependent token representations in total: 
							$$\left\{\overrightarrow{\vec h}_{k, j}^{L M}, \overleftarrow{\vec h}_{k, j}^{L M} | j = 1, 2\right\}$$
			\end{itemize}
		\item Five representations per token in total:
					$$\begin{aligned}
R_{k} &=\left\{\vec x_{k}^{L M}, \overrightarrow{\vec h}_{k, j}^{L M}, \overleftarrow{\vec h}_{k, j}^{L M} | j=1, \ldots, L\right\} \\
&=\left\{\vec h_{k, j}^{L M} | j = 0, 1, 2\right\}
\end{aligned}$$
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Graphical representation}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/elmo-pretrained-bilm}\\ 
		\beamergotobutton{\href{https://slds-lmu.github.io/seminar_nlp_ss20/transfer-learning-for-nlp-i.html}{Source: Carolin Becker}}
	\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Graphical representation}

\vfill
			
	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/elmo-pretrained-bilm-2}\\ 
		\beamergotobutton{\href{https://slds-lmu.github.io/seminar_nlp_ss20/transfer-learning-for-nlp-i.html}{Source: Carolin Becker}}
	\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Task Adaption}

\vfill

\textbf{Including ELMo in downstream tasks:}

	\begin{itemize}
		\item Calculate task-specific weights of all five representations:
					$$\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \vec h_{k, j}^{L M},$$
					where the $\vec h_{k, j}^{L M}$ are \textbf{not trainable} anymore.
		\item Trainable parameters during the adaption:
			\begin{itemize}
				\item $s_{j}^{t a s k}$ are trainable (softmax-normalized) weights
				\item $\gamma^{t a s k}$ is a trainable scaling parameter
			\end{itemize}
	\end{itemize}

	\textbf{Advantages over context free-embeddings:}

	\begin{itemize}
		\item Task-specific model has access to \textit{multiple} representations of each token
		\item Model learns to which degree to use the different representations depending on the task at hand
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Task Adaption}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 12cm]{figure/elmo-adaption}\\ 
		\beamergotobutton{\href{https://slds-lmu.github.io/seminar_nlp_ss20/transfer-learning-for-nlp-i.html}{Source: Carolin Becker}}
	\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Performance}

\vfill
			
	\begin{figure}
		\centering
		\includegraphics[width = 12cm]{figure/elmo-sota.png}\\ 
		\beamergotobutton{\href{https://aclanthology.org/N18-1202/}{Source: Peters et al., 2018}}
	\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Summary}

\vfill

	\begin{itemize}
		\item Embeddings are (bidirectionally!) contextualized \\
					(as opposed to word2vec)
		\item Embeddings are \textit{not} adapted to target domain/task \\
					(similar as for word2vec)
		\item Additional weights are learned for each downstream task \\
					(i.e. besides the embeddings, no shared knowledge across tasks)
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
