\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%\newcommand{\titlefigure}{figure/gpt_sq.png}
\newcommand{\learninggoals}{
\item Understand the concept of regularization
\item Understand L2 regularization in more detail}

\title{Deep Learning basics}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Regularization}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{Regularization}

\vfill

\begin{center}
\includegraphics[width = 0.32\textwidth]{./figure/prediction_overfit}
\includegraphics[width = 0.32\textwidth]{./figure/prediction_underfit}
\includegraphics[width = 0.32\textwidth]{./figure/prediction_smooth}
%\includegraphics[width = 0.5\textwidth]{./}
\end{center}
\begin{itemize}
%\item we must design our machine learning algorithms to perform well on a speci?c task.
%\item build set of preferences into the learning algorithm.
%\item When these preferences are aligned with
%the learning problems we ask the algorithm to solve, it performs better.
\item Overfitting vs. underfitting
\item Regularization: Any modification to
a learning algorithm for reducing its generalization error but not its training error
\item Build a preference into ML algorithm for one solution in hypothesis space over another
\item Solution space is still the same
\item Unpreferred solution is penalized: only chosen if there it fits training data much better
%it fits the training data significantly better than the preferred solution.
\end{itemize}
\begin{center}
%\includegraphics[width = 0.5\textwidth]{./}
\end{center}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{L2-Regularization}

\vfill

\begin{itemize}
\item Large parameters $\to$ overfitting
\begin{center}
$\sigma(x)$ \hspace{1.85cm}  $\sigma(2x)$\hspace{1.85cm} $\sigma(100x)$
\includegraphics[width = 0.25\textwidth]{./figure/sigmoid1x}
\includegraphics[width = 0.25\textwidth]{./figure/sigmoid2x}
\includegraphics[width = 0.25\textwidth]{./figure/sigmoid100x}
\end{center}
\item Prefer models with smaller feature weights.
\item Popular regularizers: 
\begin{itemize}
 \item Penalize large L2 norm.
 \item Penalize large L1 norm (aka LASSO, induces sparsity)
\end{itemize}
\end{itemize}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Regularization}

\vfill

\begin{itemize}
\item Add term that penalizes large l2 norm.
\item The amount of penalty is controlled by a parameter $\lambda$
\begin{itemize}
 \item Linear regression:
 $$J(\vec\theta) = MSE(\vec\theta) + \frac{\lambda}{2}\vec{\theta}^T \vec{\theta}$$
 \item Logistic regression:
 $$J(\vec\theta) = NLL(\vec\theta) + \frac{\lambda}{2}\vec{\theta}^T \vec{\theta}$$
\end{itemize}
\item From a Bayesian perspective, l2-regularization corresponds to a Gaussian prior on the parameters.
\end{itemize}


\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{L2-Regularization}

\vfill

\begin{itemize}
\item The surface of the objective function is now a combination of the original cost, and the regularization penalty.
\begin{center}
\includegraphics[width = 0.65\textwidth]{./figure/l2_regularization}
\end{center}
\end{itemize}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{L2-Regularization}

\vfill

\begin{itemize}

\item Gradient of regularization term:
$$\nabla_{\vec\theta} \frac{\lambda}{2}\vec{\theta}^T \vec{\theta} = \lambda \vec{\theta} $$
\item Gradient descent for regularized cost function:
$$\vec \theta_{t+1} := \vec \theta_t - \eta \nabla_{\vec\theta}( NLL(\vec\theta_t) + \lambda\vec{\theta}_t^T \vec{\theta}_t)$$
$$\Leftrightarrow$$
$$\vec \theta_{t+1} := (1 - \eta \lambda) \vec \theta_t - \eta \nabla_{\vec\theta}NLL(\vec\theta_t)$$
\end{itemize}

\vfill

\end{vbframe}


\endlecture
\end{document}
