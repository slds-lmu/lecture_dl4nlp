\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\usepackage{animate}

\newcommand{\titlefigure}{figure/metrics.png}
\newcommand{\learninggoals}{
\item Metrics for text output
\item Task-specific metrics
\item Task-agnostic model evaluation}

\title{BERT}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Measuring Performance}
\lecture{Deep Learning for NLP} % stays constant

% ------------------------------------------------------------------------------

\begin{frame}{ARLMs}

\vspace{-1cm}

	\begin{figure}
		\centering
		\includegraphics[width=10cm,page=13]{figure/arlm.pdf}
	\end{figure}
	
\begin{itemize}
	\item \ques How can we measure ``performance`` on this task? 
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{arlm perplexity (1)}

\vfill

\begin{itemize}
	\item Well-defined for ARLMs; trickier for MLMs
	\item Intuitively: \textit{Amount of the model's surprisal when confronted with a sequence} (higher value means higher ``surprisal``)
	\item More technically: Measure of uncertainty of a probabilistic model
	\item Example: Perplexity of a fair $k$-sided die (uniform distribution) is $k$
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{arlm perplexity (2)}

\vfill
\begin{itemize}
	\item Autoregressive factorization of the sequence:
\end{itemize}

\begin{figure}
    \centering
    \animategraphics[loop,autoplay,height=1.5cm]{2}{figure/ppl_full/frame_}{0}{11}
		\citebutton{Source: huggingface}{https://huggingface.co/docs/transformers/perplexity}
\end{figure}

\begin{itemize}
	\item Log-probability of i-th token given context: $\log(p_\theta(w_i|w_{<i}))$
	\item[] more ``certain`` model $\to$ higher log-probability
	\item Aggregate$^1$ over the whole sequence:
				$$PPL = \exp\left(\frac{1}{t} \sum_{i=1}^t \log(p_\theta(w_i|w_{<i}))\right)$$
\end{itemize}

\vfill

\footnotesize{$^1$The choice of the logâ€™s base is basically arbitrary.}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{arlm perplexity (3)}

\vfill

\begin{itemize}
	\item \textit{Lower bound} is \textbf{1}, i.e. the model predicts \textit{every} token correctly (with a certainty of 100\%)
	\item \ques Is this really desirable?
	\item \textit{Upper bound} is $|V|$, i.e. the model only provides a random guess for every token with a probability of $\frac{1}{|V|}$
	\item Selection of state-of-the-art perplexities:
			\begin{itemize}
				\item \textbf{1B Word Benchmark} \citebutton{Chelba et al., 2013}{https://arxiv.org/abs/1312.3005} ($|V| = 800k$)\\
				PPL = 21,8 \citebutton{Dai et al., 2019}{https://aclanthology.org/P19-1285/}
				\item \textbf{WikiText-103} \citebutton{Merity et al., 2016}{https://arxiv.org/abs/1609.07843} ($|V|$ = 270k)\\
				PPL = 10,8 \citebutton{Shoeybi et al., 2019}{https://arxiv.org/abs/1909.08053}
				\item \textbf{Penn Treebank} \citebutton{Marcus et al., 1994}{https://aclanthology.org/H94-1020/} ($|V| = 10k$)\\
				PPL = 20,5 \citebutton{Brown et al., 2020}{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
			\end{itemize}
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{arlm perplexity (4)}

\vfill

\begin{itemize}
	\item \textit{Problem:} Fixed context size of models (e.g. 1024 for GPT-2) 
\end{itemize}

\begin{figure}
    \centering
    \animategraphics[loop,autoplay,height=1.5cm]{2}{figure/ppl_sliding/frame_}{18}{23}
		\citebutton{Source: huggingface}{https://huggingface.co/docs/transformers/perplexity}
\end{figure}

\begin{itemize}
	\item \textit{Possible solution:} Sliding window strategy
	\item Close approximation to ``true`` autoregressive decomposition
	\item \textit{Drawback:} Computationally expensive (individual forward pass for each token)
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{evaluating generated text (1)}

\vfill

\begin{itemize}
	\item \ques How can we evaluate the quality of generated text?
	\item \textit{Use cases:} 
			\begin{itemize}
				\item Machine translation
				\item Question answering (extractive or abstractive)
				\item Dialogue generation
				\item Text summarization
				\item Image Captioning
				\item Code generation
			\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{evaluating generated text (2)}

\vfill

\textbf{Machine Translation}

\begin{itemize}
	\item Metrics based on N-gram-overlap
			\begin{itemize}
				\item BLEU (cf. Chap. 3.1)
				\item ROUGE
				\item METEOR
			\end{itemize}
	\item Metrics based pre-trained models
			\begin{itemize}
				\item BertScore
				\item RLEURT
				\item METEOR
			\end{itemize}
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{evaluating generated text (3)}

\vfill

\textbf{Question Answering / Summarization / Dialogues}

\begin{itemize}
	\item Aspects to consider
			\begin{itemize}
				\item Factual correctness
				\item Fluency
				\item Stylistic aspects
				\item Engagement
				\item ...
			\end{itemize}
	\item Human evaluation?!
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{task-specific evaluation}

\vfill

\textbf{(Binary) Document-level Classification}

\begin{itemize}
	\item Accuracy
	\item Recall / Precision
	\item F1-Score
	\item ROC-Curve / AUC
	\item ...
\end{itemize}

\textbf{(Multi-Class) Document-level classification}

\begin{itemize}
	\item Micro- / Macro-averaged F1
	\item Class-specific accuracies
	\item ...
\end{itemize}
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}