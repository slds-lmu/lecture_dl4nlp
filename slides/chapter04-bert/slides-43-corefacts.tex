\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/bert.jpeg}
\newcommand{\learninggoals}{
\item Understand the use of the transformer encoder in this model
\item Understand the architectural components}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{BERT -- Architecture}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{Predecessors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=1]{figure/transfer_learning_timeline1_nlp.pdf}}
\end{vbframe}
\begin{vbframe}{Predecessors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=2]{figure/transfer_learning_timeline1_nlp.pdf}}
\end{vbframe}
\begin{vbframe}{Predecessors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=3]{figure/transfer_learning_timeline1_nlp.pdf}}
\end{vbframe}
\begin{vbframe}{Predecessors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=4]{figure/transfer_learning_timeline1_nlp.pdf}}
\end{vbframe}
\begin{vbframe}{Predecessors of BERT}
\hbox{\hspace{-3em} \includegraphics[width=12cm,page=5]{figure/transfer_learning_timeline1_nlp.pdf}}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{context: ulmfit and gpt}

\vfill

	\textbf{Shortcomings of ELMo:}

	\begin{itemize}
		\item No adaption of the Embeddings to target domain/task
		\item Sequential nature of LSTMs: Not fully parallelizable 
	\end{itemize}

	\vspace{.3cm}
	
	\textbf{Alleviations/Alternatives:}

	\begin{itemize}
		\item ULMFiT \citebutton{Howard and Ruder, 2018}{https://www.aclweb.org/anthology/P18-1031} is a uni-directional LSTM which is fine-tuned as a whole model on data from the target domain/task.
		\item GPT \citebutton{Radford et al., 2018}{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf} is a Transformer decoder which is fine-tuned as a whole model on data from the target domain/task.
	\end{itemize}
	
\vfill


\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{bert: key facts}

\vfill

\textit{\textbf{B}idirectional \textbf{E}ncoder \textbf{R}epresentations from \textbf{T}ransformers:}

\begin{itemize}
		\item Bidirectionally contextual model
		\item[] $\to$ The embeddings of a single token depend on its left- and on its right-side context (similar to ELMo, but better)
		\item Introduces new self-supervised objective(s)
		\item[] $\to$ MLM as \textit{necessity} for the architecture to work
		\item[] $\to$ Next-Sentence-Prediction as complementary objective\\(cf. next section)
		\item Completely replaces recurrent architectures by Self-Attention\\
					$+$ simultaneously includes bidirectionality into the embeddings\\
					$+$ fine-tuned as a whole					
\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{bert: key facts}

\vfill

	\begin{itemize}
		\item Transformer \textit{encoder} as backbone of the architecture
		\item 110M (340M) parameters in total for $BERT_{Base}$ ($BERT_{Large}$)
			\begin{itemize}
				\item 12 (24) Transformer encoder blocks
				\item Embedding size of $E = 768$ (1024)
				\item Hidden layer size $H = E$
				\item $A = H/64 = 12$ (16) attention heads
				\item Feed-forward size is set to $4H$
			\end{itemize}
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Core of BERT -- The Transformer Encoder}

\begin{figure}
	\centering
		\includegraphics[width = 3cm]{figure/bert-top.png}\\ 
		\includegraphics[width = 2.8cm]{figure/bert-bottom.png}\\ 
	\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{Vaswani et al. (2017)}
\end{figure}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{A remark on "Causality"}

\vfill

\textbf{Causality is an issue!}
	
\begin{itemize}
	\item Goal: Learn contextual representations for words/tokens
	\item \textit{Self-Supervision:} Input and target sequence are the same\\
				$\rightarrow$ We modify the input to create a meaningful task 
	\item \warning Unconstrained Self-Attention makes using the LM objective infeasible
	\item \ques Why is this the case?
	\pause
	\item[] Bidirectionality at a lower layer would allow a word to see itself at later hidden layers\\
				$\rightarrow$ The model would be allowed to cheat!\\
				$\rightarrow$ This would not lead to meaningful internal representations
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{ELMo vs. GPT vs. BERT}

\vfill

\begin{figure}
\centering
\includegraphics[width = 11cm]{figure/comparison-bert.png}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1810.04805.pdf}{Devlin et al. (2018)}
\end{figure}

\textbf{Major architectural differences:}

\begin{itemize}
\item ELMo uses two separate unidirectional models to achieve bidirectionality 
  $\rightarrow$ Only "\textit{shallow}" bidirectionality
\item GPT is not bidirectional, thus no issues concerning causality
\item BERT combines the best of both worlds: $$Self\text{-}Attention + (Deep)\;Bidirectionality$$
\end{itemize} 

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{input embeddings}

\begin{figure}
	\centering
	\includegraphics[width = 10cm]{figure/bert-input.png}\\ 
	\footnotesize{Source:} \href{https://arxiv.org/pdf/1810.04805.pdf}{Devlin et al. (2018)}
\end{figure}

\begin{itemize}
	\item Two concatenated sentences as input
	\item WordPiece tokenization (Wu et al., 2016) for the inputs\\
			$\rightarrow$ Vocabulary of 30.000 tokens
	\item Learned segment $+$ position embeddings
	\item Special \texttt{[CLS]} and \texttt{[SEP]} tokens
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
