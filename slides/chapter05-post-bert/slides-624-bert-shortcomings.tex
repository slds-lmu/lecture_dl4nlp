\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/bert.jpeg}
\newcommand{\learninggoals}{
\item Problem with the [MASK] token
\item Inter-token dependencies
\item Get aware of biases}

\title{Using the Transformer}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{BERT -- Shortcommings / Critique}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{Pretrain-finetune discrepancy}

\vfill

	\begin{itemize}
		\item BERT \textit{artificially} introduces \texttt{[MASK]} tokens during pre-training
		\item \texttt{[MASK]}-token does not occur during fine-tuning\\
					$\rightarrow$ Lacks the ability to model joint probabilities\\
					$\rightarrow$ Assumes independence of predicted tokens (given the context)
		\item Other pre-training objectives (e.g. language modeling) don't have this issue
		\item Further: BERT only learns from predicting the 15\% tokens which are \texttt{[MASK]}ed (or randomly replaced / kept as is)
	\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Independence assumption}

\vspace{1.5cm}

\textbf{\texttt{[MASK]-ing procedure}:}

\begin{itemize}
	\item "Given a sentence, predict \texttt{[MASK]}ed tokens"
	\item All \texttt{[MASK]}ed tokens are predicted based on the un-\texttt{[MASK]}ed tokens
	\item \textit{Implicit assumption:} Independence of \texttt{[MASK]}ed tokens
\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[width = 9cm]{figure/xlnet-objective}\\ 
		{\tiny Prediction of [New, York] given the factorization order [is, a, city, New, York]\\\footnotesize Source: \href{https://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf} \it Yang et al. (2019)}
	\end{figure}
	
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Maximum sequence length}

\begin{figure}
\centering
\includegraphics[width = 8.5cm]{figure/bert-problem.png}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{\footnotesize Vaswani et al. (2017)}
\end{figure}

\textbf{Limitation:}

\begin{itemize}
	\item BERT can only consume sequences of up to 512 tokens
	\item Two sentences for NSP are sampled such that $$length_{sentence A} + length_{sentence B} \leq 512$$
	\item Reason: Computational complexity of Transformer scales quadratically with the sequence length\\
				$\rightarrow$ Longer sequences are disproportionally expensive
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Bias}

\vfill

\begin{itemize}
	\item Already known to exist in static pre-trained embeddings
	\item E.g. for gender: \textit{Man} is to \textit{Doctor} as \textit{Woman} is to \textit{Nurse}
	\item BERT also learns the patterns from the data it is trained on
	\item Research on Detecting/Mitigating Bias receives a lot of attention
\end{itemize}


\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Bias -- Example \href{https://aclanthology.org/2021.acl-long.416.pdf}{\beamergotobutton{Nadeem et al. (2021)}}}

\vfill

\begin{itemize}
	\item Nadeem et al. (2021) create a data set for measuring bias in LMs
	\item Four categories: Gender, Profession, Race, Religion
	\item Two types of probes: Intra- and Inter-sentence test sets
\end{itemize}

\begin{figure}%
\includegraphics[width=7cm]{figure/stereoset.png}%
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Bias -- Example}

\vfill

\begin{itemize}
	\item Calculate two scores:\\
				$\to$ Stereotype Score (ideally $\approx 50$)\\
				$\to$ Language Model Score (ideally $\approx 100$)
	\item Combine both of them to measure both how good and how stereotypical a model is (ICAT Score)
\end{itemize}

\begin{figure}%
\includegraphics[width=7cm]{figure/stereoset2.png}%
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
