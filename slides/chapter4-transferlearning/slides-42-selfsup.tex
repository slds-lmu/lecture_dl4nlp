\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\usepackage{colortbl}

\newcommand{\titlefigure}{figure/selfsup.jpg}
\newcommand{\learninggoals}{
\item Understand the difference to other learning paradigms
\item Learn to recognize self-supervision when you see it}

\title{Transfer Learning}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Self-Supervision}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{frame}{Definition}

	\vfill 
	
	\textit{Unsupervised Learning:}

	\begin{itemize}
		\item No labels attached to the data
		\item Learn patterns / clusters from the features only
	\end{itemize}
	
	\textit{Supervised Learning:}

	\begin{itemize}
		\item (Gold) Labels attached to the data
		\item Learn from the association between features and labels
	\end{itemize}
	
	\textbf{Self-Supervised Learning:}

	\begin{itemize}
		\item No \textit{external} labels attached to the data\\
					$\to$ Samples with suitable labels can be generated from the known structure of the data itself 
		\item \textit{Technically} supervised learning, but \textit{no labeling effort} $+$ simultan-\\eous ability to generate massive amounts of labeled data points
	\end{itemize}
	
	\vfill 
	
\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Self-supervised objectives}

\vfill

\textbf{Recap: Language modeling}
	
	\begin{itemize}
		\item \textit{Training objective:} Given a context, predict the next word
	\end{itemize}
	
\begin{block}{Illustration (context size $=$ 2)}
\begin{tabular}{|c|c|c|cccccc|}
\hline
\cellcolor{blue!15}The & \cellcolor{blue!65}quick & brown & fox & jumps & over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad (the, quick)
\begin{tabular}{|c|c|c|c|ccccc|}
\hline
\cellcolor{blue!15}The & \cellcolor{blue!15}quick & \cellcolor{blue!65}brown & fox & jumps & over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad ([the, quick], brown)
\begin{tabular}{|c|c|c|c|ccccc|}
\hline
The & \cellcolor{blue!15}quick & \cellcolor{blue!15}brown & \cellcolor{blue!65}fox & jumps & over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad ([quick, brown], fox)
\begin{tabular}{|c|c|c|c|ccccc|}
\hline
The & quick & \cellcolor{blue!15}brown & \cellcolor{blue!15}fox & \cellcolor{blue!65}jumps & over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad ([brown, fox], jumps)
\end{block}
	
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Self-supervised objectives}

\vfill

\textbf{Recap: Skip-gram}
	
	\begin{itemize}
		\item \textit{Training objective:} Given a word, predict the neighbouring words
    \item \textit{Generation of samples:} Sliding fixed-size window over the text
	\end{itemize}
	
\begin{block}{Illustration}
\begin{tabular}{|c|c|c|cccccc|}
\hline
\cellcolor{blue!15}The & \cellcolor{blue!65}quick & \cellcolor{blue!65}brown & fox & jumps & over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad (the, quick); (the, brown)
\begin{tabular}{|c|c|c|c|ccccc|}
\hline
\cellcolor{blue!65}The & \cellcolor{blue!15}quick & \cellcolor{blue!65}brown & \cellcolor{blue!65}fox & jumps & over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad (quick, the); (quick, brown); (quick, fox)
\begin{tabular}{|c|c|c|c|ccccc|}
\hline
\cellcolor{blue!65}The & \cellcolor{blue!65}quick & \cellcolor{blue!15}brown & \cellcolor{blue!65}fox & \cellcolor{blue!65}jumps & over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad (brown, the); (brown, quick); (brown, fox); (brown, blue)
\begin{tabular}{|c|c|c|c|ccccc|}
\hline
The & \cellcolor{blue!65}quick & \cellcolor{blue!65}brown & \cellcolor{blue!15}fox & \cellcolor{blue!65}jumps & \cellcolor{blue!65}over & the & lazy & dog \\
\hline
\end{tabular}\\
$\Rightarrow$ \quad (fox, quick); (fox, brown); (fox, jumps); (fox, over)
\end{block}
	
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Self-supervised objectives}

\vfill

\textbf{Self-supervised objectives:}
	
	\begin{itemize}
		\item Skip-gram objective (cf. word2vec \href{https://arxiv.org/pdf/1301.3781.pdf}{\beamergotobutton{Mikolov et al. (2013a)}})
		\item Language modeling objective (cf. \href{http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf}{\beamergotobutton{Bengio et al. (2003)}})
		\item \textit{Masked language modeling (MLM)} objective (cf. chapter 6)\\
					$\rightarrow$ Replace words by a \texttt{[MASK]} token and train the model to predict
		\item \textit{Permutation language modeling (PLM)} objective (cf. chapter 6)\\
					$\rightarrow$ Autoregressive objective of XLNet
		\item \textit{Replaced token detection} objective (cf. chapter 6)\\
					$\rightarrow$ Requires two models: One performing MLM \& the second model to discriminate between actual and the predicted tokens
	\end{itemize}
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Pre-training vs. fine-tuning}

\vfill

\textbf{Pre-training:}

	\begin{itemize}
		\item Using unlabeled corpora in conjunction with self-supervised objectives 
					is commonly referred to as \textit{Pre-Training} the model
		\item Generation of samples for pre-training basically effortless, exploiting 
					the inherent structure of the text
		\item Construction of different self-supervised objectives, which are assumed
			\begin{itemize}
				\item to cover different phenomena better than the others
				\item to work more efficiently for learning
			\end{itemize}
	\end{itemize}
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Pre-training vs. fine-tuning}

\vfill

\textbf{Fine-tuning:}

	\begin{itemize}
		\item The second phase of transfer learning, i.e. adapting the pre-trained model
					to a labeled data set for a specific downstream task is referred to as \textit{Fine-Tuning}
		\item Far less labeled data required compared to a scenario w/o pre-training
		\item Also possible (cf. chapter 7): No fine-tuning, but .. 
			\begin{itemize}
				\item \textit{Zero-Shot Transfer} w/o ANY labeled data
				\item \textit{Few-Shot Transfer} w/ FEW labaled data points
			\end{itemize}
		\item In both of the latter cases, good pre-training becomes even more important
	\end{itemize}
	
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
