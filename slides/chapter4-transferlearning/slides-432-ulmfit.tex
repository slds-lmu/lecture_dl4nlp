\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/ulmfit_sq.png}
\newcommand{\learninggoals}{
\item tbd}

\title{Transfer Learning}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{ULMFiT (Howard \& Ruder, 2018)}
\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------

\begin{vbframe}{Fine-tuning approach}

\vfill

\textbf{Shortcomings of ELMo:}

	\begin{itemize}
		\item No adaption of the Embeddings to target domain/task
			\begin{itemize}
				\item Source \& target domain/task might be pretty different
				\item No good representations for domain-/task-specific words
			\end{itemize}
		\item Sequential nature of LSTMs:
			\begin{itemize}
				\item Not fully parallelizable (compared to Transformers, see next chapter)
				\item Fails to capture long-range dependency during contextualization
			\end{itemize}
	\end{itemize}

	\vspace{.3cm}
	
	\textbf{Alleviations/Alternatives:}

	\begin{itemize}
		\item ULMFiT \href{https://www.aclweb.org/anthology/P18-1031.pdf}{\beamergotobutton{Howard and Ruder, 2018}} is a uni-directional LSTM which is fine-tuned as a whole model on data from the target domain/task.
		\item GPT \href{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}{\beamergotobutton{Radford et al., 2018}} is a Transformer (decoder) which is fine-tuned as a whole model on data from the target domain/task.
	\end{itemize}
	
\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{ULMFiT \href{https://www.aclweb.org/anthology/P18-1031.pdf}{\beamergotobutton{Howard and Ruder, 2018}}}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 10cm]{figure/ulmfit-overview-new}\\ 
		\footnotesize{Source:} \href{https://slds-lmu.github.io/seminar_nlp_ss20/transfer-learning-for-nlp-i.html}{\footnotesize \it Carolin Becker}
	\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Architectural Details}

\vfill

	\begin{itemize}
		\item AWD-LSTMs \href{https://arxiv.org/pdf/1708.02182.pdf}{\beamergotobutton{Merity et al., 2017}} as backbone of the architecture
			\begin{itemize}
				\item DropConnect \href{http://proceedings.mlr.press/v28/wan13.pdf}{\beamergotobutton{Wan et al., 2013}}
				\item Averaged stochastic gradient descent (ASGD) for optimiziation
			\end{itemize}
		\item Embedding layer + three LSTM layers + Softmax Layer
		\item \textbf{LM fine-tuning:}
			\begin{itemize}
				\item Discriminative fine-tuning 
			\end{itemize}
		\item \textbf{Classifier fine-tuning:}
			\begin{itemize}
				\item Concat Pooling
				\item Gradual unfreezing
			\end{itemize}
	\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{...}

\vfill



\vfill

\end{vbframe}

\endlecture
\end{document}
