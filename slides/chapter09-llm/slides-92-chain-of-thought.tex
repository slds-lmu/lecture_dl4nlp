

\input{../../style/preamble}
\usepackage{xspace}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\def\cotpfull{chain-of-thought prompting\xspace}
\def\cotp{COT-P\xspace}

%\newcommand{\titlefigure}{figure/gpt_sq.png}
\newcommand{\learninggoals}{
\item illustrate \cotpfull and point out the benefits it brings to LLMs
\item illustrate tree-of-thought and point out the benefits it brings to LLMs 
}

\definecolor{texblue}{rgb}{0, 0, 1}
\def\myblue#1{\textcolor{texblue}{#1}}

\title{\cotpfull}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}
\lecturechapter{Large Language Models (LLMs)}

\begin{vbframe}{REINSTATE LECTURE COMMAND}

\end{vbframe}


%\lecture{Deep Learning for NLP}

% ------------------------------------------------------------------------------ 

\begin{vbframe}{\cotpfull motivation}

\vfill

How to boost the reasoning capabilities of LLMs? \citebutton{Wei et al., 2021}{https://arxiv.org/abs/2109.01652}

\begin{itemize}
\item Use formal approaches, e.g., logic, symbolic reasoning
    \begin{itemize}
    \item Example: \citebutton{BeliefBank}{https://arxiv.org/abs/2109.14723}
\item Difficult to train and deploy, not widely used
    \end{itemize}
\item Standard few-shot learning via prompting works for many tasks
    \begin{itemize}
    \item Still, it works poorly for many tasks that require reasoning
    \end{itemize}
\item \cotp
    \begin{itemize}
    \item A new form of few-shot prompting
    \item Each ``training example'' has the form <input, \textit{chain of thought}, output>
    \item chain of thought:\\ series of reasoning steps that
    lead to the final answer
    \item applications: complex, commonsense, symbolic
    reasoning tasks etc
    \end{itemize}

\end{itemize}

\vfill

\end{vbframe}

\begin{frame}{Neurosymbolic approach\\ (currently
    infrequently used)}

\vfill
	
	\begin{figure}
		\centering
		\includegraphics[height = 6cm]{figure/beliefbank} 
	\end{figure}

\vfill

\end{frame}


\begin{frame}{LLMs not good at reasoning tasks}

\vfill
	
	\begin{figure}
		\centering
		\includegraphics[height = 6cm]{figure/is450,90,of,500} 
	\end{figure}

\ques What is the problem here?
% left-to-right next-token prediction

\vfill

\end{frame}


% ------------------------------------------------------------------------------ 

\begin{vbframe}{\cotpfull paradigm}

\vfill

%\textbf{\cotp enables LLMs to tackle complex arithmetic, commonsense and symbolic reasoning tasks.}

\begin{figure}
    \centering
    \includegraphics{figure/chain_of_thought.png}\\
    \citebutton{Source: Wei et al., 2022}{https://arxiv.org/pdf/2201.11903.pdf}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------ 

\begin{vbframe}{\cotpfull paradigm}

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figure/chain_of_thought2.png}
\end{figure}

\end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Benefits of \cotpfull}

\vfill

\begin{itemize}
    \item Decompose multi-step problems and thus allocate more compute to problems requiring more reasoning steps
    \item By describing the reasoning, interpretability is increased. It provides the possibility to observe where reasoning went wrong
    \item It is closer to how humans solve tasks using language
    \item Language models, if
given a well designed chain-of-thought prompt, can
    solve problems they otherwise would not be able to solve.
\end{itemize}

\vfill

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Examples of \cotpfull}

\vfill

\textbf{Examples of $<$input, chain of thought, output$>$ triples for  commonsense and symbolic reasoning}

    \citebutton{Source: Wei et al., 2022}{https://arxiv.org/pdf/2201.11903.pdf}

THE FEW SHOTS (TRAINING EXAMPLES) ARE OMITTED FROM THESE
EXAMPLES TO SAVE SPACE, BUT THIS IS CHAIN OF THOUGHT
PROMPTING, THAT IS, THE MODEL IS PROMPTED WITH EXAMPLES OF
CHAIN OF THOUGHT REASONING.


\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Examples}

\vfill

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figure/cotex1.png}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Examples}

\vfill

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figure/cotex2.png}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Examples}

\vfill

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figure/cotex3.png}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Examples}

\vfill

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figure/cotex4.png}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Examples}

\vfill

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figure/cotex5.png}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Examples}

\vfill

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figure/cotex6.png}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{\cotp improves arithmetic}

\vfill
	
	\begin{figure}
		\centering
		\includegraphics[height = 6cm]{figure/cotperformance} 
	\end{figure}

\vfill

\end{vbframe}


\begin{vbframe}{\cotp improves arithmetic}

%\vfill

SVAMP: math word problems with varying structures; MAWPS:
repository unifying math problems from different sources;
    \citebutton{Source: Wei et al., 2022}{https://arxiv.org/pdf/2201.11903.pdf}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figure/cot_performance1.png}
\end{figure}

%\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{\cotp improves commonsense}

\vfill

CSQA: Contains around 200K dialogs with a total of 1.6M
turns. Further, unlike existing large scale QA datasets
which contain simple questions that can be answered from a
single tuple, the questions in the dialogs require a larger
subgraph of the KG. 


\begin{figure}
    \centering
    \includegraphics{figure/cot_performance2.png}\\
    \citebutton{Source: Wei et al., 2022}{https://arxiv.org/pdf/2201.11903.pdf}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------ 

\begin{vbframe}{tree-of-thought: motivation}

\vfill

\begin{itemize}
\item The token-level and left-to-right decisions of the autoregressive mechanism pose a limitation for:
    \begin{itemize}
    \item Tasks where initial decisions play a pivotal role
    \item Tasks requiring exploration or strategic lookahead
    \end{itemize}
\item Strategy to solve those:
    \begin{itemize}
    \item Maintain and explore diverse alternatives instead of just picking one
    \item Evaluate current status and look ahead or backtrack to make global decisions
    \end{itemize}

\end{itemize}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Tree-of-thought: prompting paradigm}

\vfill

Schematic illustrating three approaches to problem solving
with LLMs. Rectangle box = \textit{thought} = a coherent language sequence serving as an intermediate step in problem solving.

\begin{figure}
    \centering
    \includegraphics{figure/tot_vs_cot.png}\\
\citebutton{Yao et al., 2023}{https://arxiv.org/pdf/2305.10601.pdf}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------


\begin{vbframe}{Tree-of-thought for creative writing}

\vfill

A step of deliberate search in a randomly picked Creative Writing task. Given the input, the LM samples five different plans, and then votes five times to decide which plan is best.
    
\begin{figure}
    \centering
    \includegraphics{figure/tot_creative_writing.png}\\
\citebutton{Yao et al., 2023}{https://arxiv.org/pdf/2305.10601.pdf}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Tree-of-thought for creative writing (2)}

\vfill
    
\begin{figure}
\raisebox{0pt}[\height][\depth]{\hspace{-0.85cm}%
    \includegraphics[width=1.15\textwidth]{figure/totcreativebig.png}
}
\end{figure}

\vfill

\end{vbframe}

% ------------------------------------------------------------------------------


\begin{frame}{\cotpfull: Error breakdown}

\vfill

\begin{itemize}
    \item 8\% calculator error
    \item 16\% symbol mapping error
    \item 22\% one missing step error
    \item rest: semantic issues, incoherent \cotp
\item Source: Stanford CS25: Beyond LLMs: Agents, Emergent
    Abilities, Intermediate-Guided Reasoning
\end{itemize}

\vfill

\end{frame}


\begin{frame}{\cotpfull: What could go wrong?}

\vfill

\begin{itemize}
    \item Decompose complex problems into a sequence of reasoning steps
    \item By describing the reasoning, interpretability is increased. It provides the possibility to observe where reasoning went wrong
    \item It is closer to how humans solve tasks using language
    \item Language models, if
given a well designed  chain-of-thought prompt, can
    solve problems they otherwise would not be able to solve.

\item \ques What could go wrong?
% no guarantee that the reasoning given is the reasoning employed
% is this true for humans?
% only one answer, but what happens if there is no good single answer?
% clearly a great thing for the examples given, but how
%    typical is this for what we need from language models?
%    -> go back to six examples that were blown up
%    inearlier slides
\end{itemize}

\vfill

\end{frame}

\begin{frame}{\cotpfull: Why does it work?}

\vfill

\begin{itemize}
    \item \ques Why does it work?
% make clear that reasoning is required
% make clear what kind o reasoning is rquirecd
% allocate additional compute
% LLMs are good at finding their own mistakes if
%    they look over there own output
\end{itemize}

\vfill

\end{frame}



\begin{frame}{\cotpfull: Why does it work?}

\vfill

\begin{itemize}
    \item \ques Do top-of-the-line LLMs use \cotpfull?
% show metricalfoot5.o.txt
% so this model uses chain of thought without chain of
%    thought prompting: how come?
% one possibility system prompt
% show system prompt
% e.g., "organize responses to flow well, not by source or citation
% another possibiley: standard training
% a third possibilty: new methdology, reasoning tokesn
\end{itemize}

\vfill

\end{frame}

\begin{frame}{Chain-of-Thought: Terminology}

\vfill

\begin{itemize}
    \item Shot = ``training example''
    \item few-shot prompting = few-shot learning
    \item The prompt ``think step by step'' by itself
    (without shots) is not \cotpfull.
    \item \cotpfull is defined as including shots.
    \item Chain-of-Thought is currently used as a general
    term to refer to the idea of LLMs using explicit
    reasoning steps to arrive at an answer.
    \item So the current usage of Chain-of-Thought is more
    general than \cotpfull.
\end{itemize}

\vfill

\end{frame}
\begin{frame}{Chain-of-Thought in OpenAI's o1}

\vfill

\begin{itemize}
    \item \citebutton{openai}{https://openai.com/index/learning-to-reason-with-llms/}
\item We are introducing OpenAI o1, a new large language
model trained with reinforcement learning to perform complex
reasoning. o1 thinks before it answers -- it can produce a long
internal chain of thought before responding to the user.
\item internal!
\end{itemize}

\vfill

\end{frame}

\begin{frame}{Generator-Verifier Gap (Noam Brown)}

\vfill

\begin{itemize}
    \item For many important problems, it is much easier to
    verify a solution than generating one.
    \item Chain-of-thought is expected to help for such
    problems with a generator-verifier gap.
    \item Problems with generator-verifier gap: Sudoku,
    doing math, programming
    \item Problems with less of a generator-verifier gap:
    knowledge questions (what is the capital of bhutan?),
    simple pattern matching (which language is this?)
    
\end{itemize}

\vfill

\end{frame}

\begin{frame}{Slido}

\vfill

\begin{itemize}
    \item 1313837 https://app.sli.do/event/dinLdZRBHw2fXo5R31C3Nt
    \item 1435969 https://app.sli.do/event/kWQDLpHa14256yiwxyCDr5
    \item 4039244 https://app.sli.do/event/ef5nQS8XmbhWAQDVk9CYQs
    \item 42248917 https://app.sli.do/event/5YwPZfoEFAibQ4DbFTzfj2
\end{itemize}

\vfill

\end{frame}



\endlecture
\end{document}
