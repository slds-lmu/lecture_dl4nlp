

\input{../../style/preamble}
\usepackage{xspace}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\def\cotpfull{chain-of-thought few-shot prompting\xspace}
\def\cotp{COT-FS-P\xspace}

%\newcommand{\titlefigure}{figure/gpt_sq.png}
\newcommand{\learninggoals}{
\item understand how reasoning works beyond \cotpfull
\item understand how reasoning models are trained
}

\definecolor{texblue}{rgb}{0, 0, 1}
\def\myblue#1{\textcolor{texblue}{#1}}

\title{DeepSeek Reasoning}
% \author{}
\institute{\href{https://slds-lmu.github.io/lecture_dl4nlp/}{slds-lmu.github.io/lecture\_dl4nlp}}
\date{}

\begin{document}


\lecturechapter{Large Language Models (LLMs)}

\begin{vbframe}{REINSTATE LECTURE COMMAND}
\end{vbframe}

\input{newway}


\begin{frame}{Sources}

\vfill

\begin{itemize}
    \item First major public
    reasoning LLM (as opposed to proprietary closed):
    DeepSeek 
\item \citebutton{DeepSeek-AI, 2025}{https://arxiv.org/abs/2501.12948}
\item \citebutton{Nature, 2025}{https://www.nature.com/articles/s41586-025-09422-z}
\end{itemize}

\vfill

\end{frame}

\begin{vbframe}{Data for SFT: 40B tokens!}

\vfill

\begin{figure}
    \centering
    \includegraphics[width=12cm]{figure/deepseekmath.png}
\end{figure}

\vfill

\end{vbframe}


\begin{vbframe}{Reinforcement learning}

\vfill

\begin{itemize}
\item In reinforcement learning, an agent takes a sequence
    of actions towards a goal and gets feedback on the
    success of that sequence at the end.
    \item Details: Next lecture
    \item For reasoning trajectories, the feedback is:
    correct or incorrect.
\item (thanks to Emma Brunskill) \begin{tabular}{l@{\hspace{0.2cm}}ll}
& supervised learning & reinforcement learning\\\hline
goal&optimal predictor & optimal policy\\ 
learn from experience? & yes & yes\\
generalization? & yes & yes\\
feedback & rich (translation) & sparse (in/correct)\\
feedback when? & immediately & delayed\\
exploration? & no & yes
\end{tabular}

\end{itemize}

\vfill

\end{vbframe}




\begin{frame}{Synthetic data generation}

\vfill

\begin{block}{No new-shot prompt required}
A conversation between User and Assistant. The User asks a
question and the Assistant solves it. The Assistant first
thinks about the reasoning process in the mind and then
provides the User with the answer. The reasoning process and
answer are enclosed within <think>...</think> and
<answer>...</answer> tags, respectively, that is, <think>
reasoning process here </think><answer> answer here
</answer>. User: prompt.  Assistant:”, in which the prompt
is replaced with the specific reason- ing question during
training. We intentionally limit our constraints to this
structural format, avoiding any content-specific biases to
ensure that we can accurately observe the natural
progression of the model during the RL process.
\end{block}

\vfill

\end{frame}


\begin{frame}{Inference time scaling}

\vfill

\begin{itemize}
    \item For a hard problem, it often helps to think for a
    long time to come up with the solution.
    \item Inference time scaling: teach the model to think
    for a while before giving the answer.
    \item Here, thinking means that the model produces
    potentially long output, the reasoning trace, that leads
    to the correct answer.
\end{itemize}

\vfill

\end{frame}

\begin{vbframe}{Automatic verifier/verifiable rewards: AIME example}

\vfill

\begin{figure}
    \centering
    \includegraphics[width=11.5cm]{figure/aime,example}
\end{figure}

\vfill

\end{vbframe}

\begin{vbframe}{Example of a reasoning trace}

\vfill

\begin{tabular}{lp{2cm}}
    \includegraphics[width=8cm]{figure/reasoning,trace}&
\vspace{-5cm} \ques Is this chain of thought prompting? 
\end{tabular}

\vfill

\end{vbframe}


\begin{frame}{What elements of reasoning are learned?}

\vfill

\begin{itemize}
\item dynamic strategy adaptation
    \item self-verification
    \item reflection
    \item backtracking
    \item reasoning traces are getting longer
\item exploration of altenative approches
\item diverse and sophisticated reasoning behaviours.
\end{itemize}

\vfill

\end{frame}


\begin{vbframe}{Example for Aha moment: ``wait''}

\vfill

\begin{figure}
    \centering
    \includegraphics[width=10cm]{figure/raspberry,wait}
\end{figure}

\vfill

\end{vbframe}
\begin{vbframe}{The rise of reflective terms}

\vfill

\begin{tabular}{ll}
    \includegraphics[width=8cm]{figure/reasonling}
  \begin{minipage}[b]{3cm}
Frequency of
 reflective terms
(‘wait’, ‘mistake’, ‘however’,
 ‘but’, ‘retry’, ‘error’,
 ‘verify’, ‘wrong’, ‘evaluate’, ‘check’)
in model output during
 training
\end{minipage}
\end{tabular}

\vfill

\end{vbframe}


\begin{vbframe}{Accuracy AIME: Above human average}

\vfill

\begin{figure}
    \centering
    \includegraphics[width=8cm]{figure/accuracy,aime.png}
\end{figure}

\vfill

\end{vbframe}


\begin{vbframe}{R1 scaling: Without explicit length signal}

\vfill

\begin{figure}
    \centering
    \includegraphics[width=10cm]{figure/r1scaling}
\end{figure}

\vfill

\end{vbframe}



\begin{frame}{Things we have not covered}

\vfill

\begin{itemize}
\item use an eco system of several models
\item for distillation, generation, verification
\item some ``light'' human post-editing of training data
\item rejection sampling: keep only high-quality,
    high-confidence training data
\item apart from math, the also do this for code and other domains
\item major hardware optimization work
\item claim: cost of training $<$ 1 000 000 million dollar
\end{itemize}

\vfill

\end{frame}

\input{newway}

\endlecture
\end{document}



% human-defined reasoning patterns may limit model
% exploration,
% unrestricted RL training can better incentivize the
% emergence
% of new reasoning capabilities in LLMs

%    •	Cold-start SFT data: collect thousands of long CoT
%    examples via few-shot prompting, direct prompting for
%    reflected/verified answers, and cleaned outputs from
%    R1-Zero, with light human post-processing for
%    readability.  ￼
%•	Reasoning-oriented RL with
%		  rule-based, verifiable rewards (math
%		  answers in a specified format; code
%		  checked with tests).  ￼
%•    Rejection sampling → new SFT
%			  set (curated prompts; keep only
%			  correct traces; ~600k reasoning
%			  samples), then another RL round.
			  ￼







