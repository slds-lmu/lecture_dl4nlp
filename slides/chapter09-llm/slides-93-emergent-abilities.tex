\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble_new}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Deep Learning for NLP}

\definecolor{texblue}{rgb}{0, 0, 1}
\def\myblue#1{\textcolor{texblue}{#1}}
\long\def\devour#1{}

\begin{document}

\titlemeta{
Large Language Models (LLMs)
}{
Emergent Abilities
}{
figure/bertfinetune1
}{
\item illustrate emergent abilities that LLMs reveal when they are scaled up
\item discuss counter-arguments
and counter-counter-arguments
for the concept of emergence
}

% ------------------------------------------------------------------------------

\begin{frame}{Emergent abilities}

\vfill

An \textit{emergent ability} is an ability that is not
present in small models but is present in large models.
\vskip3mm

\begin{itemize}
\item Assumption: Everything else is held constant.
    \item Questions: Is emergence a rare phenomenon?
    Are many tasks emergent?
    \item We can answer this by investigating model families
    with many sizes.
    \begin{itemize}
        \item GPT-3
        \item Chinchilla
        \item PaLM
    \end{itemize}
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Emergent abilities and model size}

\vfill

\begin{figure}
    \centering
    \includegraphics[width=13cm]{figure/emergent_abilities2.png}
    \caption{Examples of emergence in few-shot prompting. \citebutton{Wei et al., 2022}{https://arxiv.org/abs/2206.07682}}
\end{figure}

\vfill

\end{frame}
\begin{frame}{Emergent abilities and model size}

\vfill

\begin{figure}
    \centering
    \includegraphics[width=0.84\textwidth]{figure/emergent_abilities.png}
    \caption{Examples of emergence in few-shot prompting. \citebutton{Wei et al., 2022}{https://arxiv.org/abs/2206.07682}}
    \label{fig:emergent_abilities}
\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Emergent tasks in Big-Bench}

\vfill

\begin{itemize}
\item ``MODEL SIZE (TASK)'' means: MODEL can do
TASK with SIZE, but not with less than SIZE (hence emerging)
    \item GPT-3 13B (2 tasks): hindu knowledge, modified arithmetic
    \item GPT-3 175B (15 tasks): analytic entailment, codenames, phrase relatedness, question answer creation, self evaluation tutoring, ...
    \item LaMDA 137B (8 tasks): gender inclusive sentences german, repeat copy logic, sports understanding, ...
    \item PaLM 8B (3 tasks): auto debugging, sufficient information, ParsiNLU reading comprehension
    \item PaLM 64B (14 tasks): anachronisms, ascii word recognition, conceptual combinations, ...
    \item PaLM 540B (25 tasks): analogical similarity, causal judgment, code line description, crass ai, cs algorithms, ...
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Emergent tasks in MMLU}

\vfill

\begin{itemize}
    \item Chinchilla 7B (7 tasks): Professional Medicine, High School Statistics, High School Macroeconomics, High School Psychology, Anatomy, High School Government And Politics, High School Microeconomics
    \item Chinchilla 70B (44 tasks): International Law, Human Aging, Sociology, Us Foreign Policy, High School World History, Marketing, Logical Fallacies, Miscellaneous, College Biology, High School Us History, Security Studies, High School European History, ...
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Other emergent tasks}

\vfill

\begin{itemize}
    \item GPT-3 paper: 3 digit addition/subtraction (GPT-3 13B), 4-5 digit addition/substraction (GPT-3 175B), leveraging few-shot examples for word denoising (GPT-3 13B)
    \item Gopher paper: Toxicity classification (Gopher 7.1B), TruthfulQA (Gopher 280B)
    \item Patel \& Pavlick: grounded conceptual mappings (GPT-3 175B)
    \item PaLM paper: Word in Context benchmark (PaLM 540B)
\end{itemize}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

% \begin{frame}{Impact of Model Size}

% \vfill

% \vfill

% \end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{counter argument}

\citebutton{Source: Schaeffer et al., 2023}{https://arxiv.org/pdf/2304.15004.pdf}

\vfill

Two defining properties for emergent abilities in LLMs:

\begin{enumerate}
%
\item \textbf{Sharpness:} transitioning seemingly instantaneously from not present to present
%
\item \textbf{Unpredictability:} transitioning at seemingly unforeseeable model scales
%
\end{enumerate}

\vskip5mm

Claim: Emergent abilities appear only under metrics that nonlinearly or discontinuously scale model's per-token error rate:

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/metrics.png} \\ 
	\end{figure}

\vfill

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}{counter argument}

\vfill

	\begin{figure}
		\centering
		\includegraphics[width = 11cm]{figure/emergent_experiments.png}\\ 
		\citebutton{Source: Schaeffer et al., 2023}{https://arxiv.org/pdf/2304.15004.pdf}
	\end{figure}

\vfill

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Counterargument: Summary}

\vfill

%\textbf{Conclusions:}
%
\begin{itemize}
%
\item \myblue{Emergence is merely an artifact of the evaluation
		measures we use.}

\item
If TASK is emergent for family MODEL at SIZE on METRIC, then
it is often possible to choose another metric for which the
task is not emergent.
%
\item Emergent abilities can be induced in computer vision tasks as well
%
\item A task and a metric are distinct and meaningful choices when constructing a benchmark
%
\item When choosing a metric, one should consider the
metric's effect on the per-token error rate;
may require adapting the measuring process
%
\end{itemize}


\vfill

\end{frame}

% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------

\begin{frame}{counter counter argument (1)}

\vfill

	\begin{figure}
		\centering
		\includegraphics[height=4cm]{figure/pinecone.png}\\
                https://www.journals.uchicago.edu/toc/an/current,
 Robert Dryja

	\end{figure}
 \myblue{Emergence is a holistic / gestalt phenomenon. Even
 if there is a straightforward mathematical explanation for
 an emergent phenomenon, it can still make sense to
 distinguish it from subjectively non-emergent phenomena.}

\vfill

\end{frame}


\begin{frame}{counter counter argument (2)}

\vfill

%
\begin{itemize}
\item \ques What was your emergence moment with LLMs?
%\item Subjectively, there was clearly some form of ``psychological'' emergence
%when GPT3 and InstructGPT came out (several other models
%as well)

% overall the transition from n-gram language models to
% large neural language models was a quantum leap

%
\end{itemize}


\vfill

\end{frame}

\begin{frame}{Back to finetuning}

\vfill

%
\begin{itemize}
\item \ques You are working at a telecommunications
company. You are given the task of converting a German manual from
polite form (``Wir sind jederzeit unter der folgenden
Nummer fuer Sie erreichbar'') to familiar form
(``Wir sind jederzeit unter der folgenden
Nummer fuer Dich erreichbar''). How would you do this?

\end{itemize}


\vfill

\end{frame}


% ------------------------------------------------------------------------------

% \begin{frame}{Summary}

% \vfill

% \begin{itemize}
%     \item Can we improve model architectures?
%     \item Can we improve data quality and quantity? 
%     \item Better prompting.
%     \item Frontier tasks. 
%     \item Why do emergent abilities occur, and can we predict them? 
% \end{itemize}

% \vfill

% \end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
